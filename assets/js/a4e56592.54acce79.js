"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1200],{33599:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"Document Home/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","title":"Qualcomm\xae AI Hub","description":"Qualcomm AI Hub contains a large collection of pretrained AI models that are optimized to run on Dragonwing hardware on the NPU.","source":"@site/docs/Document Home/7.Application Development and Execution Guide/1.Building AI Models/2.qualcomm_ai_hub.md","sourceDirName":"Document Home/7.Application Development and Execution Guide/1.Building AI Models","slug":"/Document Home/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/7.Application Development and Execution Guide/1.Building AI Models/2.qualcomm_ai_hub.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Edge Impulse","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/edge_impulse"},"next":{"title":"LiteRT / TFLite","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite"}}');var i=t(62540),a=t(43023);const s={},r="Qualcomm\xae AI Hub",l={},d=[{value:"Finding supported models",id:"finding-supported-models",level:2},{value:"Deploying a model to NPU (Python)",id:"deploying-a-model-to-npu-python",level:2},{value:"Running the example repository",id:"running-the-example-repository",level:3},{value:"Porting the model to NPU",id:"porting-the-model-to-npu",level:3},{value:"Preprocessing inputs",id:"preprocessing-inputs",level:4},{value:"Postprocessing outputs",id:"postprocessing-outputs",level:4},{value:"End-to-end example (Python)",id:"end-to-end-example-python",level:3},{value:"Deploying a model to NPU (Edge Impulse)",id:"deploying-a-model-to-npu-edge-impulse",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"qualcomm-ai-hub",children:"Qualcomm\xae AI Hub"})}),"\n",(0,i.jsxs)(n.p,{children:["Qualcomm ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"AI Hub"})," contains a large collection of pretrained AI models that are optimized to run on Dragonwing hardware on the NPU."]}),"\n",(0,i.jsx)(n.h2,{id:"finding-supported-models",children:"Finding supported models"}),"\n",(0,i.jsx)(n.p,{children:"Models in AI Hub are categorized by supported Qualcomm chipset. To see models that will run on your development kit:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to the ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models",children:"model list"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Under 'Chipset', select:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RB3 Gen 2 Vision Kit: 'Qualcomm QCS6490 (Proxy)'"}),"\n",(0,i.jsx)(n.li,{children:"RUBIK Pi 3: 'Qualcomm QCS6490 (Proxy)'"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-a-model-to-npu-python",children:"Deploying a model to NPU (Python)"}),"\n",(0,i.jsxs)(n.p,{children:["As an example, let's deploy the ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"})," model."]}),"\n",(0,i.jsx)(n.h3,{id:"running-the-example-repository",children:"Running the example repository"}),"\n",(0,i.jsxs)(n.p,{children:["All AI Hub models come with an example repository. This is a good starting point, as it shows exactly how to ",(0,i.jsx)(n.em,{children:"run"})," the model. It shows what the input to your network should look like, and how to interpret the output (here, to map the output tensor to bounding boxes). The example repositories do ",(0,i.jsx)(n.em,{children:"NOT"})," run on the NPU or GPU yet - they run without acceleration. Let's see what our input/output should look like before we move this model to the NPU."]}),"\n",(0,i.jsxs)(n.p,{children:["On the AI Hub page for ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"}),', click "Model repository". This links you to a ',(0,i.jsx)(n.a,{href:"https://github.com/quic/ai-hub-models/tree/main/qai_hub_models/models/facemap_3dmm",children:"README"})," file with instructions on how to run the example repository."]}),"\n",(0,i.jsx)(n.p,{children:"To deploy this model, open the terminal on your development board, or an ssh session to your development board, and:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create a new venv and install some base packages:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/aihub-demo\ncd ~/aihub-demo\n\npython3 -m venv .venv\nsource .venv/bin/activate\n\npip3 install numpy setuptools Cython shapely\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Download an image with a face (640x480 resolution, JPG format) onto your development board, e.g. via:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://cdn.edgeimpulse.com/qc-ai-docs/example-images/three-people-640-480.jpg\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-0cc3ca50bf1d29e11512b6bedb879ed101e8f7bb%2Faihub-three-people-in.jpg?",alt:"",title:"Input image with three people"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Input resolution:"}),' AI Hub models require correctly sized inputs. You can find the required resolution under "Technical Details > Input resolution" (in ',(0,i.jsx)(n.em,{children:"HEIGHT x WIDTH"})," (here 480x640 => 640x480 for wxh)); or inspect the size of the input tensor on the TFLite or ONNX file."]})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Follow the instructions under 'Example & Usage' for the Facial Landmark Detection model:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Install the example (add --no-build-isolation)\npip3 install --no-build-isolation "qai-hub-models[face-det-lite]"\n\n# Run the example\n#    Use --help to see all options\npython3 -m qai_hub_models.models.face_det_lite.demo --quantize w8a8 --image ./three-people-640-480.jpg --output-dir out/\n'})}),"\n",(0,i.jsxs)(n.p,{children:["You can find the output image in ",(0,i.jsx)(n.code,{children:"out/FaceDetLitebNet_output.png"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"If you're connected over ssh, you can copy the output image back to your host computer via:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Find IP via: ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n# Then: (replace 192.168.1.148 by the IP address of your development kit)\n\nscp ubuntu@192.168.1.148:~/aihub-demo/out/FaceDetLitebNet_output.png ~/Downloads/FaceDetLitebNet_output.png\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-a7485166f25f0813e5ef1b94a8654353f644a8f7%2Faihub-three-people-annotated.png?alt=media",alt:"",title:"Lightweight face detection output"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Alright! We have a working model. For reference, on the RB3 Gen 2 Vision Kit, running this model takes 189.7ms per inference."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"porting-the-model-to-npu",children:"Porting the model to NPU"}),"\n",(0,i.jsx)(n.p,{children:"Now that we have a working reference model, let's run it on the NPU. There are three parts that you need to implement."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"You need to preprocess the data, e.g. convert the image into features that you can pass to the neural network."}),"\n",(0,i.jsxs)(n.li,{children:["You need to export the model to ONNX or TFLite, and run the model through ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," or ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"ONNX Runtime"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"You need to postprocess the output, e.g. convert the output of the neural network to bounding boxes of faces."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The model is straight forward, as you can read in the ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," and ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"ONNX Runtime"})," pages. However, the pre- and post-processing code might not be..."]}),"\n",(0,i.jsx)(n.h4,{id:"preprocessing-inputs",children:"Preprocessing inputs"}),"\n",(0,i.jsxs)(n.p,{children:["For image models most AI Hub models take a matrix of ",(0,i.jsx)(n.code,{children:"(HEIGHT, WIDTH, CHANNELS)"})," (LiteRT) or ",(0,i.jsx)(n.code,{children:"(CHANNELS, HEIGHT, WIDTH)"})," (ONNX) scaled from 0..1. If you have 1 channel, convert the image to grayscale first. If your model is quantized (most likely) you'll also need to read zero_point and scale, and scale the pixels accordingly (this is easy in LiteRT as they contain the quantization parameters, but ONNX does not have these). Typically you'll end up with data scaled linearly 0..255 (uint8) or -128..127 (int8) for quantized models - so that's relatively easy. A function that demonstrates all this in Python can be found below in the example code (",(0,i.jsx)(n.code,{children:"def load_image_litert"}),")."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.em,{children:"HOWEVER..."})," This is not guaranteed; and this is where the AI Hub example code comes in. Every AI Hub example contains the exact code used to scale inputs. In our current example - Lightweight-Face-Detection - the input is shaped ",(0,i.jsx)(n.code,{children:"(480, 640, 1)"}),". However, if you look at the ",(0,i.jsx)(n.a,{href:"https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L70",children:"preprocessing code"})," the data is not converted to grayscale, but instead only the blue channel of an RGB image is taken:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'img_array = img_array.astype("float32") / 255.0\nimg_array = img_array[np.newaxis, ...]\nimg_tensor = torch.Tensor(img_array)\nimg_tensor = img_tensor[:, :, :, -1]        # HERE WE TAKE BLUE CHANNEL, NOT CONVERT TO GRAYSCALE\n'})}),"\n",(0,i.jsx)(n.p,{children:"These kind of things are very easy to get wrong. So if you see non-matching results between your implementation and the AI Hub example: read the code. This applies even more for non-image inputs (e.g. audio). Use the demo code to understand what the model actually expects."}),"\n",(0,i.jsx)(n.h4,{id:"postprocessing-outputs",children:"Postprocessing outputs"}),"\n",(0,i.jsxs)(n.p,{children:["The same applies to postprocessing. For example, there's no standard way of mapping the output of a neural network to bounding boxes (to detect faces). For Lightweight-Face-Detection you can find the code here: ",(0,i.jsx)(n.a,{href:"https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L77",children:"face_det_lite/app.py#L77"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"If you're targeting Python, it's often easiest to copy the postprocessing code into your application; as AI Hub has a lot of dependencies that you might not want. In addition the postprocessing code operates on PyTorch tensors, and your inference runs under LiteRT or ONNX Runtime; thus, you'll need to change some small aspects. We'll show this just below in the end-to-end example."}),"\n",(0,i.jsx)(n.h3,{id:"end-to-end-example-python",children:"End-to-end example (Python)"}),"\n",(0,i.jsx)(n.p,{children:"With the explanation behind us, let's look at some code."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Open a terminal on your development board, and set up the base requirements for this example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create a new fresh directory\nmkdir -p ~/aihub-npu\ncd ~/aihub-npu\n\n# Create a new venv\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Install the LiteRT runtime (to run models) and Pillow (to parse images)\npip3 install ai-edge-litert==1.3.0 Pillow\n\n# Download an example image\nwget https://cdn.edgeimpulse.com/qc-ai-docs/example-images/three-people-640-480.jpg\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The NPU only supports uint8/int8 quantized models. Fortunately AI Hub contains pre-quantized and optimized models already. You can either:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Download the model for this tutorial (mirrored on CDN):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://cdn.edgeimpulse.com/qc-ai-docs/models/face_det_lite-lightweight-face-detection-w8a8.tflite\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Or, for any other model - download the model from AI Hub and push to your development board:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Go to ",(0,i.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/face_det_lite",children:"Lightweight-Face-Detection"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'Click "Download model".'}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'Select "TFLite" for runtime, and "w8a8" for precision.'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-a402e668a1082b6c0fc8dfdbe2cec20f204dee2d%2Faihub-download.png?alt=media",alt:"",title:"Downloading w8a8 quantized model from AI Hub in TFLite format"})}),"\n",(0,i.jsxs)(n.p,{children:["If your model is only available in ONNX format, see ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/onnxruntime",children:"Run models using ONNX Runtime"})," for instructions. The same principles as in this tutorial apply."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Download the model."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you're not downloading the model directly on your Dragonwing development board, you'll need to push the model over ssh:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Find the IP address of your development board. Run on your development board:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n\n# ... Example:\n# 192.168.1.253\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Push the .tflite file. Run from your computer:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"scp face_det_lite-lightweight-face-detection-w8a8.tflite ubuntu@192.168.1.253:~/face_det_lite-lightweight-face-detection-w8a8.tflite\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Create a new file ",(0,i.jsx)(n.code,{children:"face_detection.py"}),". This file contains the model invocation, plus the preprocessing and postprocessing code from the AI Hub example (see inline comments)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import Image, ImageDraw\nimport os, time, sys\n\ndef curr_ms():\n    return round(time.time() * 1000)\n\n# Paths\nIMAGE_IN = \'three-people-640-480.jpg\'\nIMAGE_OUT = \'three-people-640-480-overlay.jpg\'\nMODEL_PATH = \'face_det_lite-lightweight-face-detection-w8a8.tflite\'\n\n# If we pass in --use-qnn we offload to NPU\nuse_qnn = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-qnn\' else False\n\nexperimental_delegates = []\nif use_qnn:\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type":"htp"})]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=experimental_delegates\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensor details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# === BEGIN PREPROCESSING ===\n\n# Load an image (using Pillow) and make it in the right format that the interpreter expects (e.g. quantize)\n# All AI Hub image models use 0..1 inputs to start.\ndef load_image_litert(interpreter, path, single_channel_behavior: str = \'grayscale\'):\n    d = interpreter.get_input_details()[0]\n    shape = [int(x) for x in d["shape"]]  # e.g. [1, H, W, C] or [1, C, H, W]\n    dtype = d["dtype"]\n    scale, zp = d.get("quantization", (0.0, 0))\n\n    if len(shape) != 4 or shape[0] != 1:\n        raise ValueError(f"Unexpected input shape: {shape}")\n\n    # Detect layout\n    if shape[1] in (1, 3):   # [1, C, H, W]\n        layout, C, H, W = "NCHW", shape[1], shape[2], shape[3]\n    elif shape[3] in (1, 3): # [1, H, W, C]\n        layout, C, H, W = "NHWC", shape[3], shape[1], shape[2]\n    else:\n        raise ValueError(f"Cannot infer layout from shape {shape}")\n\n    # Load & resize\n    img = Image.open(path).convert("RGB").resize((W, H), Image.BILINEAR)\n    arr = np.array(img)\n    if C == 1:\n        if single_channel_behavior == \'grayscale\':\n            # Convert to luminance (H, W)\n            gray = np.asarray(Image.fromarray(arr).convert(\'L\'))\n        elif single_channel_behavior in (\'red\', \'green\', \'blue\'):\n            ch_idx = {\'red\': 0, \'green\': 1, \'blue\': 2}[single_channel_behavior]\n            gray = arr[:, :, ch_idx]\n        else:\n            raise ValueError(f"Invalid single_channel_behavior: {single_channel_behavior}")\n        # Keep shape as HWC with C=1\n        arr = gray[..., np.newaxis]\n\n    # HWC -> correct layout\n    if layout == "NCHW":\n        arr = np.transpose(arr, (2, 0, 1))  # (C,H,W)\n\n    # Scale 0..1 (all AI Hub image models use this)\n    arr = (arr / 255.0).astype(np.float32)\n\n    # Quantize if needed\n    if scale and float(scale) != 0.0:\n        q = np.rint(arr / float(scale) + int(zp))\n        if dtype == np.uint8:\n            arr = np.clip(q, 0, 255).astype(np.uint8)\n        else:\n            arr = np.clip(q, -128, 127).astype(np.int8)\n\n    return np.expand_dims(arr, 0)  # add batch\n\n# This model looks like grayscale, but AI Hub inference actually takes the BLUE channel\n# see https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L70\ninput_data = load_image_litert(interpreter, IMAGE_IN, single_channel_behavior=\'blue\')\n\n# === END PREPROCESSING (input_data contains right data) ===\n\n# Set tensor and run inference\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\n\n# Run once to warmup\ninterpreter.invoke()\n\n# Then run 10x\nstart = curr_ms()\nfor i in range(0, 10):\n    interpreter.invoke()\nend = curr_ms()\n\n# === BEGIN POSTPROCESSING ===\n\n# Grab 3 output tensors and dequantize\nq_output_0 = interpreter.get_tensor(output_details[0][\'index\'])\nscale_0, zero_point_0 = output_details[0][\'quantization\']\nhm = ((q_output_0.astype(np.float32) - zero_point_0) * scale_0)[0]\n\nq_output_1 = interpreter.get_tensor(output_details[1][\'index\'])\nscale_1, zero_point_1 = output_details[1][\'quantization\']\nbox = ((q_output_1.astype(np.float32) - zero_point_1) * scale_1)[0]\n\nq_output_2 = interpreter.get_tensor(output_details[2][\'index\'])\nscale_2, zero_point_2 = output_details[2][\'quantization\']\nlandmark = ((q_output_2.astype(np.float32) - zero_point_2) * scale_2)[0]\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/utils/bounding_box_processing.py#L369\ndef get_iou(boxA: np.ndarray, boxB: np.ndarray) -> float:\n    """\n    Given two tensors of shape (4,) in xyxy format,\n    compute the iou between the two boxes.\n    """\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n    return inter_area / float(boxA_area + boxB_area - inter_area)\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py\nclass BBox:\n    # Bounding Box\n    def __init__(\n        self,\n        label: str,\n        xyrb: list[int],\n        score: float = 0,\n        landmark: list | None = None,\n        rotate: bool = False,\n    ):\n        """\n        A bounding box plus landmarks structure to hold the hierarchical result.\n        parameters:\n            label:str the class label\n            xyrb: 4 list for bbox left, top,  right bottom coordinates\n            score:the score of the detection\n            landmark: 10x2 the landmark of the joints [[x1,y1], [x2,y2]...]\n        """\n        self.label = label\n        self.score = score\n        self.landmark = landmark\n        self.x, self.y, self.r, self.b = xyrb\n        self.rotate = rotate\n\n        minx = min(self.x, self.r)\n        maxx = max(self.x, self.r)\n        miny = min(self.y, self.b)\n        maxy = max(self.y, self.b)\n        self.x, self.y, self.r, self.b = minx, miny, maxx, maxy\n\n    @property\n    def width(self) -> int:\n        return self.r - self.x + 1\n\n    @property\n    def height(self) -> int:\n        return self.b - self.y + 1\n\n    @property\n    def box(self) -> list[int]:\n        return [self.x, self.y, self.r, self.b]\n\n    @box.setter\n    def box(self, newvalue: list[int]) -> None:\n        self.x, self.y, self.r, self.b = newvalue\n\n    @property\n    def haslandmark(self) -> bool:\n        return self.landmark is not None\n\n    @property\n    def xywh(self) -> list[int]:\n        return [self.x, self.y, self.width, self.height]\n\n# Taken from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py\ndef nms(objs: list[BBox], iou: float = 0.5) -> list[BBox]:\n    """\n    nms function customized to work on the BBox objects list.\n    parameter:\n        objs: the list of the BBox objects.\n    return:\n        the rest of the BBox after nms operation.\n    """\n    if objs is None or len(objs) <= 1:\n        return objs\n\n    objs = sorted(objs, key=lambda obj: obj.score, reverse=True)\n    keep = []\n    flags = [0] * len(objs)\n    for index, obj in enumerate(objs):\n        if flags[index] != 0:\n            continue\n\n        keep.append(obj)\n        for j in range(index + 1, len(objs)):\n            # if flags[j] == 0 and obj.iou(objs[j]) > iou:\n            if (\n                flags[j] == 0\n                and get_iou(np.array(obj.box), np.array(objs[j].box)) > iou\n            ):\n                flags[j] = 1\n    return keep\n\n# Ported from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/utils.py#L110\n# The original code uses torch.Tensor, this uses native numpy arrays\ndef detect(\n    hm: np.ndarray,           # (H, W, 1), float32\n    box: np.ndarray,          # (H, W, 4), float32\n    landmark: np.ndarray,     # (H, W, 10), float32\n    threshold: float = 0.2,\n    nms_iou: float = 0.2,\n    stride: int = 8,\n) -> list:\n    def _sigmoid(x: np.ndarray) -> np.ndarray:\n        # stable-ish sigmoid\n        out = np.empty_like(x, dtype=np.float32)\n        np.negative(x, out=out)\n        np.exp(out, out=out)\n        out += 1.0\n        np.divide(1.0, out, out=out)\n        return out\n\n    def _maxpool3x3_same(x_hw: np.ndarray) -> np.ndarray:\n        """\n        x_hw: (H, W) single-channel array.\n        3x3 max pool, stride=1, padding=1 (same as PyTorch F.max_pool2d(kernel=3,stride=1,padding=1))\n        Pure NumPy using stride tricks.\n        """\n        H, W = x_hw.shape\n        # pad with -inf so edges don\'t borrow smaller values\n        pad = 1\n        xpad = np.pad(x_hw, ((pad, pad), (pad, pad)), mode=\'constant\', constant_values=-np.inf)\n\n        # build 3x3 sliding windows using as_strided\n        s0, s1 = xpad.strides\n        shape = (H, W, 3, 3)\n        strides = (s0, s1, s0, s1)\n        windows = np.lib.stride_tricks.as_strided(xpad, shape=shape, strides=strides, writeable=False)\n        # max over the 3x3 window\n        return windows.max(axis=(2, 3))\n\n    def _topk_desc(values_flat: np.ndarray, k: int):\n        """Return (topk_values_sorted, topk_indices_sorted_desc)."""\n        if k <= 0:\n            return np.array([], dtype=values_flat.dtype), np.array([], dtype=np.int64)\n        k = min(k, values_flat.size)\n        # argpartition for top-k by value\n        idx_part = np.argpartition(-values_flat, k - 1)[:k]\n        # sort those k by value desc\n        order = np.argsort(-values_flat[idx_part])\n        idx_sorted = idx_part[order]\n        return values_flat[idx_sorted], idx_sorted\n\n    # 1) sigmoid heatmap\n    hm = _sigmoid(hm.astype(np.float32, copy=False))\n\n    # squeeze channel -> (H, W)\n    hm_hw = hm[..., 0]\n\n    # 2) 3x3 max-pool same\n    hm_pool = _maxpool3x3_same(hm_hw)\n\n    # 3) local maxima mask (keep equal to pooled)\n    # (like (hm == hm_pool).float() * hm in torch)\n    keep = (hm_hw >= hm_pool)  # >= to keep plateaus, mirrors torch equality on floats closely enough\n    candidate_scores = np.where(keep, hm_hw, 0.0).ravel()\n\n    # 4) topk up to 2000\n    num_candidates = int(keep.sum())\n    k = min(num_candidates, 2000)\n    scores_k, flat_idx_k = _topk_desc(candidate_scores, k)\n\n    H, W = hm_hw.shape\n    ys = (flat_idx_k // W).astype(np.int32)\n    xs = (flat_idx_k %  W).astype(np.int32)\n\n    # 5) gather boxes/landmarks and build outputs\n    objs = []\n    for cx, cy, score in zip(xs, ys, scores_k):\n        if score < threshold:\n            # because scores_k is sorted desc, we can break\n            break\n\n        # box offsets at (cy, cx): [x, y, r, b]\n        x, y, r, b = box[cy, cx].astype(np.float32, copy=False)\n\n        # convert to absolute xyrb in pixels (same math as torch code)\n        cxcycxcy = np.array([cx, cy, cx, cy], dtype=np.float32)\n        xyrb = (cxcycxcy + np.array([-x, -y,  r,  b], dtype=np.float32)) * float(stride)\n        xyrb = xyrb.astype(np.int32, copy=False).tolist()\n\n        # landmarks: first 5 x, next 5 y\n        x5y5 = landmark[cy, cx].astype(np.float32, copy=False)\n        x5y5 = x5y5 + np.array([cx]*5 + [cy]*5, dtype=np.float32)\n        x5y5 *= float(stride)\n        box_landmark = list(zip(x5y5[:5].tolist(), x5y5[5:].tolist()))\n\n        objs.append(BBox("0", xyrb=xyrb, score=float(score), landmark=box_landmark))\n\n    if nms_iou != -1:\n        return nms(objs, iou=nms_iou)\n    return objs\n\n# Detection code from https://github.com/quic/ai-hub-models/blob/8cdeb11df6cc835b9b0b0cf9b602c7aa83ebfaf8/qai_hub_models/models/face_det_lite/app.py#L77\ndets = detect(hm, box, landmark, threshold=0.55, nms_iou=-1, stride=8)\nres = []\nfor n in range(0, len(dets)):\n    xmin, ymin, w, h = dets[n].xywh\n    score = dets[n].score\n\n    L = int(xmin)\n    R = int(xmin + w)\n    T = int(ymin)\n    B = int(ymin + h)\n    W = int(w)\n    H = int(h)\n\n    if L < 0 or T < 0 or R >= 640 or B >= 480:\n        if L < 0:\n            L = 0\n        if T < 0:\n            T = 0\n        if R >= 640:\n            R = 640 - 1\n        if B >= 480:\n            B = 480 - 1\n\n    # Enlarge bounding box to cover more face area\n    b_Left = L - int(W * 0.05)\n    b_Top = T - int(H * 0.05)\n    b_Width = int(W * 1.1)\n    b_Height = int(H * 1.1)\n\n    if (\n        b_Left >= 0\n        and b_Top >= 0\n        and b_Width - 1 + b_Left < 640\n        and b_Height - 1 + b_Top < 480\n    ):\n        L = b_Left\n        T = b_Top\n        W = b_Width\n        H = b_Height\n        R = W - 1 + L\n        B = H - 1 + T\n\n    print(f\'Found face: x={L}, y={T}, w={W}, h={H}, score={score}\')\n\n    res.append([L, T, W, H, score])\n\n# === END POSTPROCESSING ===\n\n# Create new PIL image from the input data, stripping off the batch dim\ninput_reshaped = input_data.reshape(input_data.shape[1:])\nif input_reshaped.shape[2] == 1:\n    input_reshaped = np.squeeze(input_reshaped, axis=-1) # strip off the last dim if grayscale\n\n# And write to output image so we can debug\nimg_out = Image.fromarray(input_reshaped).convert("RGB")\ndraw = ImageDraw.Draw(img_out)\nfor bb in res:\n    L, T, W, H, score = bb\n    draw.rectangle([L, T, L + w, T + H], outline="#00FF00", width=3)\nimg_out.save(IMAGE_OUT)\n\nprint(\'\')\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Run the model on the CPU:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 face_detection.py\n\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n# Found face: x=120, y=186, w=62, h=79, score=0.8306506276130676\n# Found face: x=311, y=125, w=66, h=81, score=0.8148472309112549\n# Found face: x=424, y=173, w=64, h=86, score=0.8093323111534119\n#\n# Inference took (on average): 35.6ms. per image\n"})}),"\n",(0,i.jsx)(n.p,{children:"This already brings down our time per inference from 189.7ms to 35.6ms."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Run the model on the NPU:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 face_detection.py --use-qnn\n\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\n#\n# Found face: x=120, y=186, w=62, h=78, score=0.8255056142807007\n# Found face: x=311, y=125, w=66, h=81, score=0.8148472309112549\n# Found face: x=421, y=173, w=67, h=86, score=0.8093323111534119\n#\n# Inference took (on average): 2.4ms. per image\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udf89 That's it. By quantizing this model and porting it to the NPU we've sped the model up 79 times (!). Hopefully you have a good idea of the qualities of AI Hub, and the potential power of the NPU and the AI Engine Direct SDK. You're not limited to Python either, f.e. the ",(0,i.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/lite-rt",children:"LiteRT"})," page has C++ examples as well."]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-a-model-to-npu-edge-impulse",children:"Deploying a model to NPU (Edge Impulse)"}),"\n",(0,i.jsx)(n.p,{children:"Image classification, visual regression, and certain object detection models can be deployed through Edge Impulse."})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},43023:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(63696);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);