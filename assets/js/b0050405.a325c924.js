"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1137],{1383:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IC-LiteRT-CPU-3322117a486c0011fb31f476f3242ef2.jpeg"},22491:(e,n,t)=>{t.d(n,{A:()=>r});t(63696);var i=t(11750);const s={tabItem:"tabItem_wHwb"};var a=t(62540);function r({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,i.A)(s.tabItem,t),hidden:n,children:e})}},24391:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/Image-Classification-LiteRT-6d8779ca8e637f6c8be14d9611346fc4.png"},43023:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(63696);const s={},a=i.createContext(s);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},78296:(e,n,t)=>{t.d(n,{A:()=>v});var i=t(63696),s=t(11750),a=t(90766),r=t(49519),o=t(14395),l=t(35043),d=t(44544),p=t(15735);function c(e){return i.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,i.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,i.useMemo)(()=>{const e=n??function(e){return c(e).map(({props:{value:e,label:n,attributes:t,default:i}})=>({value:e,label:n,attributes:t,default:i}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function h({queryString:e=!1,groupId:n}){const t=(0,r.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(s),(0,i.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,a=u(e),[r,l]=(0,i.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[d,c]=h({queryString:t,groupId:s}),[g,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,p.Dv)(n);return[t,(0,i.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),f=(()=>{const e=d??g;return m({value:e,tabValues:a})?e:null})();(0,o.A)(()=>{f&&l(f)},[f]);return{selectedValue:r,selectValue:(0,i.useCallback)(e=>{if(!m({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),c(e),_(e)},[c,_,a]),tabValues:a}}var _=t(86681);const f={tabList:"tabList_J5MA",tabItem:"tabItem_l0OV"};var x=t(62540);function b({className:e,block:n,selectedValue:t,selectValue:i,tabValues:r}){const o=[],{blockElementScrollPositionUntilNextRender:l}=(0,a.a_)(),d=e=>{const n=e.currentTarget,s=o.indexOf(n),a=r[s].value;a!==t&&(l(n),i(a))},p=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:i})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:p,onClick:d,...i,className:(0,s.A)("tabs__item",f.tabItem,i?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function j({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,i.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,i.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function y(e){const n=g(e);return(0,x.jsxs)("div",{className:(0,s.A)("tabs-container",f.tabList),children:[(0,x.jsx)(b,{...n,...e}),(0,x.jsx)(j,{...n,...e})]})}function v(e){const n=(0,_.A)();return(0,x.jsx)(y,{...e,children:c(e.children)},String(n))}},78545:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>d,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","title":"LiteRT / TFLite","description":"LiteRT, formerly known as TensorFlow Lite, is Google\'s high-performance runtime for on-device AI. You can run existing quantized LiteRT models (in Python or C++) on the NPU on Dragonwing devices with a single line of code using the LiteRT delegates that are part of AI Engine Direct.","source":"@site/docs/Document Home/2.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","sourceDirName":"Document Home/2.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/2.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/3.litert_tflite.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Qualcomm AI Hub","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub"},"next":{"title":"ONNX","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"}}');var s=t(62540),a=t(43023),r=t(78296),o=t(22491);const l={},d="LiteRT / TFLite",p={},c=[{value:"Quantizing models",id:"quantizing-models",level:2},{value:"Running a model on the NPU (Python)",id:"running-a-model-on-the-npu-python",level:2},{value:"Running a model on the NPU (C++)",id:"running-a-model-on-the-npu-c",level:2},{value:"Python Examples",id:"python-examples",level:3},{value:"Vision Transformers",id:"vision-transformers",level:3},{value:"GTK-Based Image Classification App",id:"gtk-based-image-classification-app",level:3},{value:"Helper Functions",id:"helper-functions",level:3},{value:"Reference Code",id:"reference-code",level:3},{value:"Object Detection with OpenCV &amp; Wayland Display",id:"object-detection-with-opencv--wayland-display",level:3},{value:"Reference Code",id:"reference-code-1",level:3}];function u(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"litert--tflite",children:"LiteRT / TFLite"})}),"\n",(0,s.jsx)(n.p,{children:"LiteRT, formerly known as TensorFlow Lite, is Google's high-performance runtime for on-device AI. You can run existing quantized LiteRT models (in Python or C++) on the NPU on Dragonwing devices with a single line of code using the LiteRT delegates that are part of AI Engine Direct."}),"\n",(0,s.jsx)(n.h2,{id:"quantizing-models",children:"Quantizing models"}),"\n",(0,s.jsxs)(n.p,{children:["The NPU only supports uint8/int8 quantized models. Unsupported models, or unsupported layers will be automatically moved back to the CPU. You can use ",(0,s.jsx)(n.a,{href:"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide",children:"quantization-aware training"})," or ",(0,s.jsx)(n.a,{href:"https://ai.google.dev/edge/litert/models/post_training_quantization",children:"post-training quantization"}),' to quantize your LiteRT models. Make sure you follow the steps for "Full integer quantization".']}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Don't want to quantize yourself?"})," You can download a range of pre-quantized models from ",(0,s.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"Qualcomm AI Hub"}),", or use ",(0,s.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/edge-impulse",children:"Edge Impulse"})," to quantize new or existing models."]})}),"\n",(0,s.jsx)(n.h2,{id:"running-a-model-on-the-npu-python",children:"Running a model on the NPU (Python)"}),"\n",(0,s.jsx)(n.p,{children:"To offload a model to the NPU, you just need to load the LiteRT delegate; and pass it into the interpreter. E.g.:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-py",children:'from ai_edge_litert.interpreter import Interpreter, load_delegate\n\nqnn_delegate = load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})\ninterpreter = Interpreter(\n    model_path=...,\n    experimental_delegates=[qnn_delegate]\n)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"running-a-model-on-the-npu-c",children:"Running a model on the NPU (C++)"}),"\n",(0,s.jsx)(n.p,{children:"To offload a model to the NPU, you'll first need to add the following compile flags:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-makefile",children:"CFLAGS += -I${QNN_SDK_ROOT}/include\nLDFLAGS += -L${QNN_SDK_ROOT}/lib/aarch64-ubuntu-gcc9.4 -lQnnTFLiteDelegate\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then, you instantiate the LiteRT delegate and pass it to the LiteRT interpreter:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-c",children:'// == Includes ==\n#include "QNN/TFLiteDelegate/QnnTFLiteDelegate.h"\n\n// == Application code ==\n\n// Get your interpreter...\ntflite::Interpreter *interpreter = ...;\n\n// Create QNN Delegate options structure.\nTfLiteQnnDelegateOptions options = TfLiteQnnDelegateOptionsDefault();\n\n// Set the mandatory backend_type option. All other options have default values.\noptions.backend_type = kHtpBackend;\n\n// Instantiate delegate. Must not be freed until interpreter is freed.\nTfLiteDelegate* delegate = TfLiteQnnDelegateCreate(&options);\n\nTfLiteStatus status = interpreter->ModifyGraphWithDelegate(delegate);\n// Check that status == kTfLiteOk\n'})}),"\n",(0,s.jsx)(n.h3,{id:"python-examples",children:"Python Examples"}),"\n",(0,s.jsxs)(r.A,{children:[(0,s.jsxs)(o.A,{value:"Example1",label:"Vision Transformers",children:[(0,s.jsx)(n.h3,{id:"vision-transformers",children:"Vision Transformers"}),(0,s.jsxs)(n.p,{children:["Here's how you can run a Vision Transformer model (downloaded from ",(0,s.jsx)(n.a,{href:"https://aihub.qualcomm.com/models/vit",children:"AI Hub"}),") on both the CPU and the NPU using the LiteRT delegates."]}),(0,s.jsx)(n.p,{children:"Open the terminal on your development board, or an ssh session to your development board, and:"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Create a new venv, and install the LiteRT runtime and Pillow:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir -p litert-demo/\ncd litert-demo/\n\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install ai-edge-litert==1.3.0 Pillow\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"inference_vit.py"})," and add:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-py",children:'import numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import Image\nimport os, time, sys\nimport urllib.request\n\ndef curr_ms():\n    return round(time.time() * 1000)\n\nuse_npu = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-npu\' else False\n\n# Path to your quantized TFLite model and test image (will be download automatically)\nMODEL_PATH = "vit-vit-w8a8.tflite"\nIMAGE_PATH = "boa-constrictor.jpg"\nLABELS_PATH = "vit-vit-labels.txt"\n\nif not os.path.exists(MODEL_PATH):\n    print("Downloading model...")\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-w8a8.tflite\'\n    urllib.request.urlretrieve(model_url, MODEL_PATH)\n\nif not os.path.exists(LABELS_PATH):\n    print("Downloading labels...")\n    labels_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-labels.txt\'\n    urllib.request.urlretrieve(labels_url, LABELS_PATH)\n\nif not os.path.exists(IMAGE_PATH):\n    print("Downloading image...")\n    image_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/examples/boa-constrictor.jpg\'\n    urllib.request.urlretrieve(image_url, IMAGE_PATH)\n\nwith open(LABELS_PATH, \'r\') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\nexperimental_delegates = []\nif use_npu:\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=experimental_delegates\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensor details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load and preprocess image\ndef load_image(path, input_shape):\n    # Expected input shape: [1, height, width, channels]\n    _, height, width, channels = input_shape\n\n    img = Image.open(path).convert("RGB").resize((width, height))\n    img_np = np.array(img, dtype=np.uint8)  # quantized models expect uint8\n    img_np = np.expand_dims(img_np, axis=0)\n    return img_np\n\ninput_shape = input_details[0][\'shape\']\ninput_data = load_image(IMAGE_PATH, input_shape)\n\n# Set tensor and run inference\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\n\n# Run once to warmup\ninterpreter.invoke()\n\n# Then run 10x\nstart = curr_ms()\nfor i in range(0, 10):\n    interpreter.invoke()\nend = curr_ms()\n\n# Get prediction\nq_output = interpreter.get_tensor(output_details[0][\'index\'])\nscale, zero_point = output_details[0][\'quantization\']\nf_output = (q_output.astype(np.float32) - zero_point) * scale\n\n# Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\ndef softmax(x, axis=-1):\n    # subtract max for numerical stability\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n# show top-5 predictions\nscores = softmax(f_output[0])\ntop_k = scores.argsort()[-5:][::-1]\nprint("\\nTop-5 predictions:")\nfor i in top_k:\n    print(f"Class {labels[i]}: score={scores[i]}")\n\nprint(\'\')\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the model on the CPU:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py\n\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6264431476593018\n# Class rock python: score=0.047579940408468246\n# Class night snake: score=0.006721484009176493\n# Class mouse: score=0.0022421202156692743\n# Class pick: score=0.001942973816767335\n#\n# Inference took (on average): 391.1ms. per image\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Run the model on the NPU:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py --use-npu\n\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\n#\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6113042235374451\n# Class rock python: score=0.038359832018613815\n# Class night snake: score=0.011630792170763016\n# Class mouse: score=0.002294909441843629\n# Class lens cap: score=0.0018960189772769809\n#\n# Inference took (on average): 132.7ms. per image\n"})}),"\n"]}),"\n"]}),(0,s.jsx)(n.p,{children:'As you can see this model runs significantly faster on NPU - but there\'s a slight change in the output of the model. You can also see that for this model not all layers can run on NPU ("1382 nodes delegated out of 1633 nodes with 27 partitions").'})]}),(0,s.jsxs)(o.A,{value:"Example2",label:"Image Classification",children:[(0,s.jsx)(n.h3,{id:"gtk-based-image-classification-app",children:"GTK-Based Image Classification App"}),(0,s.jsxs)(n.p,{children:["Here's how you can use a GTK-based desktop application to run an image classification model\u2014downloaded from ",(0,s.jsx)(n.a,{href:"https://aihub.qualcomm.com/iot/models/googlenet?searchTerm=google&chipsets=qualcomm-qcs6490-proxy",children:"(AI Hub)"})," on both the CPU and NPU using LiteRT delegates from AI Engine Direct."]}),(0,s.jsx)(n.p,{children:"The model uses TensorFlow Lite with QNN delegate acceleration for efficient on-device inference."}),(0,s.jsxs)(n.p,{children:["Since this model isn\u2019t available on Edge Impulse, download it with precision ",(0,s.jsx)(n.code,{children:"w8a8"})," and set the runtime to ",(0,s.jsx)(n.code,{children:"TFLite"}),"."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{src:t(24391).A+"",width:"1330",height:"721"}),"\nThe next step is to push the model on to the target device",(0,s.jsx)(n.br,{}),"\n","Through scp command copy the model and image onto the device."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"scp xxxx.tflite ubuntu@IP_address:/home/ubuntu/\nscp xxx.jpg ubuntu@IP_address:/home/ubuntu/\n"})}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Download any image from internet. In this example we used firetruck image."})}),(0,s.jsx)(n.p,{children:"Open the terminal on your development board, or an ssh session to your development board, and:"}),(0,s.jsx)(n.p,{children:"Create a new venv, and install the LiteRT runtime and Pillow:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"python3 -m venv .venv-litert-demo\nsource .venv-litert-demo/bin/activate\npip3 install ai-edge-litert==1.3.0 Pillow\npip3 install opencv-python\nsudo apt install python3-gi python3-gi-cairo gir1.2-gtk-3.0\nsudo apt install python3-venv python3-full\nsudo apt install -y pkg-config cmake libcairo2-dev\nsudo apt install libgirepository1.0-dev gir1.2-glib-2.0\nsudo apt install build-essential python3-dev python3-pip pkg-config meson\n"})}),(0,s.jsxs)(n.p,{children:["Now follow the steps to create Image classification application.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"Summary"}),(0,s.jsx)(n.br,{}),"\n","\u2022  Type: Desktop GUI application",(0,s.jsx)(n.br,{}),"\n","\u2022  Functionality: Image classification using TFLite",(0,s.jsx)(n.br,{}),"\n","\u2022  Modes: CPU and QNN delegate",(0,s.jsx)(n.br,{}),"\n","\u2022  Interface: GTK-based GUI",(0,s.jsx)(n.br,{}),"\n","\u2022  Output: Top predictions with confidence bars"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Environment Setup & Imports"}),(0,s.jsx)(n.br,{}),"\n","In the step the script sets up display-related environment variables (for Linux systems) and imports necessary libraries like OpenCV, NumPy, GTK, and TensorFlow Lite."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2, numpy as np, os, time\nfrom gi.repository import Gtk, GLib, GdkPixbuf\nimport ai_edge_litert.interpreter as tflite\n"})}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"\u2022\tGTK is used for the GUI, OpenCV for image handling, and TensorFlow Lite for inference."})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Configuration Constants"}),(0,s.jsx)(n.br,{}),"\n","These constants define paths to the model, label file, and delegate library."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'TF_MODEL = "googlenet-googlenet-w8a8.tflite"\nLABELS = "imagenet_labels.txt"\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\nDEVICE_OS = "Ubuntu"\n'})}),(0,s.jsx)(n.h3,{id:"helper-functions",children:"Helper Functions"}),(0,s.jsx)(n.p,{children:"This step sets up the core logic and interface for image classification using LiteRT and GTK."}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Softmax Calculation"}),(0,s.jsx)(n.br,{}),"\n","Ensures numerical stability when converting logits to probabilities."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef stable_softmax(logits):    \nlogits = logits.astype(np.float32)    \nshifted_logits = np.clip(logits - np.max(logits), -500, 500)    \nexp_scores = np.exp(shifted_logits)    \nreturn exp_scores / np.sum(exp_scores)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Label Loader"}),(0,s.jsx)(n.br,{}),"\n","Loads class labels from a text file."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef load_labels(label_path):    \nwith open(label_path, 'r') as f: \n       return [line.strip() for line in f.readlines()]\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Image Preprocessing"}),(0,s.jsx)(n.br,{}),"\n","Prepares the image for model input: resizing, color conversion, and reshaping."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"Pythondef preprocess_image(image_path, input_shape, input_dtype):   \nimg = cv2.imread(image_path)    \nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    \nimg = cv2.resize(img, (input_shape[2], input_shape[1]))    \nimg = img.astype(input_dtype)    \nreturn np.expand_dims(img, axis=0)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Inference Execution"}),(0,s.jsx)(n.br,{}),"\n","This function will:",(0,s.jsx)(n.br,{}),"\n","\u2022\tLoads the model (with or without delegate)",(0,s.jsx)(n.br,{}),"\n","\u2022\tPrepares input",(0,s.jsx)(n.br,{}),"\n","\u2022\tRuns inference",(0,s.jsx)(n.br,{}),"\n","\u2022\tApplies softmax",(0,s.jsx)(n.br,{}),"\n","\u2022\tReturns top 4 predictions with confidence scores"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def runInference(image, use_delegate):\n    if use_delegate:\n        try:\n            delegate = tflite.load_delegate(DELEGATE_PATH, {'backend_type': 'htp'})\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\n        except:\n            model = tflite.Interpreter(model_path=TF_MODEL)\n    else:\n        model = tflite.Interpreter(model_path=TF_MODEL)\n    model.allocate_tensors()\n    input_details = model.get_input_details()\n    input_data = preprocess_image(image, input_details[0]['shape'], input_details[0]['dtype'])\n    model.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    model.invoke()\n    inference_time = time.time() - start_time\n    output_data = model.get_tensor(model.get_output_details()[0]['index'])\n    probabilities = stable_softmax(output_data[0])\n    labels = load_labels(LABELS)\n    top_indices = np.argsort(probabilities)[::-1][:4]\n    results = [(labels[i], probabilities[i] * 100) for i in top_indices]\n    return results, inference_time\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GTK GUI Components"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"File Browser Dialog"}),(0,s.jsx)(n.br,{}),"\n","Allows users to select an image file."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class FileBrowser(Gtk.FileChooserDialog):\n    def __init__(self):\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\n    def run_and_get_file(self):\n        if self.run() == Gtk.ResponseType.OK:\n            return self.get_filename()\n        self.destroy()\n'})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Main Window"}),(0,s.jsx)(n.br,{}),"\n","The GUI includes:",(0,s.jsx)(n.br,{}),"\n","\u2022\tImage display area",(0,s.jsx)(n.br,{}),"\n","\u2022\tRadio buttons to choose CPU or delegate",(0,s.jsx)(n.br,{}),"\n","\u2022\tButtons to select and reprocess image",(0,s.jsx)(n.br,{}),"\n","\u2022\tResult display with labels and progress bars"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MainWindow(Gtk.Window):\n    def __init__(self):\n        super().__init__(title="Image Classification")\n        self.set_default_size(800, 600)\n        self.imageFilepath = ""\n        ...\n'})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Image Processing and Display"}),(0,s.jsx)(n.br,{}),"\n","This method:",(0,s.jsx)(n.br,{}),"\n","\u2022\tResizes and displays the image",(0,s.jsx)(n.br,{}),"\n","\u2022\tRuns inference",(0,s.jsx)(n.br,{}),"\n","\u2022\tDisplays results with progress bars and inference time"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def process_file(self, filepath):\n    pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\n    new_width, new_height = resizeImage(pixbuf)\n    scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\n    self.image.set_from_pixbuf(scaled_pixbuf)\n\n    results, inference_time = runInference(filepath, self.use_delegate())\n    ...\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Application Entry Point"}),(0,s.jsx)(n.br,{}),"\n","Initializes and launches the GTK application."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def main():\n    app = MainWindow()\n    app.connect("destroy", Gtk.main_quit)\n    app.show_all()\n    Gtk.main()\nif __name__ == "__main__":\n    success, _ = Gtk.init_check()\n    if not success:\n        print("GTK could not be initialized.")\n        exit(1)\n    main()\n'})}),(0,s.jsx)(n.h3,{id:"reference-code",children:"Reference Code"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# -----------------------------------------------------------------------------\n#\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# -----------------------------------------------------------------------------\n\nimport cv2\nimport gi\nimport numpy as np\nimport os\nos.environ[\'xDG_RUNTIME_DIR\'] = \'/run/user/1000/\'\nos.environ[\'WAYLAND_DISPLAY\'] = \'wayland-1\'\nos.environ[\'DISPLAY\'] = \':0\'\nimport time\ngi.require_version("Gtk", "3.0")\nfrom gi.repository import Gtk, GLib, GdkPixbuf\n\n# ========= Constants =========\nTF_MODEL = "googlenet-googlenet-w8a8.tflite"\nLABELS = "/etc/labels/imagenet_labels.txt"\nDELEGATE_PATH = "libQnnTFLiteDelegate.so"\nDEVICE_OS="Ubuntu"\nUNAME = os.uname().nodename\n\nimport ai_edge_litert.interpreter as tflite\n\n# ========= Helper Functions =========\ndef stable_softmax(logits):\n    # Convert logits to float64 for higher precision\n    logits = logits.astype(np.float32)\n    \n    # Subtract the maximum logit to prevent overflow\n    shifted_logits = logits - np.max(logits)\n    \n    # Clip the shifted logits to a safe range to prevent overflow in exp\n    shifted_logits = np.clip(shifted_logits, -500, 500)\n    \n    # Calculate the exponentials and normalize\n    exp_scores = np.exp(shifted_logits)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    return probabilities\n\n# Load labels from file\ndef load_labels(label_path):\n    with open(label_path, \'r\') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef resizeImage(pixbuf):\n    original_width = pixbuf.get_width()\n    original_height = pixbuf.get_height()\n\n    # Target display size\n    max_width = 800\n    max_height = 600\n\n    # Calculate new size preserving aspect ratio\n    scale = min(max_width / original_width, max_height / original_height)\n    new_width = int(original_width * scale)\n    new_height = int(original_height * scale)\n\n    return new_width, new_height\n\n# Load and preprocess input image\ndef preprocess_image(image_path, input_shape, input_dtype):\n    # Read the image using OpenCV\n    img = cv2.imread(image_path)\n    if img is None:\n        raise ValueError(f"Failed to load image at {image_path}")\n    # Convert BGR to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    # Resize the image to the desired input shape\n    img = cv2.resize(img, (input_shape[2], input_shape[1]))\n    # Convert to the desired data type\n    img = img.astype(input_dtype)\n    # Add batch dimension\n    img = np.expand_dims(img, axis=0)\n    \n    return img\n\n# ====== Inference Function ======\ndef runInference(image, use_delegate):\n    results = []    \n    print(f"Running on {DEVICE_OS} using Delegate:{use_delegate}")\n    if use_delegate:\n        try:\n            # Load the QNN delegate library\n            delegate_options = { \'backend_type\': \'htp\' }\n            delegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\n            \n            # Load the TFLite model\n            model = tflite.Interpreter(model_path=TF_MODEL, experimental_delegates=[delegate])\n            print("INFO: Loaded QNN delegate with HTP backend")\n        except Exception as e:\n            print(f"WARNING: Failed to load QNN delegate: {e}")\n            print("INFO: Continuing without QNN delegate")\n            model = tflite.Interpreter(model_path=TF_MODEL)   \n    else:\n        model = tflite.Interpreter(model_path=TF_MODEL)  \n   \n    model.allocate_tensors()\n\n    # Get and Prepare input \n    input_details = model.get_input_details()\n    input_shape = input_details[0][\'shape\']\n    input_dtype = input_details[0][\'dtype\']\n    input_data = preprocess_image(image, input_shape, input_dtype)\n    \n    # Load input data to input tensor\n    model.set_tensor(input_details[0][\'index\'], input_data)\n    model.get_signature_list()\n    \n    # Run inference\n    try:\n        start_time = time.time()\n        model.invoke()\n        end_time = time.time()\n        print("Interpreter invoked successfully.")\n    except Exception as e:\n        print(f"Error during model invocation: {e}")\n        return []\n\n    # Calculate and print duration\n    inference_time = end_time - start_time\n\n    # Prepare output tensor details\n    output_details = model.get_output_details()\n\n    # Load output data to output tensor\n    output_data = model.get_tensor(output_details[0][\'index\'])\n\n    # Load labels and get prediction\n    labels = load_labels(LABELS)\n    predicted_index = np.argmax(output_data)\n    predicted_label = labels[predicted_index]\n    print("Predicted index:", predicted_index)\n    print("Predicted label:", predicted_label)\n    \n    # Add Softmax function\n    logits = output_data[0]\n    probabilities = stable_softmax(logits)\n\n    # Get top 4 predictions\n    top_k = 4\n    top_indices = np.argsort(probabilities)[::-1][:top_k]\n    for i in top_indices:\n        result = (labels[i], probabilities[i] * 100)\n        results.append(result)\n\n    return results, inference_time\n\n# ====== GTK GUI Classes ======\nclass FileBrowser(Gtk.FileChooserDialog):\n    def __init__(self):\n        super().__init__(title="Choose an image", action=Gtk.FileChooserAction.OPEN)\n        self.add_buttons(Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK)\n\n    def run_and_get_file(self):\n        response = super().run()\n        if response == Gtk.ResponseType.OK:\n            print("Selected file:", self.get_filename())\n            self.selected_file = self.get_filename()            \n        self.destroy()\n        return self.selected_file\n\nclass MainWindow(Gtk.Window):\n    def __init__(self):\n        super().__init__(title="Image Classification")\n        self.set_default_size(800, 600)\n        self.imageFilepath = ""\n        # Main layout\n        self.mainBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n        self.mainBox.set_margin_top(10)\n        self.mainBox.set_margin_bottom(10)\n        self.mainBox.set_margin_start(10)\n        self.mainBox.set_margin_end(10)\n        self.add(self.mainBox)\n        \n        # Main Window Image setup with fallback\n        self.image = Gtk.Image()\n        try:\n            MAIN_IMAGE = "MainWindowPic.jpg"\n            self.image.set_from_file(MAIN_IMAGE)         \n        except Exception as e:\n            print("Error loading main image:", e)\n            self.image.set_from_icon_name("image-missing", Gtk.IconSize.DIALOG)\n\n        self.mainBox.pack_start(self.image, True, True, 0)\n\n        # Set up a new box to add results and and file button\n        self.infoBox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\n        \n        # Radio button to select Delegate\n        delegate_label = Gtk.Label(label="Select Inference Mode:")\n        self.infoBox.pack_start(delegate_label, False, False, 10)\n\n        self.cpu_radio = Gtk.RadioButton.new_with_label_from_widget(None, "CPU")\n        self.delegate_radio = Gtk.RadioButton.new_with_label_from_widget(self.cpu_radio, "Delegate")\n\n        self.infoBox.pack_start(self.cpu_radio, False, False, 0)\n        self.infoBox.pack_start(self.delegate_radio, False, False, 0)\n        \n        # Radio button signal\n        self.cpu_radio.connect("toggled", self.on_radio_toggled)\n        self.delegate_radio.connect("toggled", self.on_radio_toggled)\n\n        # Open file button\n        open_button = Gtk.Button(label="Select Image")\n        open_button.connect("clicked", self.on_open_file_clicked)\n        self.infoBox.pack_start(open_button, False, True, 10)\n\n        # Reprocess Image\n        reprocess_button = Gtk.Button(label="Reprocess Image")\n        reprocess_button.connect("clicked", self.on_reprocess_image_clicked)\n        self.infoBox.pack_start(reprocess_button, False, True, 10)\n\n        # Classification results\n        self.results = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=10)\n        self.infoBox.pack_start(self.results, True, True, 0)\n        self.mainBox.pack_start(self.infoBox, True, True, 0)\n\n    def use_delegate(self):\n        return self.delegate_radio.get_active()\n\n    def on_radio_toggled(self, button):\n        if button.get_active():\n            print(f"Selected option: {button.get_label()}")\n\n    def process_file(self, filepath): \n        try:\n            # Resize Image\n            pixbuf = GdkPixbuf.Pixbuf.new_from_file(filepath)\n            new_width, new_height = resizeImage(pixbuf)\n            scaled_pixbuf = pixbuf.scale_simple(new_width, new_height, GdkPixbuf.InterpType.BILINEAR)\n            \n            # Replace the image with new image\n            self.image.set_from_pixbuf(scaled_pixbuf)\n           \n            # Run Inference\n            use_delegate = self.use_delegate()\n            print("delegate: " , use_delegate)\n            options, inference_time = runInference(filepath, use_delegate)\n\n            # Clear result box\n            for child in self.results.get_children():\n                self.results.remove(child)\n            \n            # Set up predictions\n            for label, percent in options:\n                textBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n                barBox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=10)\n                text = Gtk.Label(label=label, xalign=0)\n                text.set_size_request(100, -1) \n                \n                bar = Gtk.ProgressBar()\n                bar.set_fraction(percent / 100.0)\n                bar.set_text(f"{percent:.2f}%")\n                bar.set_show_text(True)\n                \n                textBox.pack_start(text, False, False, 0)\n                barBox.pack_start(bar, True, True, 0)\n            \n                self.results.pack_start(textBox, False, False, 0)\n                self.results.pack_start(barBox, False, False, 0)\n                self.results.show_all()\n            \n            # Add inference time label\n            time_label = Gtk.Label(label=f"Inference Time : {inference_time * 1000:.2f} ms")\n            self.results.pack_start(time_label, False, False, 50)\n            self.results.show_all()\n        except Exception as e:\n            print("Error reading file:", e)\n\n    def on_open_file_clicked(self, widget):\n        dialog = FileBrowser()\n        selected_file = dialog.run_and_get_file()\n        self.imageFilepath = selected_file\n        if selected_file:\n            self.process_file(selected_file)\n \n    def on_reprocess_image_clicked(self, widget):\n        self.process_file(self.imageFilepath)\n\n    def on_destroy(self, widget):\n        Gtk.main_quit()\n\n# === Main Entry Point ===\ndef main():\n    app = MainWindow()\n    app.connect("destroy", Gtk.main_quit)\n    app.show_all()\n    Gtk.main()\n\nif __name__ == "__main__":\n    success, _ = Gtk.init_check()\n    if not success:\n        print("GTK could not be initialized. Check environmental variables")\n        exit(1)\n\n    main() \n\n'})}),(0,s.jsx)(n.p,{children:"Now you're set up to run the application on either CPU/delegate using the command below:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"python3 classification.py\n"})}),(0,s.jsx)(n.p,{children:"Select CPU as your runtime option on the GUI:"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(1383).A+"",width:"4032",height:"3024"})}),(0,s.jsx)(n.p,{children:"Select delegate as your runtime option on the GUI:"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(81668).A+"",width:"800",height:"600"})})]}),(0,s.jsxs)(o.A,{value:"Example3",label:"Object Detection",children:[(0,s.jsx)(n.h3,{id:"object-detection-with-opencv--wayland-display",children:"Object Detection with OpenCV & Wayland Display"}),(0,s.jsxs)(n.p,{children:["This Python script performs ",(0,s.jsx)(n.strong,{children:"real-time object detection"})," on a video file using a quantized ",(0,s.jsx)(n.strong,{children:"YOLOv8 TensorFlow Lite model"})," and displays the annotated frames via ",(0,s.jsx)(n.strong,{children:"GStreamer on a Wayland display"}),". It is optimized for ",(0,s.jsx)(n.strong,{children:"edge AI scenarios"})," using hardware acceleration through the ",(0,s.jsx)(n.strong,{children:"QNN TFLite delegate"}),"."]}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["The YOLOv8 model isn't available by default. Please follow the ",(0,s.jsx)(n.strong,{children:"Step-6"})," from ",(0,s.jsx)(n.a,{href:"https://docs.qualcomm.com/bundle/publicresource/topics/80-70020-50/download-model-and-label-files.html?vproduct=1601111740013072&version=1.5&facet=Intelligent_Multimedia_SDK.SDK.2.0",children:(0,s.jsx)(n.strong,{children:"Qualcomm Intelligent Multimedia SDK"})})," to export YoloV8 quantized model."]})}),(0,s.jsx)(n.p,{children:"The next step is to push the model on to the target device\nThrough scp command copy the model onto the device."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"scp xxxx.tflite ubuntu@IP_address:/home/ubuntu/\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Initialization and Configuration"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tPaths for the model, labels, input video, and delegate are defined.",(0,s.jsx)(n.br,{}),"\n","\u2022\tConstants like frame dimensions, FPS, confidence threshold, and scaling factors are set for preprocessing and postprocessing."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"MODEL_PATH"}),' = "yolov8_det_quantized.tflite"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"LABEL_PATH"}),' = "coco_labels.txt"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"VIDEO_IN"}),' = "video.mp4"',(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"DELEGATE_PATH"}),' = "libQnnTFLiteDelegate.so"']}),(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsxs)(n.p,{children:["Use a video file that works well with object detection models.",(0,s.jsx)(n.br,{}),"\n","For best results, choose videos with clear subjects, good lighting, and minimal motion blur."]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Examples:"}),(0,s.jsx)(n.br,{}),"\n","\u2022 A street scene with vehicles and pedestrians",(0,s.jsx)(n.br,{}),"\n","\u2022 A warehouse or factory floor with visible objects",(0,s.jsx)(n.br,{}),"\n","\u2022 A static camera feed showing people or products"]})]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Loading and Delegate Setup"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tLoads the hardware delegate for accelerated inference.",(0,s.jsx)(n.br,{}),"\n","\u2022\tInitializes the TensorFlow Lite interpreter with the quantized YOLOv8 model."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"delegate_options = { 'backend_type': 'htp' }\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\ninterpreter.allocate_tensors()\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Label Loading"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tLoads COCO dataset labels for object annotation."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"labels = [l.strip() for l in open(LABEL_PATH)]\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GStreamer Pipeline Setup"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tCreates a GStreamer pipeline using appsrc to stream frames to a Wayland sink.\n\u2022\tEnables real-time display of processed frames."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"pipeline = Gst.parse_launch(\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink')\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Video Capture and Frame Processing"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tOpens the video file using OpenCV.",(0,s.jsx)(n.br,{}),"\n","\u2022\tEach frame is resized and preprocessed to match the model\u2019s input dimensions."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"cap = cv2.VideoCapture(VIDEO_IN)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Inference and Post-processing"}),(0,s.jsx)(n.br,{}),"\n","\u2022\tRuns inference on each frame.",(0,s.jsx)(n.br,{}),"\n","\u2022\tDequantizes the model outputs using predefined scales and zero-points."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"interpreter.set_tensor(in_det[0]['index'], input_tensor)\ninterpreter.invoke()\nboxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\nscores_q = interpreter.get_tensor(out_det[1]['index'])[0]\nclasses_q = interpreter.get_tensor(out_det[2]['index'])[0]\n"})}),(0,s.jsx)(n.p,{children:"\u2022\tApplies a confidence threshold to filter low-probability detections.\n\u2022\tUses Non-Maximum Suppression (NMS) to remove overlapping boxes."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"mask = scores >= CONF_THRES\nboxes_f = boxes[mask]\nscores_f = scores[mask]\nclasses_f = classes[mask]\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Annotation and Display"})}),(0,s.jsx)(n.p,{children:"\u2022\tDraws bounding boxes and labels on the frame using OpenCV.\n\u2022\tLogs the highest detection score every 100 frames."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\ncv2.putText(frame_rs, f"{lab} {sc:.2f}", (x1i, max(10,y1i-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Streaming to Wayland Display"})}),(0,s.jsx)(n.p,{children:"Converts frames to GStreamer buffers and pushes them to the pipeline with timestamps for smooth playback."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"buf = Gst.Buffer.new_allocate(None, len(data), None)\nbuf.fill(0, data)\nbuf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\ntimestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\nbuf.pts = buf.dts = int(timestamp)\nappsrc.emit('push-buffer', buf)\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Completion"}),(0,s.jsx)(n.br,{}),"\n","Gracefully shuts down the pipeline after processing all frames."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"appsrc.emit('end-of-stream')\npipeline.set_state(Gst.State.NULL)\ncap.release()\n"})}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),(0,s.jsx)(n.br,{}),"\n","This script is ideal for:",(0,s.jsx)(n.br,{}),"\n","\u2022\tSmart cameras",(0,s.jsx)(n.br,{}),"\n","\u2022\tRobotics",(0,s.jsx)(n.br,{}),"\n","\u2022\tEmbedded systems with Wayland-based GUIs",(0,s.jsx)(n.br,{}),"\n","\u2022\tReal-time monitoring in edge AI deployments"]}),(0,s.jsx)(n.h3,{id:"reference-code-1",children:"Reference Code"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# -----------------------------------------------------------------------------\n#\n# Copyright (c) Qualcomm Technologies, Inc. and/or its subsidiaries.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# -----------------------------------------------------------------------------\n\n#!/usr/bin/env python3\n\n# Import necessary libraries\nimport cv2\nimport numpy as np\nimport gi\ngi.require_version('Gst', '1.0')\nfrom gi.repository import Gst\nimport ai_edge_litert.interpreter as tflite\n\n# Initialize GStreamer\nGst.init(None)\n\n# -------------------- Parameters --------------------\nMODEL_PATH = \"/home/ubuntu/yolov8_det_quantized.tflite\"  # Path to TFLite model\nLABEL_PATH = \"/home/ubuntu/coco_labels.txt\"              # Path to label file\nVIDEO_IN = \"/etc/media/video.mp4\"                        # Input video file\nDELEGATE_PATH = \"libQnnTFLiteDelegate.so\"                # Delegate for hardware acceleration\n\n# Frame and model parameters\nFRAME_W, FRAME_H = 1600, 900\nFPS_OUT = 30\nCONF_THRES = 0.25\nNMS_IOU_THRES = 0.50\nBOX_SCALE = 3.2108588218688965\nBOX_ZP = 31.0\nSCORE_SCALE = 0.0038042240776121616\n# -------------------- Load Model --------------------\n# Load delegate for hardware acceleration\ndelegate_options = { 'backend_type': 'htp' }\ndelegate = tflite.load_delegate(DELEGATE_PATH, delegate_options)\n\n# Load and allocate TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=MODEL_PATH, experimental_delegates=[delegate])\ninterpreter.allocate_tensors()\n\n# Get input/output tensor details\nin_det = interpreter.get_input_details()\nout_det = interpreter.get_output_details()\nin_h, in_w = in_det[0][\"shape\"][1:3]\n\n# -------------------- Load Labels --------------------\nlabels = [l.strip() for l in open(LABEL_PATH)]\n\n# -------------------- GStreamer Pipeline --------------------\n# Create GStreamer pipeline to display video via Wayland\npipeline = Gst.parse_launch(\n    'appsrc name=src is-live=true block=true format=time caps=video/x-raw,format=BGR,width=1600,height=900,framerate=30/1 ! videoconvert ! waylandsink'\n)\nappsrc = pipeline.get_by_name('src')\npipeline.set_state(Gst.State.PLAYING)\n\n# -------------------- Video Input --------------------\ncap = cv2.VideoCapture(VIDEO_IN)\n\n# Scaling factors for bounding box adjustment\nsx, sy = FRAME_W / in_w, FRAME_H / in_h\n\n# Preallocate frame buffers\nframe_rs = np.empty((FRAME_H, FRAME_W, 3), np.uint8)\ninput_tensor = np.empty((1, in_h, in_w, 3), np.uint8)\n\nframe_cnt = 0\n\n# -------------------- Main Loop --------------------\nwhile True:\n    ok, frame = cap.read()\n    if not ok:\n        break\n    frame_cnt += 1\n\n    # ---------- Preprocessing ----------\n    # Resize frame to display resolution\n    cv2.resize(frame, (FRAME_W, FRAME_H), dst=frame_rs)\n\n    # Resize again to model input resolution\n    cv2.resize(frame_rs, (in_w, in_h), dst=input_tensor[0])\n\n    # ---------- Inference ----------\n    # Set input tensor and run inference\n    interpreter.set_tensor(in_det[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    # ---------- Postprocessing ----------\n    # Get raw output tensors\n    boxes_q = interpreter.get_tensor(out_det[0]['index'])[0]\n    scores_q = interpreter.get_tensor(out_det[1]['index'])[0]\n    classes_q = interpreter.get_tensor(out_det[2]['index'])[0]\n\n    # Dequantize outputs\n    boxes = BOX_SCALE * (boxes_q.astype(np.float32) - BOX_ZP)\n    scores = SCORE_SCALE * scores_q.astype(np.float32)\n    classes = classes_q.astype(np.int32)\n\n    # Filter by confidence threshold\n    mask = scores >= CONF_THRES\n    if np.any(mask):\n        boxes_f = boxes[mask]\n        scores_f = scores[mask]\n        classes_f = classes[mask]\n\n        # Convert boxes to OpenCV format\n        x1, y1, x2, y2 = boxes_f.T\n        boxes_cv2 = np.column_stack((x1, y1, x2 - x1, y2 - y1))\n\n        # Apply Non-Maximum Suppression\n        idx_cv2 = cv2.dnn.NMSBoxes(\n            bboxes=boxes_cv2.tolist(),\n            scores=scores_f.tolist(),\n            score_threshold=CONF_THRES,\n            nms_threshold=NMS_IOU_THRES\n        )\n\n        if len(idx_cv2):\n            idx = idx_cv2.flatten()\n            sel_boxes = boxes_f[idx]\n            sel_scores = scores_f[idx]\n            sel_classes = classes_f[idx]\n\n            # Debug print every 100 frames\n            if frame_cnt % 100 == 0:\n                print(f\"[{frame_cnt:4d}] max score = {sel_scores.max():.3f}\")\n\n            # Rescale boxes to display resolution\n            sel_boxes[:, [0,2]] *= sx\n            sel_boxes[:, [1,3]] *= sy\n            sel_boxes = sel_boxes.astype(np.int32)\n\n            # Clip boxes to frame boundaries\n            sel_boxes[:, [0,2]] = np.clip(sel_boxes[:, [0,2]], 0, FRAME_W-1)\n            sel_boxes[:, [1,3]] = np.clip(sel_boxes[:, [1,3]], 0, FRAME_H-1)\n\n            # Draw boxes and labels\n            for (x1i, y1i, x2i, y2i), sc, cl in zip(sel_boxes, sel_scores, sel_classes):\n                cv2.rectangle(frame_rs, (x1i, y1i), (x2i, y2i), (0,255,0), 2)\n                lab = labels[cl] if cl < len(labels) else str(cl)\n                cv2.putText(frame_rs, f\"{lab} {sc:.2f}\", (x1i, max(10,y1i-5)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n\n    # ---------- Video Output ----------\n    # Convert frame to bytes and push to GStreamer pipeline\n    data = frame_rs.tobytes()\n    buf = Gst.Buffer.new_allocate(None, len(data), None)\n    buf.fill(0, data)\n    buf.duration = Gst.util_uint64_scale_int(1, Gst.SECOND, FPS_OUT)\n    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) * Gst.MSECOND\n    buf.pts = buf.dts = int(timestamp)\n    appsrc.emit('push-buffer', buf)\n\n# -------------------- Finish --------------------\nappsrc.emit('end-of-stream')\npipeline.set_state(Gst.State.NULL)\ncap.release()\nprint(\"Done \u2013 video streamed to Wayland sink\")\n"})}),(0,s.jsx)(n.p,{children:"Now you're set up to run the application on NPU(delegate) using the command below:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"python3 ObjectDetection.py\n"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(78581).A+"",width:"975",height:"548"})})]})]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},78581:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/OpenCV-OD-66beef4f3ce80e7fddf5a740b7706421.png"},81668:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/IC-LiteRT-NPU-b2d61dda8093207893370e151a3f9838.jpeg"}}]);