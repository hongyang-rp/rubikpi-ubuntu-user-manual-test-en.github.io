"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2215],{28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(96540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}},91101:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx","title":"ONNX","description":"ONNX (Open Neural Network Exchange) is a standard format for exporting models \u2014 typically created in frameworks like PyTorch \u2014 so they can run anywhere. On Dragonwing devices you can use ONNX Runtime with AI Engine Direct to execute ONNX models directly on the NPU for maximum performance.","source":"@site/docs/Document Home/3.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/4.onnx.md","sourceDirName":"Document Home/3.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/3.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/4.onnx.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LiteRT / TFLite","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite"},"next":{"title":"Llama.cpp","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"}}');var o=r(74848),t=r(28453);const s={},a="ONNX",l={},d=[{value:"onnxruntime wheel with AI Engine Direct",id:"onnxruntime-wheel-with-ai-engine-direct",level:2},{value:"Preparing your onnx file",id:"preparing-your-onnx-file",level:2},{value:"Dynamic shapes",id:"dynamic-shapes",level:3},{value:"Quantizing models",id:"quantizing-models",level:3},{value:"Running a model on the NPU (Python)",id:"running-a-model-on-the-npu-python",level:2},{value:"Example: SqueezeNet-1.1 (Python)",id:"example-squeezenet-11-python",level:2},{value:"Tips &amp; tricks",id:"tips--tricks",level:2},{value:"Disable CPU fallback",id:"disable-cpu-fallback",level:3},{value:"Building new versions of the the onnxruntime package",id:"building-new-versions-of-the-the-onnxruntime-package",level:3}];function u(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"onnx",children:"ONNX"})}),"\n",(0,o.jsx)(n.p,{children:"ONNX (Open Neural Network Exchange) is a standard format for exporting models \u2014 typically created in frameworks like PyTorch \u2014 so they can run anywhere. On Dragonwing devices you can use ONNX Runtime with AI Engine Direct to execute ONNX models directly on the NPU for maximum performance."}),"\n",(0,o.jsx)(n.h2,{id:"onnxruntime-wheel-with-ai-engine-direct",children:"onnxruntime wheel with AI Engine Direct"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"onnxruntime"})," currently does not publish prebuilt wheels for aarch64 Linux with AI Engine Direct bindings - so you cannot install onnxruntime through ",(0,o.jsx)(n.code,{children:"pip"}),". You can download prebuilt wheels here:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://cdn.edgeimpulse.com/qc-ai-docs/wheels/onnxruntime_qnn-1.23.0-cp312-cp312-linux_aarch64.whl",children:"onnxruntime_qnn-1.23.0-cp312-cp312-linux_aarch64.whl"})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["(Install via ",(0,o.jsx)(n.code,{children:"pip3 install onnxruntime_qnn-*-linux_aarch64.whl"}),")"]}),"\n",(0,o.jsxs)(n.p,{children:["To build a wheel for other onnxruntime or Python versions, see ",(0,o.jsx)(n.a,{href:"https://github.com/edgeimpulse/onnxruntime-qnn-linux-aarch64",children:"edgeimpulse/onnxruntime-qnn-linux-aarch64"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"preparing-your-onnx-file",children:"Preparing your onnx file"}),"\n",(0,o.jsx)(n.p,{children:"The NPU only supports quantized uint8/int8 models with a fixed input shape. If your model is not quantized, or if you have a dynamic input shape your model will automatically be offloaded to the CPU. Here's some tips on how to prepare your model."}),"\n",(0,o.jsxs)(n.p,{children:["A full length tutorial for exporting a PyTorch model to ONNX is ",(0,o.jsx)(n.a,{href:"https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html",children:"available in the PyTorch documentation"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"dynamic-shapes",children:"Dynamic shapes"}),"\n",(0,o.jsxs)(n.p,{children:["If you have a model with dynamic shapes, you'll need to make them fixed shape first. You can see the shape of your network via ",(0,o.jsx)(n.a,{href:"https://netron.app",children:"Netron"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"For example, this model has dynamic shapes:"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-7a24b95e775b9580b371704771b840abc92fb72e%2Fonnxruntime1.png?alt=media",alt:"",title:"A model with dynamic shape"})}),"\n",(0,o.jsxs)(n.p,{children:["You can set a fixed shape via ",(0,o.jsx)(n.code,{children:"onnxruntime.tools.make_dynamic_shape_fixed"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3 -m onnxruntime.tools.make_dynamic_shape_fixed \\\r\n    model_without_shapes.onnx \\\r\n    model_with_shapes.onnx \\\r\n    --input_name pixel_values \\\r\n    --input_shape 1,3,224,224\n"})}),"\n",(0,o.jsx)(n.p,{children:"Afterwards your model has a fixed shape and is ready to run on your NPU."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-fe2eb378e26e2e6583bc00e0583713a98ba1bd88%2Fonnxruntime2.png?alt=media",alt:"",title:"An ONNX model with a fixed shape"})}),"\n",(0,o.jsx)(n.h3,{id:"quantizing-models",children:"Quantizing models"}),"\n",(0,o.jsxs)(n.p,{children:["The NPU only supports uint8/int8 quantized models. Unsupported models, or unsupported layers will be automatically moved back to the CPU. For a guide on quantization models, see ",(0,o.jsx)(n.a,{href:"https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html",children:"ONNX Runtime docs: Quantize ONNX Models"}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Don't want to quantize yourself?"})," You can download a range of pre-quantized models from ",(0,o.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"Qualcomm AI Hub"}),", or use ",(0,o.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/edge-impulse",children:"Edge Impulse"})," to quantize new or existing models."]})}),"\n",(0,o.jsx)(n.h2,{id:"running-a-model-on-the-npu-python",children:"Running a model on the NPU (Python)"}),"\n",(0,o.jsxs)(n.p,{children:["To offload a model to the NPU, you just need to load the ",(0,o.jsx)(n.code,{children:"QNNExecutionProvider"}),"; and pass it when creating the ",(0,o.jsx)(n.code,{children:"InferenceSession"}),". E.g.:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'import onnxruntime as ort\r\n\r\nproviders = (("QNNExecutionProvider", {\r\n    "backend_type": "htp",\r\n    "profiling_level": "detailed",\r\n}))\r\n\r\nso = ort.SessionOptions()\r\n\r\nsess = ort.InferenceSession(MODEL_PATH, sess_options=so, providers=providers)\r\nactual_providers = sess.get_providers()\r\nprint(f"Using providers: {actual_providers}")   # will show QNNExecutionProvider,CPUExecutionProvider if QNN can be loaded\n'})}),"\n",(0,o.jsx)(n.p,{children:"(Make sure you use an onnxruntime wheel with AI Engine Direct bindings, see the top of the page)"}),"\n",(0,o.jsx)(n.h2,{id:"example-squeezenet-11-python",children:"Example: SqueezeNet-1.1 (Python)"}),"\n",(0,o.jsx)(n.p,{children:"Open the terminal on your development board, or an ssh session to your development board, and:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Create a new venv, and install the onnxruntime and Pillow:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3.12 -m venv .venv-onnxruntime-demo\r\nsource .venv-onnxruntime-demo/bin/activate\r\n\r\n# onnxruntime with AI Engine Direct bindings (only works on Python3.12)\r\nwget https://cdn.edgeimpulse.com/qc-ai-docs/wheels/onnxruntime_qnn-1.23.0-cp312-cp312-linux_aarch64.whl\r\npip3 install onnxruntime_qnn-1.23.0-cp312-cp312-linux_aarch64.whl\r\nrm onnxruntime*.whl\r\n\r\n# Other dependencies\r\npip3 install Pillow\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Here's an end-to-end example running ",(0,o.jsx)(n.a,{href:"https://aihub.qualcomm.com/models/squeezenet1_1",children:"SqueezeNet-1.1"}),". Save this file as ",(0,o.jsx)(n.code,{children:"inference_onnx.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'#!/usr/bin/env python3\r\nimport os, sys, time, urllib.request\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport onnxruntime as ort\r\n\r\ndef curr_ms():\r\n    return round(time.time() * 1000)\r\n\r\nuse_npu = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-npu\' else False\r\n\r\n# Path to your quantized ONNX model and test image (will be download automatically)\r\nMODEL_PATH = "model.onnx"\r\nMODEL_DATA_PATH = "model.data"\r\nIMAGE_PATH = "boa-constrictor.jpg"\r\nLABELS_PATH = "SqueezeNet-1.1_labels.txt"\r\n\r\nif not os.path.exists(MODEL_PATH):\r\n    print("Downloading model...")\r\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_w8a8.onnx\'\r\n    urllib.request.urlretrieve(model_url, MODEL_PATH)\r\n\r\nif not os.path.exists(MODEL_DATA_PATH):\r\n    print("Downloading model data...")\r\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_w8a8.data\'\r\n    urllib.request.urlretrieve(model_url, MODEL_DATA_PATH)\r\n\r\nif not os.path.exists(LABELS_PATH):\r\n    print("Downloading labels...")\r\n    labels_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/SqueezeNet-1.1_labels.txt\'\r\n    urllib.request.urlretrieve(labels_url, LABELS_PATH)\r\n\r\nif not os.path.exists(IMAGE_PATH):\r\n    print("Downloading image...")\r\n    image_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/examples/boa-constrictor.jpg\'\r\n    urllib.request.urlretrieve(image_url, IMAGE_PATH)\r\n\r\nwith open(LABELS_PATH, \'r\') as f:\r\n    labels = [line for line in f.read().splitlines() if line.strip()]\r\n\r\nproviders = []\r\nif use_npu:\r\n    providers.append(("QNNExecutionProvider", {\r\n        "backend_type": "htp",\r\n    }))\r\nelse:\r\n    providers.append("CPUExecutionProvider")\r\n\r\nso = ort.SessionOptions()\r\n\r\nsess = ort.InferenceSession(MODEL_PATH, sess_options=so, providers=providers)\r\nactual_providers = sess.get_providers()\r\nprint(f"Using providers: {actual_providers}") # Show which providers are actually loaded\r\n\r\ninputs  = sess.get_inputs()\r\noutputs = sess.get_outputs()\r\n\r\ndef load_image_for_onnx(path, H, W):\r\n    img = Image.open(path).convert("RGB").resize((W, H))\r\n    arr = np.array(img)\r\n    arr = arr.astype(np.float32) / 255.0\r\n\r\n    arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW\r\n    arr = np.expand_dims(arr, 0)        # -> NCHW\r\n\r\n    return arr\r\n\r\n# input data scaled 0..1\r\ninput_data_f32 = load_image_for_onnx(path=IMAGE_PATH, H=224, W=224)\r\n\r\n# quantize model (cannot read these params from the onnx model I believe)\r\nscale = 1.0 / 255.0\r\nzero_point = 0\r\ninput_data_u8 = np.round(input_data_f32.astype(np.float32) / float(scale)) + int(zero_point)\r\ninput_data_u8 = np.clip(input_data_u8, 0, 255).astype(np.uint8)\r\n\r\n# Warmup once\r\n_ = sess.run(None, {sess.get_inputs()[0].name: input_data_u8})\r\n\r\n# Run 10x so we can calculate avg. runtime per inference\r\nstart = curr_ms()\r\nfor i in range(10):\r\n    out = sess.run(None, {sess.get_inputs()[0].name: input_data_u8})\r\nend = curr_ms()\r\n\r\n# Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\r\ndef softmax(x, axis=-1):\r\n    # subtract max for numerical stability\r\n    x_max = np.max(x, axis=axis, keepdims=True)\r\n    e_x = np.exp(x - x_max)\r\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\r\n\r\nscores = softmax(np.squeeze(out[0], axis=0))\r\n\r\n# Take top 5\r\ntop_k_idx = scores.argsort()[-5:][::-1]\r\n\r\nprint("\\nTop-5 predictions:")\r\nfor i in top_k_idx:\r\n    label = labels[i] if i < len(labels) else f"Class {i}"\r\n    print(f"{label}: score={scores[i]}")\r\n\r\nprint("")\r\nprint(f"Inference took (on average): {(end - start) / 10:.2f} ms per image")\n'})}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"Note: this script has hard-coded quantization parameters. If you swap out the model you'll might need to change these."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Run the model on the CPU:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3 inference_onnx.py\r\n\r\n# Top-5 predictions:\r\n# common iguana: score=0.3682704567909241\r\n# night snake: score=0.1186317503452301\r\n# water snake: score=0.1186317503452301\r\n# boa constrictor: score=0.0813227966427803\r\n# bullfrog: score=0.0813227966427803\r\n#\r\n# Inference took (on average): 6.50 ms per image\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Run the model on the NPU:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3 inference_onnx.py --use-npu\r\n\r\n# Top-5 predictions:\r\n# common iguana: score=0.30427297949790955\r\n# water snake: score=0.11838366836309433\r\n# night snake: score=0.11838366836309433\r\n# boa constrictor: score=0.11838366836309433\r\n# rock python: score=0.08115273714065552\r\n#\r\n# Inference took (on average): 1.60 ms per image\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"As you can see this model runs significantly faster on NPU - but there's a slight change in the output of the model."}),"\n",(0,o.jsx)(n.h2,{id:"tips--tricks",children:"Tips & tricks"}),"\n",(0,o.jsx)(n.h3,{id:"disable-cpu-fallback",children:"Disable CPU fallback"}),"\n",(0,o.jsx)(n.p,{children:"To debug, you might want to choose to disable fallback to the CPU via:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'so = ort.SessionOptions()\r\nso.add_session_config_entry("session.disable_cpu_ep_fallback", "1")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"building-new-versions-of-the-the-onnxruntime-package",children:"Building new versions of the the onnxruntime package"}),"\n",(0,o.jsxs)(n.p,{children:["See ",(0,o.jsx)(n.a,{href:"https://github.com/edgeimpulse/onnxruntime-qnn-linux-aarch64",children:"edgeimpulse/onnxruntime-qnn-linux-aarch64"}),"."]})]})}function c(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}}}]);