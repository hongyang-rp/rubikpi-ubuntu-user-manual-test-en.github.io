"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3904],{837:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-4-214c56c6055ceb84c6bfdd039d2cc136.png"},3411:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/img_v3_02oe_a7351608-c493-478b-a3e4-4929e3b3c17g-385abd9ab28cb4c23f2e0548777a9177.png"},3966:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-fec1344fa150f31c14898bc3df58d450.png"},6909:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-22-9c134a6e429f27e3dc49ef75f7adc374.png"},9302:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-18-af525de61dd224351c7a344902400b1a.png"},9350:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-21-29fb4dee3b1a7948034a312f8bfa8709.png"},11470:(e,i,n)=>{n.d(i,{A:()=>y});var t=n(96540),s=n(34164),a=n(23104),l=n(56347),o=n(205),r=n(57485),c=n(31682),d=n(70679);function h(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:i}=e;return!!i&&"object"==typeof i&&"value"in i}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:i,children:n}=e;return(0,t.useMemo)(()=>{const e=i??function(e){return h(e).map(({props:{value:e,label:i,attributes:n,default:t}})=>({value:e,label:i,attributes:n,default:t}))}(n);return function(e){const i=(0,c.XI)(e,(e,i)=>e.value===i.value);if(i.length>0)throw new Error(`Docusaurus error: Duplicate values "${i.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[i,n])}function u({value:e,tabValues:i}){return i.some(i=>i.value===e)}function p({queryString:e=!1,groupId:i}){const n=(0,l.W6)(),s=function({queryString:e=!1,groupId:i}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!i)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return i??null}({queryString:e,groupId:i});return[(0,r.aZ)(s),(0,t.useCallback)(e=>{if(!s)return;const i=new URLSearchParams(n.location.search);i.set(s,e),n.replace({...n.location,search:i.toString()})},[s,n])]}function f(e){const{defaultValue:i,queryString:n=!1,groupId:s}=e,a=m(e),[l,r]=(0,t.useState)(()=>function({defaultValue:e,tabValues:i}){if(0===i.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!u({value:e,tabValues:i}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${i.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const n=i.find(e=>e.default)??i[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:i,tabValues:a})),[c,h]=p({queryString:n,groupId:s}),[f,g]=function({groupId:e}){const i=function(e){return e?`docusaurus.tab.${e}`:null}(e),[n,s]=(0,d.Dv)(i);return[n,(0,t.useCallback)(e=>{i&&s.set(e)},[i,s])]}({groupId:s}),x=(()=>{const e=c??f;return u({value:e,tabValues:a})?e:null})();(0,o.A)(()=>{x&&r(x)},[x]);return{selectedValue:l,selectValue:(0,t.useCallback)(e=>{if(!u({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);r(e),h(e),g(e)},[h,g,a]),tabValues:a}}var g=n(92303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=n(74848);function b({className:e,block:i,selectedValue:n,selectValue:t,tabValues:l}){const o=[],{blockElementScrollPositionUntilNextRender:r}=(0,a.a_)(),c=e=>{const i=e.currentTarget,s=o.indexOf(i),a=l[s].value;a!==n&&(r(i),t(a))},d=e=>{let i=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;i=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;i=o[n]??o[o.length-1];break}}i?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":i},e),children:l.map(({value:e,label:i,attributes:t})=>(0,j.jsx)("li",{role:"tab",tabIndex:n===e?0:-1,"aria-selected":n===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:c,...t,className:(0,s.A)("tabs__item",x.tabItem,t?.className,{"tabs__item--active":n===e}),children:i??e},e))})}function v({lazy:e,children:i,selectedValue:n}){const a=(Array.isArray(i)?i:[i]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===n);return e?(0,t.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:a.map((e,i)=>(0,t.cloneElement)(e,{key:i,hidden:e.props.value!==n}))})}function w(e){const i=f(e);return(0,j.jsxs)("div",{className:(0,s.A)("tabs-container",x.tabList),children:[(0,j.jsx)(b,{...i,...e}),(0,j.jsx)(v,{...i,...e})]})}function y(e){const i=(0,g.A)();return(0,j.jsx)(w,{...e,children:h(e.children)},String(i))}},11769:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-26-6bf6a233dd1539ce9e1358135da6993b.png"},13977:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-8-ea69f9c326053e3bbb207e929c7cde51.png"},15123:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-1-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg"},17682:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"Document Home/ai","title":"Artificial Intelligence","description":"This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm\'s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on your RUBIK Pi 3.","source":"@site/docs/Document Home/8.ai.md","sourceDirName":"Document Home","slug":"/Document Home/ai","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/8.ai.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"toc_min_heading_level":2,"toc_max_heading_level":2},"sidebar":"tutorialSidebar","previous":{"title":"Camera Software","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/camera-software"},"next":{"title":"Comparison Between Ubuntu Desktop Version and Server Version","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server"}}');var s=n(74848),a=n(28453),l=n(11470),o=n(19365);const r={toc_min_heading_level:2,toc_max_heading_level:2},c="Artificial Intelligence",d={},h=[{value:"Overview",id:"overview",level:2},{value:"Software and hardware architecture",id:"software-and-hardware-architecture",level:2},{value:"Overall AI framework",id:"overall-ai-framework",level:3},{value:"AI hardware",id:"ai-hardware",level:3},{value:"AI software",id:"ai-software",level:3},{value:"Compile and optimize the model",id:"compile-and-optimize-the-model",level:2},{value:"AI Hub",id:"ai-hub",level:3},{value:"Environment setup",id:"environment-setup",level:4},{value:"AI Hub workflow",id:"ai-hub-workflow",level:4},{value:"LiteRT",id:"litert",level:3},{value:"Qualcomm(\xae) Intelligent Multimedia SDK",id:"qualcomm-intelligent-multimedia-sdk",level:3},{value:"Overview of Qualcomm IM SDK",id:"overview-of-qualcomm-im-sdk",level:4},{value:"Functions of commonly used Qualcomm GStreamer plugins in the Qualcomm IM SDK",id:"functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk",level:4},{value:"AI/ML sample applications",id:"aiml-sample-applications",level:2},{value:"Preparations",id:"preparations",level:3},{value:"Image classification sample application: gst-ai-classification",id:"image-classification-sample-application-gst-ai-classification",level:3},{value:"Object detection sample application: gst-ai-object-detection",id:"object-detection-sample-application-gst-ai-object-detection",level:3},{value:"Pose detection sample application: gst-ai-pose-detection",id:"pose-detection-sample-application-gst-ai-pose-detection",level:3},{value:"Image segmentation sample application: gst-ai-segmentation",id:"image-segmentation-sample-application-gst-ai-segmentation",level:3},{value:"Multi input/output object detection sample application: gst-ai-multi-input-output-object-detection",id:"multi-inputoutput-object-detection-sample-application-gst-ai-multi-input-output-object-detection",level:3},{value:"Object detection and image classification cascade sample application\uff1agst-ai-daisychain-detection-classification",id:"object-detection-and-image-classification-cascade-sample-applicationgst-ai-daisychain-detection-classification",level:3},{value:"Image depth estimation sample application: gst-ai-monodepth",id:"image-depth-estimation-sample-application-gst-ai-monodepth",level:3},{value:"AI/ML gstreamer command-line use cases",id:"aiml-gstreamer-command-line-use-cases",level:2},{value:"Preparations",id:"preparations-1",level:3},{value:"GStreamer command-line use cases using LiteRT",id:"gstreamer-command-line-use-cases-using-litert",level:3},{value:"GStreamer command-line use cases for implementing AI functions by acquiring image data through a camera",id:"gstreamer-command-line-use-cases-for-implementing-ai-functions-by-acquiring-image-data-through-a-camera",level:4},{value:"Image classification (image-classification-LiteRT-from-camera)",id:"image-classification-image-classification-litert-from-camera",level:5},{value:"Object detection (object-detection-LiteRT-from-camera)",id:"object-detection-object-detection-litert-from-camera",level:5},{value:"Image segmentation (image-segmentation-LiteRT-from-camera)",id:"image-segmentation-image-segmentation-litert-from-camera",level:5},{value:"Pose detection (pose-detection-LiteRT-from-camera)",id:"pose-detection-pose-detection-litert-from-camera",level:5},{value:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file",id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file",level:4},{value:"Image classification (image-classification-LiteRT-from-file)",id:"image-classification-image-classification-litert-from-file",level:5},{value:"Object detection (object-detection-LiteRT-from-file)",id:"object-detection-object-detection-litert-from-file",level:5},{value:"Image segmentation (image-segmentation-LiteRT-from-file)",id:"image-segmentation-image-segmentation-litert-from-file",level:5},{value:"Pose detection (pose-detection-LiteRT-from-file)",id:"pose-detection-pose-detection-litert-from-file",level:5},{value:"GStreamer command-line use cases using SNPE",id:"gstreamer-command-line-use-cases-using-snpe",level:3},{value:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a camera",id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-camera",level:4},{value:"Image classification (image-classification-LiteRT-from-camera)",id:"image-classification-image-classification-litert-from-camera-1",level:5},{value:"Object detection (object-detection-LiteRT-from-camera)",id:"object-detection-object-detection-litert-from-camera-1",level:5},{value:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file",id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file-1",level:4},{value:"Image classification (image-classification-LiteRT-from-file)",id:"image-classification-image-classification-litert-from-file-1",level:5},{value:"Object detection (object-detection-LiteRT-from-file)",id:"object-detection-object-detection-litert-from-file-1",level:5},{value:"Refences",id:"refences",level:2}];function m(e){const i={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"artificial-intelligence",children:"Artificial Intelligence"})}),"\n",(0,s.jsx)(i.p,{children:"This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm's high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on your RUBIK Pi 3."}),"\n",(0,s.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(i.p,{children:"The AI/ML development process for RUBIK Pi 3 Ubuntu is as follows:"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(19189).A+"",width:"1000",height:"124"})}),"\n",(0,s.jsx)(i.p,{children:"The above AI/ML developer workflow consists of two steps:"}),"\n",(0,s.jsx)(i.p,{children:"Step 1"}),"\n",(0,s.jsx)(i.p,{children:"Compile and optimize the model from the third-party AI framework to efficiently run on RUBIK Pi 3. For example, a Tensorflow model can be exported to a TFLite model. Optionally, quantize, fine-tune performance, and accuracy using hardware-specific customizations."}),"\n",(0,s.jsx)(i.p,{children:"Step 2"}),"\n",(0,s.jsx)(i.p,{children:"Build an application to use the optimized model to run on device inference"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Integrate the AI model into the use case pipeline."}),"\n",(0,s.jsx)(i.li,{children:"Cross-compile the application to generate an executable binary file that uses the required libraries."}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"software-and-hardware-architecture",children:"Software and hardware architecture"}),"\n",(0,s.jsx)(i.h3,{id:"overall-ai-framework",children:"Overall AI framework"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(70421).A+"",width:"1280",height:"802"})}),"\n",(0,s.jsx)(i.p,{children:"Developers can import models from ONNX, PyTorch, TensorFlow or TFLite, and use the Qualcomm AI Runtime SDK to efficiently run these models on the AI hardware of the RUBIK Pi 3, including the HTP (NPU), GPU, and CPU."}),"\n",(0,s.jsx)(i.h3,{id:"ai-hardware",children:"AI hardware"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Qualcomm Kryo\u2122 CPU - Best-in-class CPU with high performance and remarkable power efficiency."}),"\n",(0,s.jsx)(i.li,{children:"Qualcomm Adreno GPU - Suitable to execute AI workloads with balanced power and performance. AI workloads are accelerated with OpenCL kernels. You can use the GPU to accelerate model pre/postprocessing."}),"\n",(0,s.jsx)(i.li,{children:"Qualcomm Hexagon - Also known as NPU/DSP/HMX, capable of executing AI workloads with low-power and high-performance. For optimized performance, quantize the pre-trained models to one of the supported precisions."}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"ai-software",children:"AI software"}),"\n",(0,s.jsx)(i.p,{children:"The AI software stack contains various SDKs to help AI developers easily leverage the powerful AI hardware accelerators of the RUBIK Pi 3. Developers can choose one of the SDKs to deploy their AI workloads. Pre-trained models (except for TFLite models) need to be converted to the executable format supported by the selected SDK before running. TFLite models can be run directly using the TFLite Delegate."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"LiteRT"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"LiteRT models can be executed natively on RUBIK Pi 3 hardware with acceleration using the following Delegates."}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Delegate"}),(0,s.jsx)(i.th,{children:"Acceleration"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"AI Engine Direct Delegate (QNN Delegate)"}),(0,s.jsx)(i.td,{children:"CPU, GPU, and HTP"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"XNNPACK Delegate"}),(0,s.jsx)(i.td,{children:"CPU"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"GPU Delegate"}),(0,s.jsx)(i.td,{children:"GPU"})]})]})]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Qualcomm Neural Processing Engine SDK (SNPE)"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"The Qualcomm Neural Processing Engine (also known as SNPE) is a software acceleration runtime for executing deep neural networks. The SNPE SDK provides tools to convert and quantize neural networks, and accelerate their execution on hardware accelerators such as the CPU, GPU, and HTP."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"Qualcomm AI Engine Direct (QNN)"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"Qualcomm AI Engine Direct is a software architecture designed for AI/ML use cases to leverage the AI accelerator hardware on the RUBIK Pi 3."}),"\n",(0,s.jsx)(i.p,{children:"The architecture is designed to provide a unified API and modular and extensible pre-accelerator libraries, which form a reusable basis for full stack AI solutions. It provides support for runtimes such as Qualcomm Neural Processing SDK and TFLite AI Engine Direct Delegate."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:(0,s.jsx)(i.strong,{children:"AI Model Efficiency Toolkit (AIMET)"})}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:"This is an open-source library to optimize (compressing and quantizing) trained neural network models. This is a complex SDK designed to generate optimized quantized models, intended only for advanced developers."}),"\n",(0,s.jsx)(i.h2,{id:"compile-and-optimize-the-model",children:"Compile and optimize the model"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(39068).A+"",width:"1000",height:"123"})}),"\n",(0,s.jsx)(i.p,{children:"Use either of the following two ways to compile and optimize your models:"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Method"}),(0,s.jsx)(i.th,{children:"Operation"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"AI hub"}),(0,s.jsx)(i.td,{children:"Developers can import their own models and try out the pre-optimized models on cloud devices (Snapdragon devices)."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"AI software stack"}),(0,s.jsxs)(i.td,{children:["- Optimize Model with LiteRT",(0,s.jsx)("p",{children:"Directly port the LiteRT AI model to the RUBIK Pi 3 device"}),"- Optimize AI Model with Qualcomm AI Runtime SDK",(0,s.jsx)("p",{children:"Use the integrated and easily customizable Qualcomm AI Runtime (QAIRT) SDK to port your model"})]})]})]})]}),"\n",(0,s.jsx)(i.h3,{id:"ai-hub",children:"AI Hub"}),"\n",(0,s.jsx)(i.p,{children:"AI Hub provides a way to optimize, validate, and deploy machine learning models on-device for vision, audio, and speech use cases."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(64235).A+"",width:"1000",height:"402"})}),"\n",(0,s.jsx)(i.h4,{id:"environment-setup",children:"Environment setup"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Install miniconda and configure the Python environment on your computer."}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:["Download ",(0,s.jsx)(i.a,{href:"https://www.anaconda.com/download",children:"miniconda"})," from the official website and install it."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Open the command line window."}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsx)(o.A,{value:"Windows",label:"Windows",default:!0,children:(0,s.jsxs)(i.p,{children:["After the installation is complete, open the ",(0,s.jsx)(i.a,{href:"https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html",children:"Anaconda "})," prompt window through the Start menu."]})}),(0,s.jsx)(o.A,{value:"macOS/Linux",label:"macOS/Linux",children:(0,s.jsx)(i.p,{children:"After the installation is complete, open a new shell."})})]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Set up a Python virtual environment for AI Hub. In the opened command-line interface, input the following commands:"}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"conda activate\r\nconda create python=3.10 -n qai_hub\r\nconda activate qai_hub\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Install the AI Hub Python client."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'pip3 install qai-hub\r\npip3 install "qai-hub[torch]"\n'})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Log in to AI Hub."}),"\n",(0,s.jsxs)(i.p,{children:["Go to ",(0,s.jsx)(i.a,{href:"https://aihub.qualcomm.com/",children:"AI Hub"})," and log in using your Qualcomm ID to view information about jobs you create."]}),"\n",(0,s.jsxs)(i.p,{children:["After you log in, navigate to ",(0,s.jsx)(i.strong,{children:"Account"})," > ",(0,s.jsx)(i.strong,{children:"Settings"})," > ",(0,s.jsx)(i.strong,{children:"API Token"}),". This should provide an API token that you can use to configure your client."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Configure the client with your API token using the following command in your terminal."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"qai-hub configure --api_token <INSERT_API_TOKEN>\n"})}),"\n",(0,s.jsx)(i.p,{children:"Use the following command to check the list of supported devices to verify that the AI Hub Python client is installed successfully."}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"qai-hub list-devices\n"})}),"\n",(0,s.jsx)(i.p,{children:"The following results indicate that the AI Hub Python client was installed successfully:"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(3411).A+"",width:"1280",height:"833"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h4,{id:"ai-hub-workflow",children:"AI Hub workflow"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Use a pre-optimized model"}),"\n"]}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["Navigate to ",(0,s.jsx)(i.a,{href:"https://aihub.qualcomm.com/iot/models",children:"AI Hub Model Zoo"})," to access pre-optimized models available for RUBIK Pi 3."]}),"\n",(0,s.jsx)(i.li,{children:"In the left-pane, filter models available for RUBIK Pi 3 by selecting Qualcomm QCS6490 as the chipset."}),"\n",(0,s.jsx)(i.li,{children:"Select a model from the filtered view to navigate to the model page."}),"\n",(0,s.jsxs)(i.li,{children:["On the model page, select Qualcomm QCS6490 from the drop-down list and choose ",(0,s.jsx)(i.strong,{children:"TorchScript"})," > ",(0,s.jsx)(i.strong,{children:"TFLite"})," path."]}),"\n",(0,s.jsx)(i.li,{children:"After clicking the download button, the model download will begin. The downloaded model has already been pre-optimized and can be directly used to develop your own applications."}),"\n"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Import your own model"}),"\n"]}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Select a pre-trained model in PyTorch or Onnx format."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Submit a model for compilation or optimization to AI Hub using python APIs. When submitting a compilation job, you must select a device or chipset and the target runtime to compile the model. For RUBIK Pi 3, the TFLite runtime is supported."}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"Chipset"})}),(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"Runtime"})}),(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"CPU"})}),(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"GPU"})}),(0,s.jsx)(i.th,{children:(0,s.jsx)(i.strong,{children:"HTP"})})]})}),(0,s.jsx)(i.tbody,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"QCS6490"}),(0,s.jsx)(i.td,{children:"TFLite"}),(0,s.jsx)(i.td,{children:"INT8, FP16, FP32"}),(0,s.jsx)(i.td,{children:"FP16, FP32"}),(0,s.jsx)(i.td,{children:"NT8, INT16"})]})})]}),"\n",(0,s.jsx)(i.p,{children:"On submission, AI Hub generates a unique ID for the job. You can use this job ID to view job details."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"AI Hub optimizes the model based your device and runtime selections. Optionally, you can submit a job to profile or inference the optimized model (using Python APIs) on a real device provisioned from a device farm."}),"\n",(0,s.jsx)(i.p,{children:"\u2013 Profiling: Benchmarks the model on a provisioned device and provides statistics, including average inference times at the layer level, runtime configuration, etc."}),"\n",(0,s.jsx)(i.p,{children:"\u2013 Inference: Performs inference using an optimized model on data submitted as part of the inference job by running the model on a provisioned device."}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Each submitted job will be available for review in the AI Hub portal. A submitted compilation job will provide a downloadable link to the optimized model. This optimized model can then be deployed on a local development device like RUBIK Pi 3."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h3,{id:"litert",children:"LiteRT"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(84750).A+"",width:"1000",height:"636"})}),"\n",(0,s.jsx)(i.p,{children:"LiteRT is an open-source deep learning framework for on-device inference. LiteRT helps you run your models on mobile, embedded, and edge platforms by optimizing the model for latency, model size, power consumption, etc. RUBIK Pi supports executing TFLite models natively through TFLite Delegates as listed below."}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Delegate"}),(0,s.jsx)(i.th,{children:"Acceleration"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"AI Engine Direct Delegate (QNN Delegate)"}),(0,s.jsx)(i.td,{children:"CPU, GPU, HTP"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"XNNPack Delegate"}),(0,s.jsx)(i.td,{children:"CPU"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"GPU Delegate"}),(0,s.jsx)(i.td,{children:"GPU"})]})]})]}),"\n",(0,s.jsx)(i.h3,{id:"qualcomm-intelligent-multimedia-sdk",children:"Qualcomm(\xae) Intelligent Multimedia SDK"}),"\n",(0,s.jsx)(i.h4,{id:"overview-of-qualcomm-im-sdk",children:"Overview of Qualcomm IM SDK"}),"\n",(0,s.jsxs)(i.p,{children:["The Qualcomm IM SDK provides the development environment with upstream and Qualcomm ",(0,s.jsx)(i.a,{href:"https://gstreamer.freedesktop.org/",children:"GStreamer"})," plugins as APIs. You can use these APIs to develop and optimize applications, create pipelines, and customize plugins."]}),"\n",(0,s.jsx)(i.p,{children:"The following diagram shows the Qualcomm IM SDK framework:"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(3966).A+"",width:"552",height:"443"})}),"\n",(0,s.jsxs)(i.h4,{id:"functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk",children:["Functions of commonly used Qualcomm ",(0,s.jsx)(i.a,{href:"https://gstreamer.freedesktop.org/",children:"GStreamer"})," plugins in the Qualcomm IM SDK"]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Plugin"}),(0,s.jsx)(i.th,{children:"Function"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtiqmmfsrc"}),(0,s.jsx)(i.td,{children:"The qtiqmmfsrc plugin captures the video frames through Qualcomm Camera Service."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlsnpe"}),(0,s.jsx)(i.td,{children:"Loading and executing the SNPE DLC model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimltflite"}),(0,s.jsx)(i.td,{children:"Loading and executing LiteRT TFLite model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlvconverter"}),(0,s.jsx)(i.td,{children:"Converting data from the incoming video buffer into neural network tensors, while performing the necessary format conversion and resizing."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlvclassification"}),(0,s.jsx)(i.td,{children:"Post-processing the output tensors for classification use cases."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlvdetection"}),(0,s.jsx)(i.td,{children:"Post-processing the output tensors for detection use cases."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlvsegmentation"}),(0,s.jsx)(i.td,{children:"Post-processing the output tensors for pixel-level use cases such as image segmentation or depth map processing."})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"qtimlvpose"}),(0,s.jsx)(i.td,{children:"Post-processing the output tensors for pose estimation use cases."})]})]})]}),"\n",(0,s.jsx)(i.h2,{id:"aiml-sample-applications",children:"AI/ML sample applications"}),"\n",(0,s.jsx)(i.h3,{id:"preparations",children:"Preparations"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Install the software package."}),"\n",(0,s.jsxs)(i.p,{children:["Refer to the ",(0,s.jsx)(i.a,{href:"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document%20Home/run-sample-applications",children:"Run sample applications"})," chapter and ensure that the sample application can run successfully."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Download the compressed package containing the model file, label file, and JSON configuration file required for the AI/ML sample application."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:["On the device, run the following command to download the compressed package ",(0,s.jsx)(i.em,{children:"ai_sample_app_models_labels_configs.zip"}),"."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"wget https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_sample_app_models_labels_configs.zip\n"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:["Use the following command to extract the compressed package to the ",(0,s.jsx)(i.em,{children:"/etc"})," directory."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"unzip ./ai_sample_app_model_label.zip -d /etc/\n"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"List of files in the compressed package:"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Model File"}),(0,s.jsx)(i.th,{children:"Label File"}),(0,s.jsx)(i.th,{children:"Json Configuration File"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/deeplabv3_plus_mobilenet_quantized.tflite"}),(0,s.jsx)(i.td,{children:"labels/classification_0.labels"}),(0,s.jsx)(i.td,{children:"configs/config_classification.json"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/midas_quantized.tflite"}),(0,s.jsx)(i.td,{children:"labels/hrnet_pose.labels"}),(0,s.jsx)(i.td,{children:"configs/config_monodepth.json"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/hrnet_pose_quantized.tflite"}),(0,s.jsx)(i.td,{children:"labels/yolov5.labels"}),(0,s.jsx)(i.td,{children:"configs/config_daisychain_detection_classification.json"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/yolov5.tflite"}),(0,s.jsx)(i.td,{children:"labels/deeplabv3_resnet50.labels"}),(0,s.jsx)(i.td,{children:"configs/config_pose.json"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/inception_v3_quantized.tflite"}),(0,s.jsx)(i.td,{children:"labels/monodepth.labels"}),(0,s.jsx)(i.td,{children:"configs/config_detection.json"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:"models/yolov8_det_quantized.tflite"}),(0,s.jsx)(i.td,{children:"labels/yolov8.labels"}),(0,s.jsx)(i.td,{children:"configs/config_segmentation.json"})]})]})]}),"\n",(0,s.jsx)(i.h3,{id:"image-classification-sample-application-gst-ai-classification",children:"Image classification sample application: gst-ai-classification"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-classification application allows you to identify the subject in an image. The use cases are implemented using the SNPE, LiteRT, or QNN."}),"\n",(0,s.jsx)(i.p,{children:"The following figure shows the pipeline, which receives a video stream from a camera, file source, or Real-Time Streaming Protocol (RTSP), preprocesses it, runs the inference on AI hardware, and displays the results on the screen."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(87532).A+"",width:"975",height:"326"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application\uff1a",(0,s.jsx)(i.em,{children:"/etc/configs/config_classification.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": 0,\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/inception_v3_quantized.tflite",\r\n  "labels": "/etc/labels/classification_0.labels",\r\n  "constants": "Inceptionv3,q-offsets=<38.0>,q-scales=<0.17039915919303894>;",\r\n  "threshold": 40,\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n "file-path": "/etc/media/video.mp4",\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/inception_v3_quantized.tflite",\r\n  "labels": "/etc/labels/classification_0.labels",\r\n  "constants": "Inceptionv3,q-offsets=<38.0>,q-scales=<0.17039915919303894>;",\r\n  "threshold": 40,\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsxs)(i.ol,{start:"2",children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-classification --config-file=/etc/configs/config_classification.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(56007).A+"",width:"840",height:"471"})}),"\n",(0,s.jsx)(i.h3,{id:"object-detection-sample-application-gst-ai-object-detection",children:"Object detection sample application: gst-ai-object-detection"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-object-detection application allows you to detect objects within images and videos."}),"\n",(0,s.jsx)(i.p,{children:"The following figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, preprocesses it, runs inferences on AI hardware, and displays the results on the screen."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(84670).A+"",width:"975",height:"326"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application: ",(0,s.jsx)(i.em,{children:"/etc/configs/config_detection.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": "0",\r\n  "ml-framework": "tflite",\r\n  "yolo-model-type": "yolov8",\r\n  "model": "/etc/models/yolov8_det_quantized.tflite",\r\n  "labels": "/etc/labels/yolov8.labels",\r\n  "constants": "YOLOv8,q-offsets=<21.0, 0.0, 0.0>,q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;",\r\n  "threshold": 40,\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "file-path": "/etc/media/video.mp4",\r\n  "ml-framework": "tflite",\r\n  "yolo-model-type": "yolov8",\r\n  "model": "/etc/models/yolov8_det_quantized.tflite",\r\n  "labels": "/etc/labels/yolov8.labels",\r\n  "constants": "YOLOv8,q-offsets=<21.0, 0.0, 0.0>,q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;",\r\n  "threshold": 40,\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsxs)(i.ol,{start:"2",children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-object-detection --config-file=/etc/configs/config_detection.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(29018).A+"",width:"975",height:"548"})}),"\n",(0,s.jsx)(i.h3,{id:"pose-detection-sample-application-gst-ai-pose-detection",children:"Pose detection sample application: gst-ai-pose-detection"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-pose-detection application allows you to detect the body pose of the subject in an image or video."}),"\n",(0,s.jsx)(i.p,{children:"The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(19253).A+"",width:"975",height:"310"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application: ",(0,s.jsx)(i.em,{children:"/etc/configs/config_pose.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": "0",\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/hrnet_pose_quantized.tflite",\r\n  "labels": "/etc/labels/hrnet_pose.labels",\r\n  "constants": "hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;",\r\n  "threshold": 51,\r\n  "runtime": "dsp"\r\n}\r\n\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "file-path": "/etc/media/video.mp4",\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/hrnet_pose_quantized.tflite",\r\n  "labels": "/etc/labels/hrnet_pose.labels",\r\n  "constants": "hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;",\r\n  "threshold": 51,\r\n  "runtime": "dsp"\r\n}\r\n\n'})}),"\n",(0,s.jsxs)(i.ol,{start:"2",children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-pose-detection  --config-file=/etc/configs/config_pose.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(32050).A+"",width:"845",height:"479"})}),"\n",(0,s.jsx)(i.h3,{id:"image-segmentation-sample-application-gst-ai-segmentation",children:"Image segmentation sample application: gst-ai-segmentation"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-segmentation application allows you to divide an image into different and meaningful parts or segments."}),"\n",(0,s.jsx)(i.p,{children:"The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(46875).A+"",width:"975",height:"326"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application: ",(0,s.jsx)(i.em,{children:"/etc/configs/config_segmentation.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": 0,\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/deeplabv3_plus_mobilenet_quantized.tflite",\r\n  "labels": "/etc/labels/deeplabv3_resnet50.labels",\r\n  "constants": "deeplab,q-offsets=<0.0>,q-scales=<1.0>;",\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "file-path": "/etc/media/video.mp4",\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/deeplabv3_plus_mobilenet_quantized.tflite",\r\n  "labels": "/etc/labels/deeplabv3_resnet50.labels",\r\n  "constants": "deeplab,q-offsets=<0.0>,q-scales=<1.0>;",\r\n  "runtime": "dsp"\r\n}\n'})}),"\n",(0,s.jsxs)(i.ol,{start:"2",children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-segmentation  --config-file=/etc/configs/config_segmentation.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(84896).A+"",width:"975",height:"548"})}),"\n",(0,s.jsx)(i.h3,{id:"multi-inputoutput-object-detection-sample-application-gst-ai-multi-input-output-object-detection",children:"Multi input/output object detection sample application: gst-ai-multi-input-output-object-detection"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-multi-input-output-object-detection application allows you to perform object detection on video streams from multiple sources such as two cameras, two video files, or over the network protocol such as RTSP."}),"\n",(0,s.jsx)(i.p,{children:"The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(63017).A+"",width:"975",height:"267"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect two cameras before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-multi-input-output-object-detection --num-camera=2 --out-file=/opt/H.mp4 -d --model=/etc/models/yolov5.tflite --labels=/etc/labels/yolov5.labels\n"})}),"\n",(0,s.jsx)(i.p,{children:"The current command demonstrates a use case where video streams from two cameras are used simultaneously for object detection, with the inference results saved to an MP4 file and displayed on a screen."}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(36223).A+"",width:"657",height:"183"})}),"\n",(0,s.jsx)(i.h3,{id:"object-detection-and-image-classification-cascade-sample-applicationgst-ai-daisychain-detection-classification",children:"Object detection and image classification cascade sample application\uff1agst-ai-daisychain-detection-classification"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-daisychain-detection-classification sample application can perform cascaded object detection and classification using a camera, file, or RTSP stream."}),"\n",(0,s.jsx)(i.p,{children:"The figure shows the pipeline, which receives the input from a live camera feed, file, or other sources, performs preprocessing, conducts inference on the AI hardware, and displays the detection and classification results on the screen."}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.em,{children:"Figure: gst-ai-daisychain-detection-classification send pipeline"}),"\r\n",(0,s.jsx)(i.img,{src:n(83012).A+"",width:"1280",height:"405"})]}),"\n",(0,s.jsxs)(i.p,{children:[(0,s.jsx)(i.em,{children:"Figure: gst-ai-daisychain-detection-classification inference pipeline"}),"\r\n",(0,s.jsx)(i.img,{src:n(22934).A+"",width:"1280",height:"487"})]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application: ",(0,s.jsx)(i.em,{children:"/etc/configs/config_daisychain_detection_classification.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": 0,\r\n  "detection-model": "/etc/models/yolov8_det_quantized.tflite",\r\n  "detection-labels": "/etc/labels/yolov8.labels",\r\n  "classification-model": "/etc/models/inception_v3_quantized.tflite",\r\n  "classification-labels": "/etc/labels/classification_0.labels",\r\n  "detection-constants": "YOLOv8,q-offsets=<21.0, 0.0, 0.0>,q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;",\r\n  "classification-constants": "Inceptionv3,q-offsets=<38.0>,q-scales=<0.17039915919303894>;"\r\n}\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "file-path": "/etc/media/video.mp4",\r\n  "detection-model": "/etc/models/yolov8_det_quantized.tflite",\r\n  "detection-labels": "/etc/labels/yolov8.labels",\r\n  "classification-model": "/etc/models/inception_v3_quantized.tflite",\r\n  "classification-labels": "/etc/labels/classification_0.labels",\r\n  "detection-constants": "YOLOv8,q-offsets=<21.0, 0.0, 0.0>,q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;",\r\n  "classification-constants": "Inceptionv3,q-offsets=<38.0>,q-scales=<0.17039915919303894>;"\r\n}\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-daisychain-detection-classification   --config-file=/etc/configs/config_daisychain_detection_classification.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(11769).A+"",width:"1280",height:"720"})}),"\n",(0,s.jsx)(i.h3,{id:"image-depth-estimation-sample-application-gst-ai-monodepth",children:"Image depth estimation sample application: gst-ai-monodepth"}),"\n",(0,s.jsx)(i.p,{children:"The gst-ai-monodepth sample application can acquire visual data from three types of input sources: a live camera feed, a local video file, or a network RTSP stream. It uses a monocular depth estimation algorithm (without requiring a stereo camera) to automatically analyze and infer the depth information of each object in the scene \u2014 that is, how far each object is from the camera or observer."}),"\n",(0,s.jsx)(i.p,{children:"The diagram below shows the workflow of the sample application."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(45186).A+"",width:"975",height:"312"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["JSON configuration file used by the sample application: ",(0,s.jsx)(i.em,{children:"/etc/configs/config_monodepth.json"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"Follow these steps to perform testing:"})}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsx)(i.li,{children:"Use the camera as the input source:"}),"\n"]}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsx)(i.p,{children:"Connect the camera before testing."})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "camera": 0,\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/midas_quantized.tflite",\r\n  "labels": "/etc/labels/monodepth.labels",\r\n  "constants": "Midas,q-offsets=<0.0>,q-scales=<6.846843242645264>;",\r\n  "runtime": "dsp"\r\n}\r\n\n'})}),"\n",(0,s.jsx)(i.p,{children:"Alternatively, use the MP4 video file as the input source:"}),"\n",(0,s.jsx)(i.admonition,{type:"note",children:(0,s.jsxs)(i.p,{children:["Set the ",(0,s.jsx)(i.code,{children:"file-path"})," attribute below to the path of your MP4 video file."]})}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-json",children:'{\r\n  "file-path": "/etc/media/video.mp4",\r\n  "ml-framework": "tflite",\r\n  "model": "/etc/models/midas_quantized.tflite",\r\n  "labels": "/etc/labels/monodepth.labels",\r\n  "constants": "Midas,q-offsets=<0.0>,q-scales=<6.846843242645264>;",\r\n  "runtime": "dsp"\r\n}\r\n\n'})}),"\n",(0,s.jsxs)(i.ol,{start:"2",children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\ngst-ai-classification --config-file=/etc/configs/config_monodepth.json\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(35691).A+"",width:"986",height:"480"})}),"\n",(0,s.jsx)(i.h2,{id:"aiml-gstreamer-command-line-use-cases",children:"AI/ML gstreamer command-line use cases"}),"\n",(0,s.jsx)(i.p,{children:"The AI/ML GStreamer command line use cases demonstrate the practical scenario of using the GStreamer plugin from QIM on a RUBIK Pi 3 device to feed data from a live camera or a local video file and then run a model. The following will detail the steps to run the sample applications."}),"\n",(0,s.jsx)(i.h3,{id:"preparations-1",children:"Preparations"}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Install the software package."}),"\n",(0,s.jsxs)(i.p,{children:["Refer to the ",(0,s.jsx)(i.a,{href:"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document%20Home/run-sample-applications",children:"Run sample applications"})," chapter and ensure that the sample application can run successfully."]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Download the model file and label file."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:["Use the following command on the device to download the compressed file ",(0,s.jsx)(i.em,{children:"ai_gstreamer_command_line_models_labels.zip"}),", which contains the model files and label files required for the AI/ML GStreamer command line use cases."]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"wget https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_gstreamer_command_line_models_labels.zip\n"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsxs)(i.p,{children:["Use the following command to extract the compressed file to the ",(0,s.jsx)(i.em,{children:"/etc"})," directory:"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"unzip ./ai_gstreamer_command_line_models_labels.zip -d /opt/\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"List of GStreamer command-line use cases using LiteRT and the corresponding model files and label files"})}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"GStreamer Command-line Use Case"}),(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"Required Model"}),(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"Required Label File"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"image-classification-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"resnet101-resnet101-w8a8.tflite"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"classification_0.labels"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"object-detection-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"yolov8_det_quantized.tflite"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"yolov8.labels"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"image-segmentation-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"deeplabv3_plus_mobilenet_quantized.tflite"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"deeplabv3_resnet50.labels"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"pose-detection-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"hrnet_pose_quantized.tflite"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"hrnet_pose.labels"})]})]})]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"List of GStreamer command-line use cases using SNPE and the corresponding model files and label files"})}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"GStreamer Command-line Use Case"}),(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"Required Model"}),(0,s.jsx)(i.th,{style:{textAlign:"left"},children:"Required Label File"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"image-classification-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"inceptionv3.dlc"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"classification.labels"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"object-detection-LiteRT-from-camera/file"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"yolonas.dlc"}),(0,s.jsx)(i.td,{style:{textAlign:"left"},children:"yolonas.labels"})]})]})]}),"\n",(0,s.jsx)(i.h3,{id:"gstreamer-command-line-use-cases-using-litert",children:"GStreamer command-line use cases using LiteRT"}),"\n",(0,s.jsx)(i.h4,{id:"gstreamer-command-line-use-cases-for-implementing-ai-functions-by-acquiring-image-data-through-a-camera",children:"GStreamer command-line use cases for implementing AI functions by acquiring image data through a camera"}),"\n",(0,s.jsx)(i.h5,{id:"image-classification-image-classification-litert-from-camera",children:"Image classification (image-classification-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the resnet101-resnet101-w8a8.tflite model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(85780).A+"",width:"975",height:"316"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0  ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;"  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants="Inception,q-offsets=<-38.0>,q-scales=<0.17039915919303894>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. \n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(15123).A+"",width:"1280",height:"960"})}),"\n",(0,s.jsx)(i.h5,{id:"object-detection-object-detection-litert-from-camera",children:"Object detection (object-detection-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the yolov8_det_quantized.tflite model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(6909).A+"",width:"975",height:"289"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants="YOLOv8,q-offsets=<21.0, 0.0, 0.0>,    q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(9350).A+"",width:"956",height:"486"})}),"\n",(0,s.jsx)(i.h5,{id:"image-segmentation-image-segmentation-litert-from-camera",children:"Image segmentation (image-segmentation-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the deeplabv3_plus_mobilenet_quantized.tflite model with HTP. The segmentation results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(90895).A+"",width:"731",height:"211"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants="deeplab,q-offsets=<-61.0>,q-scales=<0.06232302635908127>;" ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(87391).A+"",width:"918",height:"519"})}),"\n",(0,s.jsx)(i.h5,{id:"pose-detection-pose-detection-litert-from-camera",children:"Pose detection (pose-detection-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the hrnet_pose_quantized.tflite model with HTP. The pose detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(9302).A+"",width:"975",height:"289"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants="hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(20785).A+"",width:"864",height:"534"})}),"\n",(0,s.jsx)(i.h4,{id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file",children:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file"}),"\n",(0,s.jsx)(i.h5,{id:"image-classification-image-classification-litert-from-file",children:"Image classification (image-classification-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the resnet101-resnet101-w8a8.tflite model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(837).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;"  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants="Inception,q-offsets=<-38.0>,q-scales=<0.17039915919303894>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. \n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(18617).A+"",width:"1280",height:"960"})}),"\n",(0,s.jsx)(i.h5,{id:"object-detection-object-detection-litert-from-file",children:"Object detection (object-detection-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the yolov8_det_quantized.tflite model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(83324).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants="YOLOv8,q-offsets=<21.0, 0.0, 0.0>,    q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(80840).A+"",width:"956",height:"486"})}),"\n",(0,s.jsx)(i.h5,{id:"image-segmentation-image-segmentation-litert-from-file",children:"Image segmentation (image-segmentation-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the deeplabv3_plus_mobilenet_quantized.tflite model with HTP. The segmentation results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(72055).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants="deeplab,q-offsets=<-61.0>,q-scales=<0.06232302635908127>;" ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(96122).A+"",width:"918",height:"519"})}),"\n",(0,s.jsx)(i.h5,{id:"pose-detection-pose-detection-litert-from-file",children:"Pose detection (pose-detection-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the hrnet_pose_quantized.tflite model with HTP. The pose detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(98414).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants="hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(17699).A+"",width:"864",height:"534"})}),"\n",(0,s.jsx)(i.h3,{id:"gstreamer-command-line-use-cases-using-snpe",children:"GStreamer command-line use cases using SNPE"}),"\n",(0,s.jsx)(i.h4,{id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-camera",children:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a camera"}),"\n",(0,s.jsx)(i.h5,{id:"image-classification-image-classification-litert-from-camera-1",children:"Image classification (image-classification-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to SNPE, which performs inference using the inceptionv3.dlc model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(13977).A+"",width:"2086",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink="waylandsink sync=true fullscreen=true"  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(62497).A+"",width:"1280",height:"960"})}),"\n",(0,s.jsx)(i.h5,{id:"object-detection-object-detection-litert-from-camera-1",children:"Object detection (object-detection-LiteRT-from-camera)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses a camera to capture images in real time and sends them to SNPE, which performs inference using the yolonas.labels model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(98224).A+"",width:"2086",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink='waylandsink fullscreen=true sync=true' split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=\"</heads/Mul, /heads/Sigmoid>\" ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(21872).A+"",width:"956",height:"486"})}),"\n",(0,s.jsx)(i.h4,{id:"gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file-1",children:"GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file"}),"\n",(0,s.jsx)(i.h5,{id:"image-classification-image-classification-litert-from-file-1",children:"Image classification (image-classification-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to SNPE, which performs inference using the inceptionv3.dlc model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(31338).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:'sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink="waylandsink sync=true fullscreen=true"  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.\n'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(88900).A+"",width:"1280",height:"960"})}),"\n",(0,s.jsx)(i.h5,{id:"object-detection-object-detection-litert-from-file-1",children:"Object detection (object-detection-LiteRT-from-file)"}),"\n",(0,s.jsx)(i.p,{children:"This sample application uses an MP4 file to obtain image data and sends it to SNPE, which performs inference using the yolonas.labels model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline."}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(70195).A+"",width:"2688",height:"696"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"Execute the following commands to run the sample application:"}),"\n"]}),"\n",(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-bash",children:"sudo -i\r\nexport XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1\r\ngst-launch-1.0 -e --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink='waylandsink fullscreen=true sync=true' split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=\"</heads/Mul, /heads/Sigmoid>\" ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.\n"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"The result is shown below:"}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{src:n(38231).A+"",width:"956",height:"486"})}),"\n",(0,s.jsx)(i.h2,{id:"refences",children:"Refences"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Reference documentation for compiling sample applications"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://docs.qualcomm.com/bundle/publicresource/topics/80-90441-15/introduction.html?product=1601111740057201&facet=AI%20developer%20workflow",children:"AI Developer_Workflow"})}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:"Reference documentation for the QIM SDK and all AI/MM sample applications"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.a,{href:"https://docs.qualcomm.com/bundle/publicresource/topics/80-70020-50/example-applications.html?vproduct=1601111740013072&version=1.5&facet=Qualcomm%20Intelligent%20Multimedia%20SDK",children:"Qualcomm Intelligent Multimedia SDK (IM SDK) Reference - Qualcomm\xae Linux Documentation"})}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},17699:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-15-f68564401dde9af3270e947cddd533a9.png"},18617:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg"},19189:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-17-42008d5cf061cb50439d591bb130a429.jpg"},19253:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-8-ee712ed3b63800f979729e008bc80e8c.png"},19365:(e,i,n)=>{n.d(i,{A:()=>l});n(96540);var t=n(34164);const s={tabItem:"tabItem_Ymn6"};var a=n(74848);function l({children:e,hidden:i,className:n}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,t.A)(s.tabItem,n),hidden:i,children:e})}},20785:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-17-f68564401dde9af3270e947cddd533a9.png"},21872:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-27-29fb4dee3b1a7948034a312f8bfa8709.png"},22934:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-3-1162dd6f3167f2250ad0193eb4f4e0a4.png"},28453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>o});var t=n(96540);const s={},a=t.createContext(s);function l(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(a.Provider,{value:i},e.children)}},29018:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-29-cc32640c9f279203020de85f7b24bb51.png"},31338:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-10-ba0daee74e210ca9bfdd5f611fa2635a.png"},32050:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-7-f1971359954c508282a3baff03b79e55.png"},35691:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-24-bc557e41d8257d66befc033cce7e7331.png"},36223:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-2-6c70089573f2a146c7506e93efa075a0.png"},38231:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-28-29fb4dee3b1a7948034a312f8bfa8709.png"},39068:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-1-c6ef0a4354b2cbbc47774f089bd26625.jpg"},45186:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-25-5c7e87f10c88e121fbbf48e5a5e91776.png"},46875:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-6-04cca60377ca19c815ee01ba9ba8561e.png"},56007:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-11-082a11bc07fd6a1828909518f3fdc27c.png"},62497:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-3-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg"},63017:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-4-446e5b939bd22358dc6d1f07fdf63d7b.png"},64235:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-2-0db7dab67f14ed37b4747e3188270d12.jpg"},70195:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-11-78ba5f302400fab97b557b2d11724176.png"},70421:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-13-3232f4794704c115830d318f11e71727.png"},72055:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-6-f343bca1a2e74ce2f47395299b649386.png"},80840:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-16-29fb4dee3b1a7948034a312f8bfa8709.png"},83012:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-1-b0223958c35a657b84a001c69ff12767.png"},83324:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-5-3143d47d7398911c36fff70e8d7539a5.png"},84670:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-10-89cf58dc0388cb76a8b8e499f5c48ca3.png"},84750:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-3-0cdc3b0210199ca7f0e51bd428123c79.jpg"},84896:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-5-8dc29396b381b73bf3fb5425b1bd2c4d.png"},85780:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-23-66cc812771313ac9fecbf078ad04297d.png"},87391:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-19-686f9af59c269c1cd7b1ad597c2d581c.png"},87532:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-12-741d9d30a3784aacdfa297be189e3470.png"},88900:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-2-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg"},90895:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-20-8c1e9b67a67210d4f7b2cc27b8163eb4.png"},96122:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/image-14-686f9af59c269c1cd7b1ad597c2d581c.png"},98224:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-9-342c8096a6e6e4d8b6d05e9f96629bd3.png"},98414:(e,i,n)=>{n.d(i,{A:()=>t});const t=n.p+"assets/images/diagram-7-2afa9bb0f9e54645a99a01a8a48829e5.png"}}]);