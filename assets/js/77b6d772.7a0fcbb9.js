"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5861],{43023:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var s=i(63696);const t={},o=s.createContext(t);function l(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(o.Provider,{value:n},e.children)}},52341:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie","title":"Genie","description":"A select number of Large Language Models (LLMs) and Vision Language Models (VLMs) can run on the NPU on your Dragonwing development board using the Qualcomm Gen AI Inference Extensions (Genie). These models have been ported and optimized by Qualcomm to be as efficient as possible on hardware. Genie only supports a subset of manually ported models, so if your favourite model is not listed, look at Run LLMs / VLMs using llama.cpp to run models on the GPU as a fallback.","source":"@site/docs/Document Home/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/6.genie.md","sourceDirName":"Document Home/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution","slug":"/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/7.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/6.genie.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Llama.cpp","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"},"next":{"title":"Update JSON Config","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/IMSDK/Update JSON Config"}}');var t=i(62540),o=i(43023);const l={},a="Genie",r={},d=[{value:"Installing AI Runtime SDK - Community Edition",id:"installing-ai-runtime-sdk---community-edition",level:2},{value:"Finding supported models",id:"finding-supported-models",level:2},{value:"Running Qwen2.5-0.5B-Instruct",id:"running-qwen25-05b-instruct",level:2},{value:"Serving a UI or API through QAI AppBuilder",id:"serving-a-ui-or-api-through-qai-appbuilder",level:2},{value:"Tips and tricks",id:"tips-and-tricks",level:2},{value:"Downloading files from HuggingFace that require authentication",id:"downloading-files-from-huggingface-that-require-authentication",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"genie",children:"Genie"})}),"\n",(0,t.jsxs)(n.p,{children:["A select number of Large Language Models (LLMs) and Vision Language Models (VLMs) can run on the NPU on your Dragonwing development board using the ",(0,t.jsx)(n.a,{href:"https://www.qualcomm.com/developer/software/gen-ai-inference-extensions",children:"Qualcomm Gen AI Inference Extensions (Genie)"}),". These models have been ported and optimized by Qualcomm to be as efficient as possible on hardware. Genie only supports a subset of manually ported models, so if your favourite model is not listed, look at ",(0,t.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/llama-cpp",children:"Run LLMs / VLMs using llama.cpp"})," to run models on the GPU as a fallback."]}),"\n",(0,t.jsx)(n.h2,{id:"installing-ai-runtime-sdk---community-edition",children:"Installing AI Runtime SDK - Community Edition"}),"\n",(0,t.jsxs)(n.p,{children:["First install the ",(0,t.jsx)(n.a,{href:"https://softwarecenter.qualcomm.com/catalog/item/Qualcomm_AI_Runtime_Community?osArch=Any&osType=All&version=2.35.0.250530",children:"AI Runtime SDK - Community Edition"}),". Open the terminal on your development board, or an ssh session to your development board, and run:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"wget -qO- https://cdn.edgeimpulse.com/qc-ai-docs/device-setup/install_ai_runtime_sdk_2.35.sh | bash\n"})}),"\n",(0,t.jsx)(n.h2,{id:"finding-supported-models",children:"Finding supported models"}),"\n",(0,t.jsx)(n.p,{children:"Genie-compatible LLM models can be found in a few places:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://aiot.aidlux.com/en/models",children:"Aplux model zoo"}),":","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Under 'Chipset', select:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"RB3 Gen 2 Vision Kit: 'Qualcomm QCS6490'"}),"\n",(0,t.jsx)(n.li,{children:"RUBIK Pi 3: 'Qualcomm QCS6490'"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Under 'NLP', select \"Text Generation\"."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://aihub.qualcomm.com/models?domain=Generative+AI",children:"Qualcomm AI Hub"}),":","\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Under 'Chipset', select:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"RB3 Gen 2 Vision Kit: 'Qualcomm QCS6490 (Proxy)'"}),"\n",(0,t.jsx)(n.li,{children:"RUBIK Pi 3: 'Qualcomm QCS6490 (Proxy)'"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Under 'Domain/Use Case', select \"Generative AI\"."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["As an example, let's deploy the ",(0,t.jsx)(n.a,{href:"https://aiot.aidlux.com/en/models/detail/149?modelType=9&soc=2",children:"Qwen2.5-0.5B-Instruct"})," model - which runs on all Dragonwing development boards."]}),"\n",(0,t.jsx)(n.h2,{id:"running-qwen25-05b-instruct",children:"Running Qwen2.5-0.5B-Instruct"}),"\n",(0,t.jsx)(n.p,{children:"When you download a model you'll need 3 files:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["One or more ",(0,t.jsx)(n.code,{children:"*.serialized.bin"})," files - these contain the weights of the model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"tokenizer.json"})," - a serialized configuration file that defines how text is split into tokens, mapping between characters, subwords, and their integer IDs used by an LLM. These can typically be downloaded from the model space on HuggingFace. A list of links for Genie-supported models is on ",(0,t.jsx)(n.a,{href:"https://github.com/quic/ai-hub-apps/tree/main/tutorials/llm_on_genie",children:"quic/ai-hub-apps: LLM On-Device Deployment > Prepare Genie configs"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["A Genie config file - with instructions on how to run this model through Genie. These can be found on GitHub for models in AI Hub: ",(0,t.jsx)(n.a,{href:"https://github.com/quic/ai-hub-apps/tree/main/tutorials/llm_on_genie/configs/genie",children:"quic/ai-hub-apps: tutorials/llm_on_genie/configs/genie"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Let's grab all of these and run Qwen2.5-0.5B-Instruct. Open the terminal on your development board, or an ssh session to your development board, and:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Download the model onto your development board."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Go to the ",(0,t.jsx)(n.a,{href:"https://aiot.aidlux.com/en/models/detail/149?modelType=9&soc=2",children:"Aplux model zoo: Qwen2.5-0.5B-Instruct"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Sign up for an Aplux account."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Under 'Device', select the QCS6490."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'Click "Download Model & Test code".'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-50d65ed63d098563fd3b18b0ed09066ca758b00f%2Faplux1.png?alt=media",alt:"",title:"Downloading Genie-compatible models for the QCS6490"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"After downloading, push the ZIP file to your development board over ssh:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Find the IP address of your development board. Run on your development board:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n\n# ... Example:\n# 192.168.1.253\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Push the .zip file. Run from your computer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"scp qnn229_qcs6490_cl4096.zip ubuntu@192.168.1.253:~/qnn229_qcs6490_cl4096.zip\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Unzip the model. From your development board:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir -p genie-models/\nunzip -d genie-models/qwen2.5-0.5b-instruct/ qnn229_qcs6490_cl4096.zip\nrm qnn229_qcs6490_cl4096.zip\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Run your model:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd genie-models/qwen2.5-0.5b-instruct/\n\ngenie-t2t-run -c ./qwen2.5-0.5b-instruct-htp.json -p '<|im_start|>system\n    You are Qwen, created by Alibaba Cloud. You are a helpful assistant that responds in English.<|im_end|><|im_start|>user\n    What is the capital of the Netherlands?<|im_end|><|im_start|>assistant'\n\n# Using libGenie.so version 1.9.0\n#\n# [BEGIN]:\n# The capital of the Netherlands is Amsterdam.[END]\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Great! You now have this LLM running under Genie."}),"\n",(0,t.jsx)(n.h2,{id:"serving-a-ui-or-api-through-qai-appbuilder",children:"Serving a UI or API through QAI AppBuilder"}),"\n",(0,t.jsxs)(n.p,{children:["To use Genie models from your application you can use the ",(0,t.jsx)(n.a,{href:"https://github.com/quic/ai-engine-direct-helper",children:"QAI AppBuilder"})," repository. The AppBuilder repo has both a OpenAI compatible chat completion API, as well as a Web UI to interact with your model (just like ",(0,t.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/llama-cpp",children:"llama.cpp"}),")."]}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Heavy development:"})," The AppBuilder is under heavy development. We've tried to pin the versions as much as we can, but using newer versions of the AppBuilder might not work with the instructions below."]})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Install the AppBuilder:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo apt install -y yq\n\n# Clone the repository (can switch back to upstream once https://github.com/quic/ai-engine-direct-helper/pull/16 is landed)\ngit clone https://github.com/edgeimpulse/ai-engine-direct-helper\ncd ai-engine-direct-helper\ngit submodule update --init --recursive\ngit checkout linux-paths\n\n# Create a new venv\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# Build the wheel\npip3 install setuptools\npython setup.py bdist_wheel\npip3 install ./dist/qai_appbuilder-*-linux_aarch64.whl\n\n# Install other dependencies\npip3 install \\\n    uvicorn==0.35.0 \\\n    pydantic_settings==2.10.1 \\\n    fastapi==0.116.1 \\\n    langchain==0.3.27 \\\n    langchain-core==0.3.75 \\\n    langchain-community==0.3.29 \\\n    sse_starlette==3.0.2 \\\n    pypdf==6.0.0 \\\n    python-pptx==1.0.2 \\\n    docx2txt==0.9 \\\n    openai==1.107.0 \\\n    json-repair==0.50.1 \\\n    qai_hub==0.36.0 \\\n    py3_wget==1.0.13 \\\n    torch==2.8.0 \\\n    transformers==4.56.1 \\\n    gradio==5.44.1 \\\n    diffusers==0.35.1\n\n# Where you\'ve downloaded the weights, and created the config files before\nWEIGHTS_DIR=~/genie-models/qwen2.5-0.5b-instruct/\nMODEL_NAME=qwen2_5-0_5b-instruct\n\n# Create a new directory and link the files\nmkdir -p samples/genie/python/models/$MODEL_NAME\ncd samples/genie/python/models/$MODEL_NAME\n\n# Patch up config\ncp $WEIGHTS_DIR/*instruct-htp.json config.json\njq --arg pwd "$PWD" \'.dialog.tokenizer.path |= if startswith($pwd + "/") then . else $pwd + "/" + . end\' config.json > tmp && mv tmp config.json\njq --arg pwd "$PWD" \'.dialog.engine.backend.extensions |= if startswith($pwd + "/") then . else $pwd + "/" + . end\' config.json > tmp && mv tmp config.json\njq --arg pwd "$PWD" \'.dialog.engine.model.binary["ctx-bins"] |= map(if startswith($pwd + "/") then . else $pwd + "/" + . end)\' config.json > tmp && mv tmp config.json\n\n# Symlink other files\nln -s $WEIGHTS_DIR/*.json .\nln -s $WEIGHTS_DIR/*okenizer.json tokenizer.json\nln -s $WEIGHTS_DIR/*.serialized.bin .\necho "prompt_tags_1: <|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.\nprompt_tags_2: <|im_end|>\\n<|im_start|>assistant\\n" > prompt.conf\n\n# Navigate back to samples/ directory\ncd ../../../..\n\n# Create empty tokenizer files, otherwise they will be downloaded... (which will fail)\nif [ ! -f genie/python/models/Phi-3.5-mini/tokenizer.json ]; then\n    echo \'{}\' > genie/python/models/Phi-3.5-mini/tokenizer.json\nfi\nif [ ! -f genie/python/models/IBM-Granite-v3.1-8B/tokenizer.json ]; then\n    echo \'{}\' > genie/python/models/IBM-Granite-v3.1-8B/tokenizer.json\nfi\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Run the Web UI (from the ",(0,t.jsx)(n.code,{children:"samples/"})," directory):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Find the IP address of your development board\nifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n\n# ... Example:\n# 192.168.1.253\n\n# Run the Web UI\npython webui/GenieWebUI.py\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Now open ",(0,t.jsx)(n.a,{href:"http://192.168.1.253:8976",children:"http://192.168.1.253:8976"}),' (replace with your IP) in your web browser (on your computer) to interact with the model. Make sure to select the model first using the "models" dropdown.']}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-d22ee694bd20caf302bda5cc6c697f3c8575ada4%2Fgenie-webui.png?alt=media",alt:"",title:"ai-engine-direct-helper WebUI demo"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You can also programmatically access this server using the OpenAI Chat Completions API. E.g. from Python:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Start the server (from the ",(0,t.jsx)(n.code,{children:"samples/"})," directory):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'python genie/python/GenieAPIService.py --modelname "qwen2_5-0_5b-instruct"   --loadmodel --profile\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["From a ",(0,t.jsx)(n.em,{children:"new terminal"}),", create a new venv and install ",(0,t.jsx)(n.code,{children:"requests"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python3 -m venv .venv-chat\nsource .venv/bin/activate\npip3 install requests\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create a new file ",(0,t.jsx)(n.code,{children:"chat.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import requests\n\n# if running from your own computer, replace localhost with the IP address of your development board\nurl = "http://localhost:8910/v1/chat/completions"\n\npayload = {\n    "model": "qwen2_5-0_5b-instruct",\n    "messages": [\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Explain Qualcomm in one sentence."}\n    ],\n    "temperature": 0.7,\n    "max_tokens": 200\n}\n\nresponse = requests.post(url, headers={ "Content-Type": "application/json" }, json=payload)\nprint(response.json())\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Run ",(0,t.jsx)(n.code,{children:"chat.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python3 chat.py\n\n# {'id': 'genie-llm', 'model': 'IBM-Granite', 'object': 'chat.completion', 'created': 1757512757, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Qualcomm is a leading American technology company that designs, manufactures, and markets mobile phone chips and other wireless communication products.', 'tool_call_id': None, 'tool_calls': None}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["(Model seems to always return ",(0,t.jsx)(n.code,{children:"IBM-Granite"}),", you can disregard this)"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tips-and-tricks",children:"Tips and tricks"}),"\n",(0,t.jsx)(n.h3,{id:"downloading-files-from-huggingface-that-require-authentication",children:"Downloading files from HuggingFace that require authentication"}),"\n",(0,t.jsxs)(n.p,{children:["If you want to download files, e.g. the ",(0,t.jsx)(n.code,{children:"tokenizer.json"})," file from ",(0,t.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/",children:"Llama-3.2-1B-Instruct"}),", that require permission or authentication:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Go to ",(0,t.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/",children:"the model page on HuggingFace"}),", sign in (or sign up), and fill in the form to get access to the model."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create a new HuggingFace access token with 'Read' permissions at ",(0,t.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"https://huggingface.co/settings/tokens"}),", and configure it on your development board:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"export HF_TOKEN=hf_gs...\n\n# Optionally add ^ to ~/.bash_profile to ensure it gets loaded automatically in the future.\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Once you're granted access you can now download the tokenizer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'wget --header="Authorization: Bearer $HF_TOKEN" https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/tokenizer.json\n'})}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);