"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3652],{11610:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Document Home/ai/litert_tflite","title":"LiteRT / TFLite","description":"LiteRT, formerly known as TensorFlow Lite, is Google\'s high-performance runtime for on-device AI. You can run existing quantized LiteRT models (in Python or C++) on the NPU on Dragonwing devices with a single line of code using the LiteRT delegates that are part of AI Engine Direct.","source":"@site/docs/Document Home/8.ai/3.litert_tflite.md","sourceDirName":"Document Home/8.ai","slug":"/Document Home/ai/litert_tflite","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/litert_tflite","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/8.ai/3.litert_tflite.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Qualcomm AI Hub","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/qualcomm_ai_hub"},"next":{"title":"ONNX","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/onnx"}}');var o=t(74848),a=t(28453);const r={},s="LiteRT / TFLite",l={},d=[{value:"Quantizing models",id:"quantizing-models",level:2},{value:"Running a model on the NPU (Python)",id:"running-a-model-on-the-npu-python",level:2},{value:"Running a model on the NPU (C++)",id:"running-a-model-on-the-npu-c",level:2},{value:"Example: Vision Transformers (Python)",id:"example-vision-transformers-python",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"litert--tflite",children:"LiteRT / TFLite"})}),"\n",(0,o.jsx)(n.p,{children:"LiteRT, formerly known as TensorFlow Lite, is Google's high-performance runtime for on-device AI. You can run existing quantized LiteRT models (in Python or C++) on the NPU on Dragonwing devices with a single line of code using the LiteRT delegates that are part of AI Engine Direct."}),"\n",(0,o.jsx)(n.h2,{id:"quantizing-models",children:"Quantizing models"}),"\n",(0,o.jsxs)(n.p,{children:["The NPU only supports uint8/int8 quantized models. Unsupported models, or unsupported layers will be automatically moved back to the CPU. You can use ",(0,o.jsx)(n.a,{href:"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide",children:"quantization-aware training"})," or ",(0,o.jsx)(n.a,{href:"https://ai.google.dev/edge/litert/models/post_training_quantization",children:"post-training quantization"}),' to quantize your LiteRT models. Make sure you follow the steps for "Full integer quantization".']}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Don't want to quantize yourself?"})," You can download a range of pre-quantized models from ",(0,o.jsx)(n.a,{href:"https://aihub.qualcomm.com",children:"Qualcomm AI Hub"}),", or use ",(0,o.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/edge-impulse",children:"Edge Impulse"})," to quantize new or existing models."]})}),"\n",(0,o.jsx)(n.h2,{id:"running-a-model-on-the-npu-python",children:"Running a model on the NPU (Python)"}),"\n",(0,o.jsx)(n.p,{children:"To offload a model to the NPU, you just need to load the LiteRT delegate; and pass it into the interpreter. E.g.:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from ai_edge_litert.interpreter import Interpreter, load_delegate\n\nqnn_delegate = load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})\ninterpreter = Interpreter(\n    model_path=...,\n    experimental_delegates=[qnn_delegate]\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"running-a-model-on-the-npu-c",children:"Running a model on the NPU (C++)"}),"\n",(0,o.jsx)(n.p,{children:"To offload a model to the NPU, you'll first need to add the following compile flags:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-makefile",children:"CFLAGS += -I${QNN_SDK_ROOT}/include\nLDFLAGS += -L${QNN_SDK_ROOT}/lib/aarch64-ubuntu-gcc9.4 -lQnnTFLiteDelegate\n"})}),"\n",(0,o.jsx)(n.p,{children:"Then, you instantiate the LiteRT delegate and pass it to the LiteRT interpreter:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-c",children:'// == Includes ==\n#include "QNN/TFLiteDelegate/QnnTFLiteDelegate.h"\n\n// == Application code ==\n\n// Get your interpreter...\ntflite::Interpreter *interpreter = ...;\n\n// Create QNN Delegate options structure.\nTfLiteQnnDelegateOptions options = TfLiteQnnDelegateOptionsDefault();\n\n// Set the mandatory backend_type option. All other options have default values.\noptions.backend_type = kHtpBackend;\n\n// Instantiate delegate. Must not be freed until interpreter is freed.\nTfLiteDelegate* delegate = TfLiteQnnDelegateCreate(&options);\n\nTfLiteStatus status = interpreter->ModifyGraphWithDelegate(delegate);\n// Check that status == kTfLiteOk\n'})}),"\n",(0,o.jsx)(n.h2,{id:"example-vision-transformers-python",children:"Example: Vision Transformers (Python)"}),"\n",(0,o.jsxs)(n.p,{children:["Here's how you can run a Vision Transformer model (downloaded from ",(0,o.jsx)(n.a,{href:"https://aihub.qualcomm.com/models/vit",children:"AI Hub"}),") on both the CPU and the NPU using the LiteRT delegates."]}),"\n",(0,o.jsx)(n.p,{children:"Open the terminal on your development board, or an ssh session to your development board, and:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Create a new venv, and install the LiteRT runtime and Pillow:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"mkdir -p litert-demo/\ncd litert-demo/\n\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install ai-edge-litert==1.3.0 Pillow\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"inference_vit.py"})," and add:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'import numpy as np\nfrom ai_edge_litert.interpreter import Interpreter, load_delegate\nfrom PIL import Image\nimport os, time, sys\nimport urllib.request\n\ndef curr_ms():\n    return round(time.time() * 1000)\n\nuse_npu = True if len(sys.argv) >= 2 and sys.argv[1] == \'--use-npu\' else False\n\n# Path to your quantized TFLite model and test image (will be download automatically)\nMODEL_PATH = "vit-vit-w8a8.tflite"\nIMAGE_PATH = "boa-constrictor.jpg"\nLABELS_PATH = "vit-vit-labels.txt"\n\nif not os.path.exists(MODEL_PATH):\n    print("Downloading model...")\n    model_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-w8a8.tflite\'\n    urllib.request.urlretrieve(model_url, MODEL_PATH)\n\nif not os.path.exists(LABELS_PATH):\n    print("Downloading labels...")\n    labels_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/models/vit-vit-labels.txt\'\n    urllib.request.urlretrieve(labels_url, LABELS_PATH)\n\nif not os.path.exists(IMAGE_PATH):\n    print("Downloading image...")\n    image_url = \'https://cdn.edgeimpulse.com/qc-ai-docs/examples/boa-constrictor.jpg\'\n    urllib.request.urlretrieve(image_url, IMAGE_PATH)\n\nwith open(LABELS_PATH, \'r\') as f:\n    labels = [line for line in f.read().splitlines() if line.strip()]\n\nexperimental_delegates = []\nif use_npu:\n    experimental_delegates = [load_delegate("libQnnTFLiteDelegate.so", options={"backend_type": "htp"})]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=MODEL_PATH,\n    experimental_delegates=experimental_delegates\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensor details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load and preprocess image\ndef load_image(path, input_shape):\n    # Expected input shape: [1, height, width, channels]\n    _, height, width, channels = input_shape\n\n    img = Image.open(path).convert("RGB").resize((width, height))\n    img_np = np.array(img, dtype=np.uint8)  # quantized models expect uint8\n    img_np = np.expand_dims(img_np, axis=0)\n    return img_np\n\ninput_shape = input_details[0][\'shape\']\ninput_data = load_image(IMAGE_PATH, input_shape)\n\n# Set tensor and run inference\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\n\n# Run once to warmup\ninterpreter.invoke()\n\n# Then run 10x\nstart = curr_ms()\nfor i in range(0, 10):\n    interpreter.invoke()\nend = curr_ms()\n\n# Get prediction\nq_output = interpreter.get_tensor(output_details[0][\'index\'])\nscale, zero_point = output_details[0][\'quantization\']\nf_output = (q_output.astype(np.float32) - zero_point) * scale\n\n# Image classification models in AI Hub miss a Softmax() layer at the end of the model, so add it manually\ndef softmax(x, axis=-1):\n    # subtract max for numerical stability\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n# show top-5 predictions\nscores = softmax(f_output[0])\ntop_k = scores.argsort()[-5:][::-1]\nprint("\\nTop-5 predictions:")\nfor i in top_k:\n    print(f"Class {labels[i]}: score={scores[i]}")\n\nprint(\'\')\nprint(f\'Inference took (on average): {(end - start) / 10}ms. per image\')\n'})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Run the model on the CPU:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py\n\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6264431476593018\n# Class rock python: score=0.047579940408468246\n# Class night snake: score=0.006721484009176493\n# Class mouse: score=0.0022421202156692743\n# Class pick: score=0.001942973816767335\n#\n# Inference took (on average): 391.1ms. per image\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Run the model on the NPU:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python3 inference_vit.py --use-npu\n\n# INFO: TfLiteQnnDelegate delegate: 1382 nodes delegated out of 1633 nodes with 27 partitions.\n#\n# INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n#\n# Top-5 predictions:\n# Class boa constrictor: score=0.6113042235374451\n# Class rock python: score=0.038359832018613815\n# Class night snake: score=0.011630792170763016\n# Class mouse: score=0.002294909441843629\n# Class lens cap: score=0.0018960189772769809\n#\n# Inference took (on average): 132.7ms. per image\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'As you can see this model runs significantly faster on NPU - but there\'s a slight change in the output of the model. You can also see that for this model not all layers can run on NPU ("1382 nodes delegated out of 1633 nodes with 27 partitions").'})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(96540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);