"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3800],{28453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>i});var s=l(96540);const a={},o=s.createContext(a);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}},57559:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>t,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Document Home/ai/llama_cpp","title":"Llama.cpp","description":"You can a wide range of Large Language Models (LLMs) and Vision Language Models (VLMs) on your Dragowning development boards using llama.cpp. Models running under llama.cpp run on the GPU, not on the NPU. You can run a subset of models on the NPU via GENIE.","source":"@site/docs/Document Home/8.ai/5.llama_cpp.md","sourceDirName":"Document Home/8.ai","slug":"/Document Home/ai/llama_cpp","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/llama_cpp","draft":false,"unlisted":false,"editUrl":"https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/8.ai/5.llama_cpp.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ONNX","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/onnx"},"next":{"title":"Genie","permalink":"/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai/genie"}}');var a=l(74848),o=l(28453);const r={},i="Llama.cpp",t={},c=[{value:"Builing llama.cpp",id:"builing-llamacpp",level:2},{value:"Downloading and quantizing a model",id:"downloading-and-quantizing-a-model",level:3},{value:"Running your first LLM using llama-cli",id:"running-your-first-llm-using-llama-cli",level:3},{value:"Serving LLMs using llama-server",id:"serving-llms-using-llama-server",level:3},{value:"Serving multi-modal LLMs",id:"serving-multi-modal-llms",level:3},{value:"Tips &amp; tricks",id:"tips--tricks",level:2},{value:"Comparing CPU performance",id:"comparing-cpu-performance",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"llamacpp",children:"Llama.cpp"})}),"\n",(0,a.jsxs)(n.p,{children:["You can a wide range of Large Language Models (LLMs) and Vision Language Models (VLMs) on your Dragowning development boards using ",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp",children:"llama.cpp"}),". Models running under llama.cpp run on the ",(0,a.jsx)(n.em,{children:"GPU"}),", not on the ",(0,a.jsx)(n.em,{children:"NPU"}),". You can run a subset of models on the NPU via ",(0,a.jsx)(n.a,{href:"https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/genie",children:"GENIE"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"builing-llamacpp",children:"Builing llama.cpp"}),"\n",(0,a.jsx)(n.p,{children:"You'll need to build some dependencies for llama.cpp. Open the terminal on your development board, or an ssh session to your development board, and run:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Install build dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install -y cmake ninja-build curl libcurl4-openssl-dev\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Install the OpenCL headers and ICD loader library:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'mkdir -p ~/dev/llm\n\n# Symlink the OpenCL shared library\nsudo rm -f /usr/lib/libOpenCL.so\nsudo ln -s /lib/aarch64-linux-gnu/libOpenCL.so.1.0.0 /usr/lib/libOpenCL.so\n\n# OpenCL headers\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-Headers\ncd OpenCL-Headers\ngit checkout 5d52989617e7ca7b8bb83d7306525dc9f58cdd46\nmkdir -p build && cd build\ncmake .. -G Ninja \\\n    -DBUILD_TESTING=OFF \\\n    -DOPENCL_HEADERS_BUILD_TESTING=OFF \\\n    -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF \\\n    -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"\ncmake --build . --target install\n\n# ICD Loader\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader\ncd OpenCL-ICD-Loader\ngit checkout 02134b05bdff750217bf0c4c11a9b13b63957b04\nmkdir -p build && cd build\ncmake .. -G Ninja \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_PREFIX_PATH="$HOME/dev/llm/opencl" \\\n    -DCMAKE_INSTALL_PREFIX="$HOME/dev/llm/opencl"\ncmake --build . --target install\n\n# Symlink OpenCL headers\nsudo rm -f /usr/include/CL\nsudo ln -s ~/dev/llm/opencl/include/CL/ /usr/include/CL\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Build llama.cpp with the OpenCL backend:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/dev/llm\n\n# Clone repository\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp\n\n# We've tested this commit explicitly, you can try master if you want bleeding edge\ngit checkout f6da8cb86a28f0319b40d9d2a957a26a7d875f8c\n\n# Build\nmkdir -p build\ncd build\ncmake .. -G Ninja \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DGGML_OPENCL=ON\nninja -j`nproc`\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Add the llama.cpp paths to your PATH:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cd ~/dev/llm/llama.cpp/build/bin\n\necho "" >> ~/.bash_profile\necho "# Begin llama.cpp" >> ~/.bash_profile\necho "export PATH=\\$PATH:$PWD" >> ~/.bash_profile\necho "# End llama.cpp" >> ~/.bash_profile\necho "" >> ~/.bash_profile\n\n# To use the llama.cpp files in your current session\nsource ~/.bash_profile\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"You now have llama.cpp:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"llama-cli --version\n# ggml_opencl: selected platform: 'QUALCOMM Snapdragon(TM)'\n# ggml_opencl: device: 'QUALCOMM Adreno(TM) 635 (OpenCL 3.0 Adreno(TM) 635)'\n# ggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: 0808.0.7 Compiler E031.49.02.00\n# ggml_opencl: vector subgroup broadcast support: true\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"downloading-and-quantizing-a-model",children:"Downloading and quantizing a model"}),"\n",(0,a.jsxs)(n.p,{children:["To run GPU-accelerated models you'll want pure 4-bit quantized (",(0,a.jsx)(n.code,{children:"Q4_0"}),") models in GGUF format (the llama.cpp format, ",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp/discussions/2948",children:"conversion guide"}),"). You can either find pre-quantized models, or quantize a model yourself using ",(0,a.jsx)(n.code,{children:"llama-quantize"}),". For example, for Qwen2-1.5B-Instruct:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Grab ",(0,a.jsx)(n.a,{href:"https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF",children:"Qwen2-1.5B-Instruct"})," in fp16 format from HuggingFace, and quantize using ",(0,a.jsx)(n.code,{children:"llama-quantize"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Download fp16 model\nwget https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-fp16.gguf\n\n# Quantize (pure Q4_0)\nllama-quantize --pure qwen2-1_5b-instruct-fp16.gguf qwen2-1_5b-instruct-q4_0-pure.gguf Q4_0\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Now follow either the llama.cpp compilation instructions to run this model."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"running-your-first-llm-using-llama-cli",children:"Running your first LLM using llama-cli"}),"\n",(0,a.jsxs)(n.p,{children:["You're now ready to run the LLM via ",(0,a.jsx)(n.code,{children:"llama-cli"}),". It'll automatically offload layers to the GPU:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off\n\n# ... You\'ll see:\n# load_tensors: offloaded 29/29 layers to GPU\n# ...\n# Knock knock, 11:59 pm ... rest of the story\n'})}),"\n",(0,a.jsx)(n.p,{children:"\ud83d\ude80 You now have an LLM running on the GPU of your device!"}),"\n",(0,a.jsx)(n.h3,{id:"serving-llms-using-llama-server",children:"Serving LLMs using llama-server"}),"\n",(0,a.jsxs)(n.p,{children:["Next, you can use ",(0,a.jsx)(n.code,{children:"llama-server"})," to start a web server with a chat interface, and an OpenAI compatible chat completions API."]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"First, find the IP address of your development board:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1'\n\n# ... Example:\n# 192.168.1.253\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Start the server via:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"llama-server -m ./qwen2-1_5b-instruct-q4_0-pure.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["On your computer, open a web browser and navigate to ",(0,a.jsx)(n.code,{children:"http://192.168.1.253:9876"})," (replace the IP address with the one you found in 1.):"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-1c1d0f018f51abf311522cf4f398bcc4b69fb102%2Fllamacpp1.png?alt=media",alt:"",title:"Serving LLMs using llama-server"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"You can also programmatically access this server using the OpenAI Chat Completions API. E.g. from Python:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a new venv and install ",(0,a.jsx)(n.code,{children:"requests"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 -m venv .venv-chat\nsource .venv/bin/activate\npip3 install requests\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a new file ",(0,a.jsx)(n.code,{children:"chat.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import requests\n\n# if running from your own computer, replace localhost with the IP address of your development board\nurl = "http://localhost:9876/v1/chat/completions"\n\npayload = {\n    "messages": [\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "Explain Qualcomm in one sentence."}\n    ],\n    "temperature": 0.7,\n    "max_tokens": 200\n}\n\nresponse = requests.post(url, headers={ "Content-Type": "application/json" }, json=payload)\nprint(response.json())\n'})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Run ",(0,a.jsx)(n.code,{children:"chat.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python3 chat.py\n\n# ...\n# {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': 'Qualcomm is a leading global technology company that designs, develops, licenses, and markets semiconductor-based products and mobile platform technologies to major telecommunications and consumer electronics manufacturers worldwide.'}}], 'created': 1757073340, 'model': 'gpt-3.5-turbo', 'system_fingerprint': 'b6362-f6da8cb8', 'object': 'chat.completion', 'usage': {'completion_tokens': 34, 'prompt_tokens': 26, 'total_tokens': 60}, 'id': 'chatcmpl-3O7l005WG1DzN191FTNomJNweHMoH8Is', 'timings': {'prompt_n': 12, 'prompt_ms': 303.581, 'prompt_per_token_ms': 25.298416666666668, 'prompt_per_second': 39.52816546490064, 'predicted_n': 34, 'predicted_ms': 4052.23, 'predicted_per_token_ms': 119.18323529411765, 'predicted_per_second': 8.390441806116632}}\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"serving-multi-modal-llms",children:"Serving multi-modal LLMs"}),"\n",(0,a.jsxs)(n.p,{children:["You can also use multi-modal LLMs. For example ",(0,a.jsx)(n.a,{href:"https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF",children:"SmolVLM-500M-Instruct-GGUF"}),". Download both the Q4_0 quantized weights (or quantize them yourself), and download the CLIP encoder ",(0,a.jsx)(n.code,{children:"mmproj-*.gguf"})," file. For example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Download weights\nwget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf\nwget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf\n\n# Quantize model (mmproj- models are not quantizable via llama-quantize, see below)\nllama-quantize --pure SmolVLM-500M-Instruct-f16.gguf SmolVLM-500M-Instruct-q4_0-pure.gguf Q4_0\n\n# Server the model\nllama-server -m ./SmolVLM-500M-Instruct-q4_0-pure.gguf --mmproj ./mmproj-SmolVLM-500M-Instruct-f16.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-272f54a74290f52156033bda8b2d480621ae78ab%2Fllamacpp2.png?alt=media%22",alt:"",title:"Serving multi-modal LLMs using llama-server"})}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"CLIP model is still fp16:"})," The ",(0,a.jsx)(n.code,{children:"mmproj"})," model is still fp16; and thus processing images will be slow. There's code to quantize the CLIP encoder in ",(0,a.jsx)(n.a,{href:"https://github.com/ggml-org/llama.cpp/pull/11644",children:"older versions of llama.cpp"}),"."]})}),"\n",(0,a.jsx)(n.h2,{id:"tips--tricks",children:"Tips & tricks"}),"\n",(0,a.jsx)(n.h3,{id:"comparing-cpu-performance",children:"Comparing CPU performance"}),"\n",(0,a.jsxs)(n.p,{children:["Add ",(0,a.jsx)(n.code,{children:"-ngl 0"})," to the ",(0,a.jsx)(n.code,{children:"llama-*"})," commands to skip offloading layers to the GPU. Models will run on CPU, and you can compare performance with GPU."]}),"\n",(0,a.jsx)(n.p,{children:"E.g. for the Qwen2-1.5B-Instruct Q4_0 on RB3 Gen 2 Vision Kit:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"GPU:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off\n\n# llama_perf_sampler_print:    sampling time =     225.78 ms /   133 runs   (    1.70 ms per token,   589.06 tokens per second)\n# llama_perf_context_print:        load time =    5338.13 ms\n# llama_perf_context_print: prompt eval time =     201.32 ms /     5 tokens (   40.26 ms per token,    24.84 tokens per second)\n# llama_perf_context_print:        eval time =   13214.35 ms /   127 runs   (  104.05 ms per token,     9.61 tokens per second)\n# llama_perf_context_print:       total time =   18958.06 ms /   132 tokens\n# llama_perf_context_print:    graphs reused =        122\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"CPU:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -ngl 99 -c 2048 -s 11 -n 128 -p "Knock knock, " -fa off -ngl 0\n\n# llama_perf_sampler_print:    sampling time =      23.47 ms /   133 runs   (    0.18 ms per token,  5666.08 tokens per second)\n# llama_perf_context_print:        load time =     677.25 ms\n# llama_perf_context_print: prompt eval time =     253.39 ms /     5 tokens (   50.68 ms per token,    19.73 tokens per second)\n# llama_perf_context_print:        eval time =   17751.29 ms /   127 runs   (  139.77 ms per token,     7.15 tokens per second)\n# llama_perf_context_print:       total time =   18487.26 ms /   132 tokens\n# llama_perf_context_print:    graphs reused =        122\n'})}),"\n",(0,a.jsx)(n.p,{children:"Here the GPU evaluates tokens ~33% faster than the CPU."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);