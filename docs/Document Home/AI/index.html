<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Document Home/AI" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Artificial Intelligence | RUBIK Pi Ubuntu24.04 user Guide</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Artificial Intelligence | RUBIK Pi Ubuntu24.04 user Guide"><meta data-rh="true" name="description" content="This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on the RUBIK Pi 3."><meta data-rh="true" property="og:description" content="This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on the RUBIK Pi 3."><link data-rh="true" rel="icon" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/favicon.png"><link data-rh="true" rel="canonical" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI" hreflang="en"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Document Home","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/document-home"},{"@type":"ListItem","position":2,"name":"Artificial Intelligence","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI"}]}</script><link rel="alternate" type="application/rss+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/rss.xml" title="RUBIK Pi Ubuntu24.04 user Guide RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/atom.xml" title="RUBIK Pi Ubuntu24.04 user Guide Atom Feed"><link rel="stylesheet" href="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/css/styles.a22174bd.css">
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/runtime~main.9fdd5390.js" defer="defer"></script>
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/main.22a7b2ed.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><div class="navbar__logo"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">RUBIK Pi Ubuntu24.04 User Guide</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/document-home">Tutorial</a><a class="navbar__item navbar__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/document-home">Document Home</a><button aria-label="Collapse sidebar category &#x27;Document Home&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/">Quick Start</a><button aria-label="Collapse sidebar category &#x27;Quick Start&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/Accessories">Accessories</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/Set up your device">Set up your device</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/Run sample applications">Run sample applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/Update software">Update software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Quick Start/Further study">Further study</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Peripherals and Interfaces/">Peripherals and Interfaces</a><button aria-label="Expand sidebar category &#x27;Peripherals and Interfaces&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/IMSDK-TFLite">IMSDK-TFLite</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Edge-Impulse">Edge-Impulse</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/develop-applications">Develop your own application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Robot-development">Robot-development</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Camera-software">Camera software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/AI">Artificial Intelligence</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server">ubuntu-desktop-vs-server</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/IoT-connectivity">IoT-connectivity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Linux-Kernel">Linux Kernel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Datasheet">Datasheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Troubleshooting">FAQ</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/development-regulatory-notice">Development regulatory notice</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/document-home"><span>Document Home</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Artificial Intelligence</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Artificial Intelligence</h1></header>
<p>This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on the RUBIK Pi 3.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>The AI/ML development process for RUBIK Pi 3 Ubuntu is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-17-72ba88d633feb2c5a2f010e13c24323a.png" width="1379" height="171" class="img_ev3q"></p>
<p>The above AI/ML developer workflow consists of two steps:</p>
<p>Step 1</p>
<p>■ Compile and optimize the model from the third-party AI framework to efficiently run on Qualcomm hardware. For example, a Tensorflow model can be exported to a TFLite model. Optionally, quantize, fine-tune performance, and accuracy using hardware-specific customizations.</p>
<p>Step 2</p>
<p>Build an application to use the optimized model to run on device inference</p>
<p>■ Integrate the AI model into the use case pipeline.</p>
<p>■ Cross-compile the application to generate an executable binary file that uses the required libraries.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="software-and-hardware-architecture">Software and hardware architecture<a href="#software-and-hardware-architecture" class="hash-link" aria-label="Direct link to Software and hardware architecture" title="Direct link to Software and hardware architecture">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="overall-ai-framework">Overall AI framework<a href="#overall-ai-framework" class="hash-link" aria-label="Direct link to Overall AI framework" title="Direct link to Overall AI framework">​</a></h3>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-19-7dc987e876c7f8c9d496531e740f2f92.png" width="1395" height="875" class="img_ev3q"></p>
<p>Developers can import models from ONNX, PyTorch, TensorFlow or TFLite, and use the Qualcomm AI Runtime SDK to efficiently run these models on the AI hardware of the RUBIK Pi 3, including the HTP (NPU), GPU, and CPU.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hardware">AI hardware<a href="#ai-hardware" class="hash-link" aria-label="Direct link to AI hardware" title="Direct link to AI hardware">​</a></h3>
<p>■ Qualcomm Kryo™ CPU - Best-in-class CPU with high performance and remarkable power efficiency.</p>
<p>■ Qualcomm Adreno GPU - Suitable to execute AI workloads with balanced power and performance. AI workloads are accelerated with OpenCL kernels. You can use the GPU to accelerate model pre/postprocessing.</p>
<p>■ Qualcomm Hexagon - Also known as NPU/DSP/HMX, capable of executing AI workloads with low-power and high-performance. For optimized performance, quantize the pre-trained models to one of the supported precisions.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-software">AI software<a href="#ai-software" class="hash-link" aria-label="Direct link to AI software" title="Direct link to AI software">​</a></h3>
<p>The AI software stack contains various SDKs to help AI developers easily leverage the powerful AI hardware accelerators of the RUBIK Pi 3. Developers can choose one of the SDKs to deploy their AI workloads. Pre-trained models (except for TFLite models) need to be converted to the executable format supported by the selected SDK  before running. TFLite models can be run directly using the TFLite Delegate.</p>
<p>■ <strong>LiteRT</strong></p>
<p>LiteRT models can be executed natively on RUBIK Pi 3 hardware with acceleration using the following Delegates.</p>
<table><thead><tr><th>Delegate</th><th>Acceleration</th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU, and HTP</td></tr><tr><td>XNNPACK Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<p>■ <strong>Qualcomm Neural Processing Engine SDK (SNPE)</strong></p>
<p>The Qualcomm Neural Processing Engine (also known as SNPE) is a software acceleration runtime for executing deep neural networks. The SNPE SDK provides tools to convert and quantize neural networks, and accelerate their execution on hardware accelerators such as the CPU, GPU, and HTP.</p>
<p>■ <strong>Qualcomm AI Engine Direct (QNN)</strong></p>
<p>Qualcomm AI Engine Direct is a software architecture designed for AI/ML use cases to leverage the AI accelerator hardware on the RUBIK Pi 3.</p>
<p>The architecture is designed to provide a unified API and modular and extensible pre-accelerator libraries, which form a reusable basis for full stack AI solutions. It provides support for runtimes such as Qualcomm Neural Processing SDK and TFLite AI Engine Direct Delegate.</p>
<p>■ <strong>AI Model Efficiency Toolkit (AIMET)</strong></p>
<p>Open-source library to optimize (compressing and quantizing) trained neural network models. This is a complex SDK designed to generate optimized quantized models, intended only for advanced developers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="compile-and-optimize-the-model">Compile and optimize the model<a href="#compile-and-optimize-the-model" class="hash-link" aria-label="Direct link to Compile and optimize the model" title="Direct link to Compile and optimize the model">​</a></h2>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-1-d83cffabac63309d8a0726d77da5e099.png" width="1386" height="171" class="img_ev3q"></p>
<p>Use either of the following two ways to compile and optimize your models:</p>
<table><thead><tr><th>Method</th><th>Operation</th></tr></thead><tbody><tr><td>AI hub</td><td>Developers can import their own models and try out the pre-optimized models on cloud devices (Snapdragon devices).</td></tr><tr><td>AI software stack</td><td>- Optimize Model with LiteRT<p>Directly port the LiteRT AI model to the RUBIK Pi 3 device</p>- Optimize AI Model with Qualcomm AI Runtime SDK<p>Use the integrated and easily customizable Qualcomm AI Runtime (QAIRT) SDK to port your model</p></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub">AI Hub<a href="#ai-hub" class="hash-link" aria-label="Direct link to AI Hub" title="Direct link to AI Hub">​</a></h3>
<p>AI Hub provides a way to optimize, validate, and deploy machine learning models on-device for vision, audio, and speech use cases.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-2-3a3b9e96a1f9946bd6c8327984756c56.png" width="1320" height="531" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="environment-setup">Environment setup<a href="#environment-setup" class="hash-link" aria-label="Direct link to Environment setup" title="Direct link to Environment setup">​</a></h4>
<ol>
<li>
<p>Install miniconda and configure the Python environment on your computer.</p>
<ol>
<li>
<p>Download <a href="https://www.anaconda.com/download" target="_blank" rel="noopener noreferrer">miniconda</a> from the official website and install it.</p>
</li>
<li>
<p>Open the command-line interface.</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Windows</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">macOS/Linux</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><p>After the installation is complete, open the <a href="https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html" target="_blank" rel="noopener noreferrer">Anaconda</a> prompt window through the Start menu.</p></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>After the installation is complete, open a new shell.</p></div></div></div>
</li>
<li>
<p>Set up a Python virtual environment for AI Hub. In the opened command-line interface, input the following commands:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">conda activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda create python=3.10 -n qai_hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate qai_hub</span><br></span></code></pre></div></div>
</li>
</ol>
</li>
<li>
<p>Install the AI Hub Python client.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install qai-hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install &quot;qai-hub[torch]&quot;</span><br></span></code></pre></div></div>
</li>
<li>
<p>Log in to AI Hub.</p>
<p>Go to <a href="https://aihub.qualcomm.com/" target="_blank" rel="noopener noreferrer">AI Hub</a> and log in using your Qualcomm ID to view information about jobs you create.</p>
<p>After you log in, navigate to <strong>Account</strong> &gt; <strong>Settings</strong> &gt; <strong>API Token</strong>. This should provide an API token that you can use to configure your client.</p>
</li>
<li>
<p>Configure the client with your API token using the following command in your terminal.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qai-hub configure --api_token &lt;INSERT_API_TOKEN&gt;</span><br></span></code></pre></div></div>
<p>Use the following command to check the list of supported devices to verify that the AI Hub Python client is installed successfully.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qai-hub list-devices</span><br></span></code></pre></div></div>
<p>The following results indicate that the AI Hub Python client was installed successfully:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-20-fa25dcb50279a4e5143c9fdeee4dbcfa.png" width="1558" height="1015" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub-workflow">AI Hub workflow<a href="#ai-hub-workflow" class="hash-link" aria-label="Direct link to AI Hub workflow" title="Direct link to AI Hub workflow">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="use-a-pre-optimized-model">Use a pre-optimized model<a href="#use-a-pre-optimized-model" class="hash-link" aria-label="Direct link to Use a pre-optimized model" title="Direct link to Use a pre-optimized model">​</a></h5>
<ol>
<li>
<p>Navigate to <a href="https://aihub.qualcomm.com/iot/models" target="_blank" rel="noopener noreferrer">AI Hub Model Zoo</a> to access pre-optimized models available for RUBIK Pi 3.</p>
</li>
<li>
<p>In the left-pane, filter models available for RUBIK Pi 3 by selecting Qualcomm QCS6490 as the chipset.</p>
</li>
<li>
<p>Select a model from the filtered view to navigate to the model page.</p>
</li>
<li>
<p>On the model page, select Qualcomm QCS6490 from the drop-down list and choose <strong>TorchScript</strong> &gt; <strong>TFLite path</strong>.</p>
</li>
<li>
<p>After clicking the download button, the model download will begin. The downloaded model has already been pre-optimized and can be directly used to develop your own applications.</p>
</li>
</ol>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="import-your-own-model">Import your own model:<a href="#import-your-own-model" class="hash-link" aria-label="Direct link to Import your own model:" title="Direct link to Import your own model:">​</a></h5>
<ol>
<li>
<p>Select a pre-trained model in PyTorch or Onnx format.</p>
</li>
<li>
<p>Submit a model for compilation or optimization to AI Hub using python APIs. When submitting a compilation job, you must select a device or chipset and the target runtime to compile the model. For RUBIK Pi 3, the TFLite runtime is supported.</p>
<table><thead><tr><th><strong>Chipset</strong></th><th><strong>Runtime</strong></th><th><strong>CPU</strong></th><th><strong>GPU</strong></th><th><strong>HTP</strong></th></tr></thead><tbody><tr><td>QCS6490</td><td>TFLite</td><td>INT8,FP16, FP32</td><td>FP16,FP32</td><td>NT8,INT16</td></tr></tbody></table>
<p>On submission, AI Hub generates a unique ID for the job. You can use this job ID to view job details.</p>
</li>
<li>
<p>AI Hub optimizes the model based your device and runtime selections. Optionally, you can submit a job to profile or inference the optimized model (using Python APIs) on a real device provisioned from a device farm.</p>
<p>– Profiling: Benchmarks the model on a provisioned device and provides statistics, including average inference times at the layer level, runtime configuration, etc.</p>
<p>– Inference: Performs inference using an optimized model on data submitted as part of the inference job by running the model on a provisioned device.</p>
</li>
<li>
<p>Each submitted job will be available for review in the AI Hub portal. A submitted compilation job will provide a downloadable link to the optimized model. This optimized model can then be deployed on a local development device like RUBIK Pi 3.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="litert">LiteRT<a href="#litert" class="hash-link" aria-label="Direct link to LiteRT" title="Direct link to LiteRT">​</a></h3>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-3-68a81cf8fe228b3f0b417685af922956.png" width="1078" height="686" class="img_ev3q"></p>
<p>LiteRT is an open-source deep learning framework for on-device inference. LiteRT helps you run your models on mobile, embedded, and edge platforms by optimizing the model for latency, model size, power consumption, etc. Qualcomm supports executing TFLite models natively on RUBIK Pi 3 through TFLite Delegates as listed below.</p>
<table><thead><tr><th>Delegate</th><th>Acceleration</th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU, HTP</td></tr><tr><td>XNNPack Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-ai-runtime-sdk">Qualcomm AI Runtime SDK<a href="#qualcomm-ai-runtime-sdk" class="hash-link" aria-label="Direct link to Qualcomm AI Runtime SDK" title="Direct link to Qualcomm AI Runtime SDK">​</a></h3>
<p>Qualcomm AI Runtime SDK is an all-in-one SDK to port ML models to run on Qualcomm (RUBIK Pi 3) hardware accelerators. The SDK includes tools provided by Qualcomm Neural Processing Engine (SNPE) and Qualcomm AI Engine Direct (QNN), which can be used to convert and quantize models trained in PyTorch and TensorFlow and execute these models on CPU, GPU, and HTP. Learn more about <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/" target="_blank" rel="noopener noreferrer">SNPE</a> and <a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/" target="_blank" rel="noopener noreferrer">QNN</a>.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-4-61298e236fb55c11dc896a828185bbb2.png" width="670" height="778" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="aiml-sample-applications">AI/ML sample applications<a href="#aiml-sample-applications" class="hash-link" aria-label="Direct link to AI/ML sample applications" title="Direct link to AI/ML sample applications">​</a></h2>
<p>The AI/ML sample application demonstrates the actual scenario of feeding data from a real-time camera or a local video file, and then running the model on the RUBIK Pi 3 device. The following will detail the steps to run sample applications.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preparations">Preparations<a href="#preparations" class="hash-link" aria-label="Direct link to Preparations" title="Direct link to Preparations">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="install-the-software-package">Install the software package<a href="#install-the-software-package" class="hash-link" aria-label="Direct link to Install the software package" title="Direct link to Install the software package">​</a></h4>
<p>Ensure that the sample applications can run normally based on [Run sample applications](1.%20Quick%20Start/3.run sample applications.md)</p>
<p>For the desktop version, start Weston as follows.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">systemctl stop gdm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo dpkg-reconfigure weston-autostart</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="download-the-model-files-and-label-files">Download the model files and label files<a href="#download-the-model-files-and-label-files" class="hash-link" aria-label="Direct link to Download the model files and label files" title="Direct link to Download the model files and label files">​</a></h4>
<p>Sample application names for LiteRT and the corresponding model files and label files:</p>
<table><thead><tr><th>Sample Application</th><th>Required Model</th><th>Required Label File</th></tr></thead><tbody><tr><td>image-classification-LiteRT-from-camera/file</td><td>resnet101-resnet101-w8a8.tflite</td><td>classification_0.labels</td></tr><tr><td>object-detection-LiteRT-from-camera/file</td><td>yolov8_det_quantized.tflite</td><td>yolov8.labels</td></tr><tr><td>image-segmentation-LiteRT-from-camera/file</td><td>deeplabv3_plus_mobilenet_quantized.tflite</td><td>deeplabv3_resnet50.labels</td></tr><tr><td>pose-detection-LiteRT-from-camera/file</td><td>hrnet_pose_quantized.tflite</td><td>hrnet_pose.labels</td></tr></tbody></table>
<p>Sample application names for SNPE and the corresponding model files and label files:</p>
<table><thead><tr><th>Sample Application</th><th>Required Model</th><th>Required Label File</th></tr></thead><tbody><tr><td>image-classification-LiteRT-from-camera/file</td><td>inceptionv3.dlc</td><td>classification.labels</td></tr><tr><td>object-detection-LiteRT-from-camera/file</td><td>yolonas.dlc</td><td>yolonas.labels</td></tr></tbody></table>
<ol>
<li>
<p>Click to download the compressed package <a href="https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_sample_app_model_label.zip" target="_blank" rel="noopener noreferrer"><em>ai_sample_app_model_label.zip</em></a> of model files and label files required for the sample application .</p>
</li>
<li>
<p>Use the following command to push all model files and label files from the compressed package to the <em>opt</em> directory of the device.</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Ubuntu</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Windows</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push ./* /opt</span><br></span></code></pre></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push  /opt</span><br></span></code></pre></div></div></div></div></div>
<p>Alternatively, individually push the model files and label files required by the sample application to the <em>/opt</em> directory of the device when using the following sample application.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="litert-sample-applications">LiteRT sample applications<a href="#litert-sample-applications" class="hash-link" aria-label="Direct link to LiteRT sample applications" title="Direct link to LiteRT sample applications">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="implement-ai-functionality-by-acquiring-image-information-through-cameras">Implement AI functionality by acquiring image information through cameras<a href="#implement-ai-functionality-by-acquiring-image-information-through-cameras" class="hash-link" aria-label="Direct link to Implement AI functionality by acquiring image information through cameras" title="Direct link to Implement AI functionality by acquiring image information through cameras">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-camera">Image classification (image-classification-LiteRT-from-camera)<a href="#image-classification-image-classification-litert-from-camera" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-camera)" title="Direct link to Image classification (image-classification-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRt for inference using the resnet101-resnet101-w8a8.tflite model via HTP. The classification results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-5-b5eee26c70ad2eb4d0e851197c68f4f9.png" width="1397" height="544" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push resnet101-resnet101-w8a8.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push classification.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0  ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;&quot;  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants=&quot;Inception,q-offsets=&lt;-38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. </span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-18-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-camera">Object detection (object-detection-LiteRT-from-camera)<a href="#object-detection-object-detection-litert-from-camera" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-camera)" title="Direct link to Object detection (object-detection-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT for inference using the yolov8_det_quantized.tflite model via HTP. The object detection results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-6-f5f0a997438d579f0a6448df50765678.png" width="1395" height="542" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolov8_det_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolov8.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants=&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,    q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-22-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="semantic-segmentation-image-segmentation-litert-from-camera">Semantic segmentation (image-segmentation-LiteRT-from-camera)<a href="#semantic-segmentation-image-segmentation-litert-from-camera" class="hash-link" aria-label="Direct link to Semantic segmentation (image-segmentation-LiteRT-from-camera)" title="Direct link to Semantic segmentation (image-segmentation-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT for inference using the deeplabv3_plus_mobilenet_quantized.tflite model via HTP. The semantic segmentation results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-7-bcfdc6e7a4f2e3a12cfd030cadbce793.png" width="1388" height="594" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push deeplabv3_plus_mobilenet_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push deeplabv3_resnet50.labels  /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants=&quot;deeplab,q-offsets=&lt;-61.0&gt;,q-scales=&lt;0.06232302635908127&gt;;&quot; ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-23-1a966d6b57441eb8d7c1e07a78935316.png" width="918" height="519" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="pose-detection-pose-detection-litert-from-camera">Pose detection (pose-detection-LiteRT-from-camera)<a href="#pose-detection-pose-detection-litert-from-camera" class="hash-link" aria-label="Direct link to Pose detection (pose-detection-LiteRT-from-camera)" title="Direct link to Pose detection (pose-detection-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT for inference using the hrnet_pose_quantized.tflite model via HTP. The pose detection results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-8-ab4b44d5c06fcd984d6acf66dceb4446.png" width="1396" height="551" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push hrnet_pose_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push hrnet_pose.labels  /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants=&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-21-f68564401dde9af3270e947cddd533a9.png" width="864" height="534" class="img_ev3q"></p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="implement-ai-functionality-by-acquiring-image-information-through-recorded-mp4-files">Implement AI functionality by acquiring image information through recorded MP4 files<a href="#implement-ai-functionality-by-acquiring-image-information-through-recorded-mp4-files" class="hash-link" aria-label="Direct link to Implement AI functionality by acquiring image information through recorded MP4 files" title="Direct link to Implement AI functionality by acquiring image information through recorded MP4 files">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-file">Image classification (image-classification-LiteRT-from-file)<a href="#image-classification-image-classification-litert-from-file" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-file)" title="Direct link to Image classification (image-classification-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to retrieve images and sends them to LiteRT for inference using the resnet101-resnet101-w8a8.tflite model via HTP. The classification results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-9-418c21942ce729d02f967dea4b10920e.png" width="1413" height="524" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push resnet101-resnet101-w8a8.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push classification.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;&quot;  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants=&quot;Inception,q-offsets=&lt;-38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. </span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-18-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-file">Object detection (object-detection-LiteRT-from-file)<a href="#object-detection-object-detection-litert-from-file" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-file)" title="Direct link to Object detection (object-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to extract images and sends them to LiteRT for inference using the yolov8_det_quantized.tflite model via HTP. The object detection results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-10-82e4e0e861d68dd74f7b3e7681f78dca.png" width="1273" height="482" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolov8_det_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolov8.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants=&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,    q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-22-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="semantic-segmentation-image-segmentation-litert-from-file">Semantic segmentation (image-segmentation-LiteRT-from-file)<a href="#semantic-segmentation-image-segmentation-litert-from-file" class="hash-link" aria-label="Direct link to Semantic segmentation (image-segmentation-LiteRT-from-file)" title="Direct link to Semantic segmentation (image-segmentation-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to extract images and sends them to LiteRT for inference using the deeplabv3_plus_mobilenet_quantized.tflite model via HTP. The semantic segmentation results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-11-e48f1add367de6bdc6c3aa3360867a18.png" width="1404" height="539" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push deeplabv3_plus_mobilenet_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push deeplabv3_resnet50.labels  /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants=&quot;deeplab,q-offsets=&lt;-61.0&gt;,q-scales=&lt;0.06232302635908127&gt;;&quot; ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-23-1a966d6b57441eb8d7c1e07a78935316.png" width="918" height="519" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="pose-detection-pose-detection-litert-from-file">Pose detection (pose-detection-LiteRT-from-file)<a href="#pose-detection-pose-detection-litert-from-file" class="hash-link" aria-label="Direct link to Pose detection (pose-detection-LiteRT-from-file)" title="Direct link to Pose detection (pose-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to extract images and sends them to LiteRT for inference using the hrnet_pose_quantized.tflite model via HTP. The pose detection results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-12-9e54b1f8e1ad8a89e3ef7339586e00b9.png" width="1407" height="523" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push hrnet_pose_quantized.tflite /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push hrnet_pose.labels  /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants=&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-21-f68564401dde9af3270e947cddd533a9.png" width="864" height="534" class="img_ev3q"></p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="snpe-sample-applications">SNPE sample applications<a href="#snpe-sample-applications" class="hash-link" aria-label="Direct link to SNPE sample applications" title="Direct link to SNPE sample applications">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="implement-ai-functionality-by-acquiring-image-information-through-cameras-1">Implement AI functionality by acquiring image information through cameras<a href="#implement-ai-functionality-by-acquiring-image-information-through-cameras-1" class="hash-link" aria-label="Direct link to Implement AI functionality by acquiring image information through cameras" title="Direct link to Implement AI functionality by acquiring image information through cameras">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-camera-1">Image classification (image-classification-LiteRT-from-camera)<a href="#image-classification-image-classification-litert-from-camera-1" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-camera)" title="Direct link to Image classification (image-classification-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to SNPE for inference using the inceptionv3.dlc model via HTP. The classification results and image information are then displayed on a monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-13-b688093195ae78e15c119e8974e89698.png" width="1392" height="556" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push inceptionv3.dlc /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push classification.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink=&quot;waylandsink sync=true fullscreen=true&quot;  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-18-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-camera-1">Object detection (object-detection-LiteRT-from-camera)<a href="#object-detection-object-detection-litert-from-camera-1" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-camera)" title="Direct link to Object detection (object-detection-LiteRT-from-camera)">​</a></h5>
<p>The sample application uses a camera to capture images in real-time and sends them to SNPE for HTP inference using the yolonas.labels model. The object detection results and image information are then displayed on the monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-14-3eb09f7879e9a48b5437ba384bf2501f.png" width="1397" height="537" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolonas.dlc /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolonas.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&#x27;waylandsink fullscreen=true sync=true&#x27; split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=&quot;&lt;/heads/Mul, /heads/Sigmoid&gt;&quot; ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-22-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="implement-ai-functionality-by-acquiring-image-information-through-recorded-mp4-files-1">Implement AI functionality by acquiring image information through recorded MP4 files<a href="#implement-ai-functionality-by-acquiring-image-information-through-recorded-mp4-files-1" class="hash-link" aria-label="Direct link to Implement AI functionality by acquiring image information through recorded MP4 files" title="Direct link to Implement AI functionality by acquiring image information through recorded MP4 files">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-file-1">Image classification (image-classification-LiteRT-from-file)<a href="#image-classification-image-classification-litert-from-file-1" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-file)" title="Direct link to Image classification (image-classification-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to capture images and sends them to SNPE for HTP inference using the inceptionv3.dlc model. The classification results and image information are then displayed on the monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-15-d0b62e5c93f8bfd151ef109c944cadae.png" width="1404" height="546" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push inceptionv3.dlc /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push classification.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink=&quot;waylandsink sync=true fullscreen=true&quot;  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-18-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-file-1">Object detection (object-detection-LiteRT-from-file)<a href="#object-detection-object-detection-litert-from-file-1" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-file)" title="Direct link to Object detection (object-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to capture images and sends them to SNPE for HTP inference using the yolonas.labels model. The object detection results and image information are then displayed on the monitor through Weston. Refer to the diagram below for the specific pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-16-23d2df90b88652293e41eb061f46deed.png" width="1406" height="553" class="img_ev3q"></p>
<ul>
<li>
<p>Push the necessary model files and label files from the model compressed package to the <em>/opt</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolonas.dlc /opt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">adb push yolonas.labels /opt</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run the following commands to run the sample application:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&#x27;waylandsink fullscreen=true sync=true&#x27; split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=&quot;&lt;/heads/Mul, /heads/Sigmoid&gt;&quot; ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.</span><br></span></code></pre></div></div>
</li>
<li>
<p>The result is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-22-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="purposes-and-functions-of-ai-related-plugins">Purposes and functions of AI-related plugins<a href="#purposes-and-functions-of-ai-related-plugins" class="hash-link" aria-label="Direct link to Purposes and functions of AI-related plugins" title="Direct link to Purposes and functions of AI-related plugins">​</a></h3>
<table><thead><tr><th>Plug-in Name</th><th>Function</th></tr></thead><tbody><tr><td>qtimlsnpe</td><td>Loading and executing the SNPE DLC model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose.</td></tr><tr><td>qtimltflite</td><td>Loading and executing LiteRT TFLite model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose.</td></tr><tr><td>qtimlvconverter</td><td>Converting data from the incoming video buffer into neural network tensors, while performing the necessary format conversion and resizing.</td></tr><tr><td>qtimlvclassification</td><td>Post-processing the output tensors for classification use cases.</td></tr><tr><td>qtimlvdetection</td><td>Post-processing the output tensors for detection use cases.</td></tr><tr><td>qtimlvsegmentation</td><td>Post-processing the output tensors for pixel-level use cases such as image segmentation or depth map processing.</td></tr><tr><td>qtimlvpose</td><td>Post-processing the output tensors for pose estimation use cases.</td></tr></tbody></table></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/8.AI.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Camera-software"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Camera software</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">ubuntu-desktop-vs-server</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#software-and-hardware-architecture" class="table-of-contents__link toc-highlight">Software and hardware architecture</a><ul><li><a href="#overall-ai-framework" class="table-of-contents__link toc-highlight">Overall AI framework</a></li><li><a href="#ai-hardware" class="table-of-contents__link toc-highlight">AI hardware</a></li><li><a href="#ai-software" class="table-of-contents__link toc-highlight">AI software</a></li></ul></li><li><a href="#compile-and-optimize-the-model" class="table-of-contents__link toc-highlight">Compile and optimize the model</a><ul><li><a href="#ai-hub" class="table-of-contents__link toc-highlight">AI Hub</a></li><li><a href="#litert" class="table-of-contents__link toc-highlight">LiteRT</a></li><li><a href="#qualcomm-ai-runtime-sdk" class="table-of-contents__link toc-highlight">Qualcomm AI Runtime SDK</a></li></ul></li><li><a href="#aiml-sample-applications" class="table-of-contents__link toc-highlight">AI/ML sample applications</a><ul><li><a href="#preparations" class="table-of-contents__link toc-highlight">Preparations</a></li><li><a href="#litert-sample-applications" class="table-of-contents__link toc-highlight">LiteRT sample applications</a></li><li><a href="#snpe-sample-applications" class="table-of-contents__link toc-highlight">SNPE sample applications</a></li><li><a href="#purposes-and-functions-of-ai-related-plugins" class="table-of-contents__link toc-highlight">Purposes and functions of AI-related plugins</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/Qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>