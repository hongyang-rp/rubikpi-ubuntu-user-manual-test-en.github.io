<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Llama.cpp | RUBIK Pi Ubuntu24.04 user Guide</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama.cpp | RUBIK Pi Ubuntu24.04 user Guide"><meta data-rh="true" name="description" content="You can a wide range of Large Language Models (LLMs) and Vision Language Models (VLMs) on your Dragonwing development boards using llama.cpp. Models running under llama.cpp run on the GPU, not on the NPU. You can run a subset of models on the NPU via GENIE."><meta data-rh="true" property="og:description" content="You can a wide range of Large Language Models (LLMs) and Vision Language Models (VLMs) on your Dragonwing development boards using llama.cpp. Models running under llama.cpp run on the GPU, not on the NPU. You can run a subset of models on the NPU via GENIE."><link data-rh="true" rel="icon" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/favicon.png"><link data-rh="true" rel="canonical" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" hreflang="en"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ubuntu System V1.0.0","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100"},{"@type":"ListItem","position":2,"name":"Llama.cpp","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp"}]}</script><link rel="alternate" type="application/rss+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/rss.xml" title="RUBIK Pi Ubuntu24.04 user Guide RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/atom.xml" title="RUBIK Pi Ubuntu24.04 user Guide Atom Feed"><link rel="stylesheet" href="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/css/styles.9d6804d5.css">
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/runtime~main.af1eba00.js" defer="defer"></script>
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/main.59cc9350.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><div class="navbar__logo"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">RUBIK Pi Ubuntu24.04 User Guide</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100">Tutorial</a><a class="navbar__item navbar__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100">Ubuntu System V1.0.0</a><button aria-label="Collapse sidebar category &#x27;Ubuntu System V1.0.0&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/peripheral-compatibility-list">Peripheral Compatibility List</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/quick-start">Quick Start</a><button aria-label="Collapse sidebar category &#x27;Quick Start&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/quick-start/Get familiar">Get started with RUBIK Pi 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/quick-start/Prebuilt Sample Applications">Prebuilt Sample Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/quick-start/QDemo">Qualcomm® Qdemo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/quick-start/Explore-more">Explore Sample Applications</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/device-setup">Device Setup</a><button aria-label="Collapse sidebar category &#x27;Device Setup&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Device Setup/set-up-your-device">Setup your device</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/update-software">Update Software</a><button aria-label="Collapse sidebar category &#x27;Update Software&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Update-Software/flash-over-android">Flash Canonical Ubuntu 24.04 Over Android/QLI Build</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Update-Software/Device Setup/set-up-your-device">Device Setup</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Update-Software/upgrade-ubuntu">Upgrade to Canonical Ubuntu Build</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Update-Software/Reset-system-image">Reset to Factory Ubuntu Image</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/peripherals-and-interfaces/">Peripherals and Interfaces</a><button aria-label="Expand sidebar category &#x27;Peripherals and Interfaces&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server">Ubuntu Desktop Vs Server</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/edge_impulse">Application Development and Execution Guide</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/edge_impulse">Building AI Models</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/edge_impulse">Edge Impulse</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Building AI Models/qualcomm_ai_hub">Qualcomm® AI Hub</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite">Framework-Driven AI Sample Execution</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/litert_tflite">LiteRT / TFLite</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx">ONNX</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/llama_cpp">Llama.cpp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie">Genie</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/IMSDK/Update JSON Config">Qualcomm® IMSDK</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/IMSDK/Update JSON Config">Update JSON Config</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/IMSDK/Customize-Sample">Customize existing Sample Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/IMSDK/Develop-app">Develop your first Application</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Robotics-Sample-Applications/Robotics Sample Applications">Qualcomm® Intelligent Robotics (QIR) SDK</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Robotics-Sample-Applications/Robotics Sample Applications">Robotics Sample Applications</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Troubleshooting/update-software-advanced">Troubleshooting</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/linux-kernel">Linux Kernel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/datasheet">Datasheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/development-regulatory-notice">Development Regulatory Notice</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Dogfooding-Guide/Setup">Dogfooding-Guide</a></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/notice">Notice for Use</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/run-sample-applications">Run sample applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/edge-impulse">Use Edge Impulse</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/develop-applications">Develop Your Own Application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/robot-development">Robot Development</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/camera-software">Camera Software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/iot-connectivity">IoT Connectivity</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100"><span>Ubuntu System V1.0.0</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Application Development and Execution Guide</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Framework-Driven AI Sample Execution</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Llama.cpp</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Llama.cpp</h1></header>
<p>You can a wide range of Large Language Models (LLMs) and Vision Language Models (VLMs) on your Dragonwing development boards using <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a>. Models running under llama.cpp run on the <em>GPU</em>, not on the <em>NPU</em>. You can run a subset of models on the NPU via <a href="https://qc-ai-test.gitbook.io/qc-ai-test-docs/running-building-ai-models/genie" target="_blank" rel="noopener noreferrer">GENIE</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="building-llamacpp">Building llama.cpp<a href="#building-llamacpp" class="hash-link" aria-label="Direct link to Building llama.cpp" title="Direct link to Building llama.cpp">​</a></h2>
<p>You&#x27;ll need to build some dependencies for llama.cpp. Open the terminal on your development board, or an ssh session to your development board, and run:</p>
<ol>
<li>
<p>Install build dependencies:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt install -y cmake ninja-build curl libcurl4-openssl-dev</span><br></span></code></pre></div></div>
</li>
<li>
<p>Install the OpenCL headers and ICD loader library:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Symlink the OpenCL shared library</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo rm -f /usr/lib/libOpenCL.so</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo ln -s /lib/aarch64-linux-gnu/libOpenCL.so.1.0.0 /usr/lib/libOpenCL.so</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># OpenCL headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/KhronosGroup/OpenCL-Headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd OpenCL-Headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout 5d52989617e7ca7b8bb83d7306525dc9f58cdd46</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build &amp;&amp; cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DBUILD_TESTING=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DOPENCL_HEADERS_BUILD_TESTING=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_INSTALL_PREFIX=&quot;$HOME/dev/llm/opencl&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake --build . --target install</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ICD Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd OpenCL-ICD-Loader</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout 02134b05bdff750217bf0c4c11a9b13b63957b04</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build &amp;&amp; cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_BUILD_TYPE=Release \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_PREFIX_PATH=&quot;$HOME/dev/llm/opencl&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_INSTALL_PREFIX=&quot;$HOME/dev/llm/opencl&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake --build . --target install</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Symlink OpenCL headers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo rm -f /usr/include/CL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo ln -s ~/dev/llm/opencl/include/CL/ /usr/include/CL</span><br></span></code></pre></div></div>
</li>
<li>
<p>Build llama.cpp with the OpenCL backend:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Clone repository</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/ggml-org/llama.cpp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd llama.cpp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># We&#x27;ve tested this commit explicitly, you can try master if you want bleeding edge</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git checkout f6da8cb86a28f0319b40d9d2a957a26a7d875f8c</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake .. -G Ninja \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DCMAKE_BUILD_TYPE=Release \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DBUILD_SHARED_LIBS=OFF \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -DGGML_OPENCL=ON</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ninja -j`nproc`</span><br></span></code></pre></div></div>
</li>
<li>
<p>Add the llama.cpp paths to your PATH:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd ~/dev/llm/llama.cpp/build/bin</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;# Begin llama.cpp&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;export PATH=\$PATH:$PWD&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;# End llama.cpp&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">echo &quot;&quot; &gt;&gt; ~/.bash_profile</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># To use the llama.cpp files in your current session</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source ~/.bash_profile</span><br></span></code></pre></div></div>
</li>
<li>
<p>You now have llama.cpp:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli --version</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: selected platform: &#x27;QUALCOMM Snapdragon(TM)&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: device: &#x27;QUALCOMM Adreno(TM) 635 (OpenCL 3.0 Adreno(TM) 635)&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: OpenCL driver: OpenCL 3.0 QUALCOMM build: 0808.0.7 Compiler E031.49.02.00</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ggml_opencl: vector subgroup broadcast support: true</span><br></span></code></pre></div></div>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="downloading-and-quantizing-a-model">Downloading and quantizing a model<a href="#downloading-and-quantizing-a-model" class="hash-link" aria-label="Direct link to Downloading and quantizing a model" title="Direct link to Downloading and quantizing a model">​</a></h3>
<p>To run GPU-accelerated models you&#x27;ll want pure 4-bit quantized (<code>Q4_0</code>) models in GGUF format (the llama.cpp format, <a href="https://github.com/ggml-org/llama.cpp/discussions/2948" target="_blank" rel="noopener noreferrer">conversion guide</a>). You can either find pre-quantized models, or quantize a model yourself using <code>llama-quantize</code>. For example, for Qwen2-1.5B-Instruct:</p>
<ol>
<li>
<p>Grab <a href="https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF" target="_blank" rel="noopener noreferrer">Qwen2-1.5B-Instruct</a> in fp16 format from HuggingFace, and quantize using <code>llama-quantize</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Download fp16 model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/resolve/main/qwen2-1_5b-instruct-fp16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Quantize (pure Q4_0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-quantize --pure qwen2-1_5b-instruct-fp16.gguf qwen2-1_5b-instruct-q4_0-pure.gguf Q4_0</span><br></span></code></pre></div></div>
</li>
<li>
<p>Now follow either the llama.cpp compilation instructions to run this model.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-your-first-llm-using-llama-cli">Running your first LLM using llama-cli<a href="#running-your-first-llm-using-llama-cli" class="hash-link" aria-label="Direct link to Running your first LLM using llama-cli" title="Direct link to Running your first LLM using llama-cli">​</a></h3>
<p>You&#x27;re now ready to run the LLM via <code>llama-cli</code>. It&#x27;ll automatically offload layers to the GPU:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ... You&#x27;ll see:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># load_tensors: offloaded 29/29 layers to GPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Knock knock, 11:59 pm ... rest of the story</span><br></span></code></pre></div></div>
<p>🚀 You now have an LLM running on the GPU of your device!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="serving-llms-using-llama-server">Serving LLMs using llama-server<a href="#serving-llms-using-llama-server" class="hash-link" aria-label="Direct link to Serving LLMs using llama-server" title="Direct link to Serving LLMs using llama-server">​</a></h3>
<p>Next, you can use <code>llama-server</code> to start a web server with a chat interface, and an OpenAI compatible chat completions API.</p>
<ol>
<li>
<p>First, find the IP address of your development board:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ifconfig | grep -Eo &#x27;inet (addr:)?([0-9]*\.){3}[0-9]*&#x27; | grep -Eo &#x27;([0-9]*\.){3}[0-9]*&#x27; | grep -v &#x27;127.0.0.1&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ... Example:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 192.168.1.253</span><br></span></code></pre></div></div>
</li>
<li>
<p>Start the server via:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-server -m ./qwen2-1_5b-instruct-q4_0-pure.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876</span><br></span></code></pre></div></div>
</li>
<li>
<p>On your computer, open a web browser and navigate to <code>http://192.168.1.253:9876</code> (replace the IP address with the one you found in 1.):</p>
<p><img decoding="async" loading="lazy" src="https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-1c1d0f018f51abf311522cf4f398bcc4b69fb102%2Fllamacpp1.png?alt=media" alt="" title="Serving LLMs using llama-server" class="img_ev3q"></p>
</li>
<li>
<p>You can also programmatically access this server using the OpenAI Chat Completions API. E.g. from Python:</p>
<ol>
<li>
<p>Create a new venv and install <code>requests</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 -m venv .venv-chat</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source .venv/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install requests</span><br></span></code></pre></div></div>
</li>
<li>
<p>Create a new file <code>chat.py</code>:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> requests</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># if running from your own computer, replace localhost with the IP address of your development board</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">url </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;http://localhost:9876/v1/chat/completions&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">payload </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;messages&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;system&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;role&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;user&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;content&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Explain Qualcomm in one sentence.&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;temperature&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.7</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;max_tokens&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">response </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> requests</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">post</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">url</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> headers</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type&quot;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> json</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">payload</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">response</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">json</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
</li>
<li>
<p>Run <code>chat.py</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 chat.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># {&#x27;choices&#x27;: [{&#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;index&#x27;: 0, &#x27;message&#x27;: {&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;Qualcomm is a leading global technology company that designs, develops, licenses, and markets semiconductor-based products and mobile platform technologies to major telecommunications and consumer electronics manufacturers worldwide.&#x27;}}], &#x27;created&#x27;: 1757073340, &#x27;model&#x27;: &#x27;gpt-3.5-turbo&#x27;, &#x27;system_fingerprint&#x27;: &#x27;b6362-f6da8cb8&#x27;, &#x27;object&#x27;: &#x27;chat.completion&#x27;, &#x27;usage&#x27;: {&#x27;completion_tokens&#x27;: 34, &#x27;prompt_tokens&#x27;: 26, &#x27;total_tokens&#x27;: 60}, &#x27;id&#x27;: &#x27;chatcmpl-3O7l005WG1DzN191FTNomJNweHMoH8Is&#x27;, &#x27;timings&#x27;: {&#x27;prompt_n&#x27;: 12, &#x27;prompt_ms&#x27;: 303.581, &#x27;prompt_per_token_ms&#x27;: 25.298416666666668, &#x27;prompt_per_second&#x27;: 39.52816546490064, &#x27;predicted_n&#x27;: 34, &#x27;predicted_ms&#x27;: 4052.23, &#x27;predicted_per_token_ms&#x27;: 119.18323529411765, &#x27;predicted_per_second&#x27;: 8.390441806116632}}</span><br></span></code></pre></div></div>
</li>
</ol>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="serving-multi-modal-llms">Serving multi-modal LLMs<a href="#serving-multi-modal-llms" class="hash-link" aria-label="Direct link to Serving multi-modal LLMs" title="Direct link to Serving multi-modal LLMs">​</a></h3>
<p>You can also use multi-modal LLMs. For example <a href="https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF" target="_blank" rel="noopener noreferrer">SmolVLM-500M-Instruct-GGUF</a>. Download both the Q4_0 quantized weights (or quantize them yourself), and download the CLIP encoder <code>mmproj-*.gguf</code> file. For example:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Download weights</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/SmolVLM-500M-Instruct-f16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wget https://huggingface.co/ggml-org/SmolVLM-500M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-500M-Instruct-f16.gguf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Quantize model (mmproj- models are not quantizable via llama-quantize, see below)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-quantize --pure SmolVLM-500M-Instruct-f16.gguf SmolVLM-500M-Instruct-q4_0-pure.gguf Q4_0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Server the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama-server -m ./SmolVLM-500M-Instruct-q4_0-pure.gguf --mmproj ./mmproj-SmolVLM-500M-Instruct-f16.gguf --no-warmup -b 128 -c 2048 -s 11 -n 128 --host 0.0.0.0 --port 9876</span><br></span></code></pre></div></div>
<p><img decoding="async" loading="lazy" src="https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-272f54a74290f52156033bda8b2d480621ae78ab%2Fllamacpp2.png?alt=media%22" alt="" title="Serving multi-modal LLMs using llama-server" class="img_ev3q"></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p><strong>CLIP model is still fp16:</strong> The <code>mmproj</code> model is still fp16; and thus processing images will be slow. There&#x27;s code to quantize the CLIP encoder in <a href="https://github.com/ggml-org/llama.cpp/pull/11644" target="_blank" rel="noopener noreferrer">older versions of llama.cpp</a>.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tips--tricks">Tips &amp; tricks<a href="#tips--tricks" class="hash-link" aria-label="Direct link to Tips &amp; tricks" title="Direct link to Tips &amp; tricks">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="comparing-cpu-performance">Comparing CPU performance<a href="#comparing-cpu-performance" class="hash-link" aria-label="Direct link to Comparing CPU performance" title="Direct link to Comparing CPU performance">​</a></h3>
<p>Add <code>-ngl 0</code> to the <code>llama-*</code> commands to skip offloading layers to the GPU. Models will run on CPU, and you can compare performance with GPU.</p>
<p>E.g. for the Qwen2-1.5B-Instruct Q4_0 on RB3 Gen 2 Vision Kit:</p>
<p><strong>GPU:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_sampler_print:    sampling time =     225.78 ms /   133 runs   (    1.70 ms per token,   589.06 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        load time =    5338.13 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print: prompt eval time =     201.32 ms /     5 tokens (   40.26 ms per token,    24.84 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        eval time =   13214.35 ms /   127 runs   (  104.05 ms per token,     9.61 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:       total time =   18958.06 ms /   132 tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:    graphs reused =        122</span><br></span></code></pre></div></div>
<p><strong>CPU:</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">llama-cli -m ./qwen2-1_5b-instruct-q4_0-pure.gguf -no-cnv --no-warmup -b 128 -ngl 99 -c 2048 -s 11 -n 128 -p &quot;Knock knock, &quot; -fa off -ngl 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_sampler_print:    sampling time =      23.47 ms /   133 runs   (    0.18 ms per token,  5666.08 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        load time =     677.25 ms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print: prompt eval time =     253.39 ms /     5 tokens (   50.68 ms per token,    19.73 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:        eval time =   17751.29 ms /   127 runs   (  139.77 ms per token,     7.15 tokens per second)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:       total time =   18487.26 ms /   132 tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># llama_perf_context_print:    graphs reused =        122</span><br></span></code></pre></div></div>
<p>Here the GPU evaluates tokens ~33% faster than the CPU.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/3.Application Development and Execution Guide/2.Framework-Driven AI Sample Execution/5.llama_cpp.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/onnx"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">ONNX</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/Application Development and Execution Guide/Framework-Driven AI Sample Execution/genie"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Genie</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#building-llamacpp" class="table-of-contents__link toc-highlight">Building llama.cpp</a><ul><li><a href="#downloading-and-quantizing-a-model" class="table-of-contents__link toc-highlight">Downloading and quantizing a model</a></li><li><a href="#running-your-first-llm-using-llama-cli" class="table-of-contents__link toc-highlight">Running your first LLM using llama-cli</a></li><li><a href="#serving-llms-using-llama-server" class="table-of-contents__link toc-highlight">Serving LLMs using llama-server</a></li><li><a href="#serving-multi-modal-llms" class="table-of-contents__link toc-highlight">Serving multi-modal LLMs</a></li></ul></li><li><a href="#tips--tricks" class="table-of-contents__link toc-highlight">Tips &amp; tricks</a><ul><li><a href="#comparing-cpu-performance" class="table-of-contents__link toc-highlight">Comparing CPU performance</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/Qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>