<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Document Home/ai" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Artificial Intelligence | RUBIK Pi Ubuntu24.04 user Guide</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Artificial Intelligence | RUBIK Pi Ubuntu24.04 user Guide"><meta data-rh="true" name="description" content="This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on your RUBIK Pi 3."><meta data-rh="true" property="og:description" content="This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on your RUBIK Pi 3."><link data-rh="true" rel="icon" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/favicon.png"><link data-rh="true" rel="canonical" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai" hreflang="en"><link data-rh="true" rel="alternate" href="https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ubuntu System V1.0.0","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100"},{"@type":"ListItem","position":2,"name":"Artificial Intelligence","item":"https://rubikpi-ubuntu-user-manual-test-en.github.io/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai"}]}</script><link rel="alternate" type="application/rss+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/rss.xml" title="RUBIK Pi Ubuntu24.04 user Guide RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog/atom.xml" title="RUBIK Pi Ubuntu24.04 user Guide Atom Feed"><link rel="stylesheet" href="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/css/styles.9d6804d5.css">
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/runtime~main.a9573870.js" defer="defer"></script>
<script src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/js/main.905349ae.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><div class="navbar__logo"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/rubikpi-ubuntu-user-manual-test-en.github.io/img/rubik-pi-logo.png" alt="RUBIK Pi Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">RUBIK Pi Ubuntu24.04 User Guide</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100">Tutorial</a><a class="navbar__item navbar__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100">Ubuntu System V1.0.0</a><button aria-label="Collapse sidebar category &#x27;Ubuntu System V1.0.0&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/quick-start/">Quick Start</a><button aria-label="Expand sidebar category &#x27;Quick Start&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/peripherals-and-interfaces/">Peripherals and Interfaces</a><button aria-label="Expand sidebar category &#x27;Peripherals and Interfaces&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/run-sample-applications">Run sample applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/edge-impulse">Use Edge Impulse</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/develop-applications">Develop Your Own Application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/robot-development">Robot Development</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/camera-software">Camera Software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ai">Artificial Intelligence</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server">Comparison Between Ubuntu Desktop Version and Server Version</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/iot-connectivity">IoT Connectivity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/linux-kernel">Linux Kernel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/datasheet">Datasheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/troubleshooting">Troubleshooting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/development-regulatory-notice">Development Regulatory Notice</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/notice">Notice for Use</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/category/ubuntu-system-v100"><span>Ubuntu System V1.0.0</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Artificial Intelligence</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Artificial Intelligence</h1></header>
<p>This chapter introduces how to use the Qualcomm AI Runtime SDK. The SDK enables AI developers to easily leverage Qualcomm&#x27;s high-performance hardware for machine learning inference. It supports running or converting models trained with popular frameworks like TensorFlow, PyTorch, ONNX, and LiteRT, allowing them to run efficiently on your RUBIK Pi 3.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>The AI/ML development process for RUBIK Pi 3 Ubuntu is as follows:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-17-42008d5cf061cb50439d591bb130a429.jpg" width="1000" height="124" class="img_ev3q"></p>
<p>The above AI/ML developer workflow consists of two steps:</p>
<p>Step 1</p>
<p>Compile and optimize the model from the third-party AI framework to efficiently run on RUBIK Pi 3. For example, a Tensorflow model can be exported to a TFLite model. Optionally, quantize, fine-tune performance, and accuracy using hardware-specific customizations.</p>
<p>Step 2</p>
<p>Build an application to use the optimized model to run on device inference</p>
<ul>
<li>Integrate the AI model into the use case pipeline.</li>
<li>Cross-compile the application to generate an executable binary file that uses the required libraries.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="software-and-hardware-architecture">Software and hardware architecture<a href="#software-and-hardware-architecture" class="hash-link" aria-label="Direct link to Software and hardware architecture" title="Direct link to Software and hardware architecture">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="overall-ai-framework">Overall AI framework<a href="#overall-ai-framework" class="hash-link" aria-label="Direct link to Overall AI framework" title="Direct link to Overall AI framework">​</a></h3>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-13-3232f4794704c115830d318f11e71727.png" width="1280" height="802" class="img_ev3q"></p>
<p>Developers can import models from ONNX, PyTorch, TensorFlow or TFLite, and use the Qualcomm AI Runtime SDK to efficiently run these models on the AI hardware of the RUBIK Pi 3, including the HTP (NPU), GPU, and CPU.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hardware">AI hardware<a href="#ai-hardware" class="hash-link" aria-label="Direct link to AI hardware" title="Direct link to AI hardware">​</a></h3>
<ul>
<li>Qualcomm Kryo™ CPU - Best-in-class CPU with high performance and remarkable power efficiency.</li>
<li>Qualcomm Adreno GPU - Suitable to execute AI workloads with balanced power and performance. AI workloads are accelerated with OpenCL kernels. You can use the GPU to accelerate model pre/postprocessing.</li>
<li>Qualcomm Hexagon - Also known as NPU/DSP/HMX, capable of executing AI workloads with low-power and high-performance. For optimized performance, quantize the pre-trained models to one of the supported precisions.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-software">AI software<a href="#ai-software" class="hash-link" aria-label="Direct link to AI software" title="Direct link to AI software">​</a></h3>
<p>The AI software stack contains various SDKs to help AI developers easily leverage the powerful AI hardware accelerators of the RUBIK Pi 3. Developers can choose one of the SDKs to deploy their AI workloads. Pre-trained models (except for TFLite models) need to be converted to the executable format supported by the selected SDK before running. TFLite models can be run directly using the TFLite Delegate.</p>
<ul>
<li><strong>LiteRT</strong></li>
</ul>
<p>LiteRT models can be executed natively on RUBIK Pi 3 hardware with acceleration using the following Delegates.</p>
<table><thead><tr><th>Delegate</th><th>Acceleration</th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU, and HTP</td></tr><tr><td>XNNPACK Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<ul>
<li><strong>Qualcomm Neural Processing Engine SDK (SNPE)</strong></li>
</ul>
<p>The Qualcomm Neural Processing Engine (also known as SNPE) is a software acceleration runtime for executing deep neural networks. The SNPE SDK provides tools to convert and quantize neural networks, and accelerate their execution on hardware accelerators such as the CPU, GPU, and HTP.</p>
<ul>
<li><strong>Qualcomm AI Engine Direct (QNN)</strong></li>
</ul>
<p>Qualcomm AI Engine Direct is a software architecture designed for AI/ML use cases to leverage the AI accelerator hardware on the RUBIK Pi 3.</p>
<p>The architecture is designed to provide a unified API and modular and extensible pre-accelerator libraries, which form a reusable basis for full stack AI solutions. It provides support for runtimes such as Qualcomm Neural Processing SDK and TFLite AI Engine Direct Delegate.</p>
<ul>
<li><strong>AI Model Efficiency Toolkit (AIMET)</strong></li>
</ul>
<p>This is an open-source library to optimize (compressing and quantizing) trained neural network models. This is a complex SDK designed to generate optimized quantized models, intended only for advanced developers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="compile-and-optimize-the-model">Compile and optimize the model<a href="#compile-and-optimize-the-model" class="hash-link" aria-label="Direct link to Compile and optimize the model" title="Direct link to Compile and optimize the model">​</a></h2>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-1-c6ef0a4354b2cbbc47774f089bd26625.jpg" width="1000" height="123" class="img_ev3q"></p>
<p>Use either of the following two ways to compile and optimize your models:</p>
<table><thead><tr><th>Method</th><th>Operation</th></tr></thead><tbody><tr><td>AI hub</td><td>Developers can import their own models and try out the pre-optimized models on cloud devices (Snapdragon devices).</td></tr><tr><td>AI software stack</td><td>- Optimize Model with LiteRT<p>Directly port the LiteRT AI model to the RUBIK Pi 3 device</p>- Optimize AI Model with Qualcomm AI Runtime SDK<p>Use the integrated and easily customizable Qualcomm AI Runtime (QAIRT) SDK to port your model</p></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub">AI Hub<a href="#ai-hub" class="hash-link" aria-label="Direct link to AI Hub" title="Direct link to AI Hub">​</a></h3>
<p>AI Hub provides a way to optimize, validate, and deploy machine learning models on-device for vision, audio, and speech use cases.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-2-0db7dab67f14ed37b4747e3188270d12.jpg" width="1000" height="402" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="environment-setup">Environment setup<a href="#environment-setup" class="hash-link" aria-label="Direct link to Environment setup" title="Direct link to Environment setup">​</a></h4>
<ol>
<li>
<p>Install miniconda and configure the Python environment on your computer.</p>
<ol>
<li>
<p>Download <a href="https://www.anaconda.com/download" target="_blank" rel="noopener noreferrer">miniconda</a> from the official website and install it.</p>
</li>
<li>
<p>Open the command line window.</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Windows</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">macOS/Linux</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><p>After the installation is complete, open the <a href="https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html" target="_blank" rel="noopener noreferrer">Anaconda </a> prompt window through the Start menu.</p></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>After the installation is complete, open a new shell.</p></div></div></div>
</li>
<li>
<p>Set up a Python virtual environment for AI Hub. In the opened command-line interface, input the following commands:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">conda activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda create python=3.10 -n qai_hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate qai_hub</span><br></span></code></pre></div></div>
</li>
</ol>
</li>
<li>
<p>Install the AI Hub Python client.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install qai-hub</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install &quot;qai-hub[torch]&quot;</span><br></span></code></pre></div></div>
</li>
<li>
<p>Log in to AI Hub.</p>
<p>Go to <a href="https://aihub.qualcomm.com/" target="_blank" rel="noopener noreferrer">AI Hub</a> and log in using your Qualcomm ID to view information about jobs you create.</p>
<p>After you log in, navigate to <strong>Account</strong> &gt; <strong>Settings</strong> &gt; <strong>API Token</strong>. This should provide an API token that you can use to configure your client.</p>
</li>
<li>
<p>Configure the client with your API token using the following command in your terminal.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qai-hub configure --api_token &lt;INSERT_API_TOKEN&gt;</span><br></span></code></pre></div></div>
<p>Use the following command to check the list of supported devices to verify that the AI Hub Python client is installed successfully.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qai-hub list-devices</span><br></span></code></pre></div></div>
<p>The following results indicate that the AI Hub Python client was installed successfully:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/img_v3_02oe_a7351608-c493-478b-a3e4-4929e3b3c17g-385abd9ab28cb4c23f2e0548777a9177.png" width="1280" height="833" class="img_ev3q"></p>
</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="ai-hub-workflow">AI Hub workflow<a href="#ai-hub-workflow" class="hash-link" aria-label="Direct link to AI Hub workflow" title="Direct link to AI Hub workflow">​</a></h4>
<ul>
<li>Use a pre-optimized model</li>
</ul>
<ol>
<li>Navigate to <a href="https://aihub.qualcomm.com/iot/models" target="_blank" rel="noopener noreferrer">AI Hub Model Zoo</a> to access pre-optimized models available for RUBIK Pi 3.</li>
<li>In the left-pane, filter models available for RUBIK Pi 3 by selecting Qualcomm QCS6490 as the chipset.</li>
<li>Select a model from the filtered view to navigate to the model page.</li>
<li>On the model page, select Qualcomm QCS6490 from the drop-down list and choose <strong>TorchScript</strong> &gt; <strong>TFLite</strong> path.</li>
<li>After clicking the download button, the model download will begin. The downloaded model has already been pre-optimized and can be directly used to develop your own applications.</li>
</ol>
<ul>
<li>Import your own model</li>
</ul>
<ol>
<li>
<p>Select a pre-trained model in PyTorch or Onnx format.</p>
</li>
<li>
<p>Submit a model for compilation or optimization to AI Hub using python APIs. When submitting a compilation job, you must select a device or chipset and the target runtime to compile the model. For RUBIK Pi 3, the TFLite runtime is supported.</p>
<table><thead><tr><th><strong>Chipset</strong></th><th><strong>Runtime</strong></th><th><strong>CPU</strong></th><th><strong>GPU</strong></th><th><strong>HTP</strong></th></tr></thead><tbody><tr><td>QCS6490</td><td>TFLite</td><td>INT8, FP16, FP32</td><td>FP16, FP32</td><td>NT8, INT16</td></tr></tbody></table>
<p>On submission, AI Hub generates a unique ID for the job. You can use this job ID to view job details.</p>
</li>
<li>
<p>AI Hub optimizes the model based your device and runtime selections. Optionally, you can submit a job to profile or inference the optimized model (using Python APIs) on a real device provisioned from a device farm.</p>
<p>– Profiling: Benchmarks the model on a provisioned device and provides statistics, including average inference times at the layer level, runtime configuration, etc.</p>
<p>– Inference: Performs inference using an optimized model on data submitted as part of the inference job by running the model on a provisioned device.</p>
</li>
<li>
<p>Each submitted job will be available for review in the AI Hub portal. A submitted compilation job will provide a downloadable link to the optimized model. This optimized model can then be deployed on a local development device like RUBIK Pi 3.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="litert">LiteRT<a href="#litert" class="hash-link" aria-label="Direct link to LiteRT" title="Direct link to LiteRT">​</a></h3>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-3-0cdc3b0210199ca7f0e51bd428123c79.jpg" width="1000" height="636" class="img_ev3q"></p>
<p>LiteRT is an open-source deep learning framework for on-device inference. LiteRT helps you run your models on mobile, embedded, and edge platforms by optimizing the model for latency, model size, power consumption, etc. RUBIK Pi supports executing TFLite models natively through TFLite Delegates as listed below.</p>
<table><thead><tr><th>Delegate</th><th>Acceleration</th></tr></thead><tbody><tr><td>AI Engine Direct Delegate (QNN Delegate)</td><td>CPU, GPU, HTP</td></tr><tr><td>XNNPack Delegate</td><td>CPU</td></tr><tr><td>GPU Delegate</td><td>GPU</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="qualcomm-intelligent-multimedia-sdk">Qualcomm(®) Intelligent Multimedia SDK<a href="#qualcomm-intelligent-multimedia-sdk" class="hash-link" aria-label="Direct link to Qualcomm(®) Intelligent Multimedia SDK" title="Direct link to Qualcomm(®) Intelligent Multimedia SDK">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-qualcomm-im-sdk">Overview of Qualcomm IM SDK<a href="#overview-of-qualcomm-im-sdk" class="hash-link" aria-label="Direct link to Overview of Qualcomm IM SDK" title="Direct link to Overview of Qualcomm IM SDK">​</a></h4>
<p>The Qualcomm IM SDK provides the development environment with upstream and Qualcomm <a href="https://gstreamer.freedesktop.org/" target="_blank" rel="noopener noreferrer">GStreamer</a> plugins as APIs. You can use these APIs to develop and optimize applications, create pipelines, and customize plugins.</p>
<p>The following diagram shows the Qualcomm IM SDK framework:</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-fec1344fa150f31c14898bc3df58d450.png" width="552" height="443" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk">Functions of commonly used Qualcomm <a href="https://gstreamer.freedesktop.org/" target="_blank" rel="noopener noreferrer">GStreamer</a> plugins in the Qualcomm IM SDK<a href="#functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk" class="hash-link" aria-label="Direct link to functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk" title="Direct link to functions-of-commonly-used-qualcomm-gstreamer-plugins-in-the-qualcomm-im-sdk">​</a></h4>
<table><thead><tr><th>Plugin</th><th>Function</th></tr></thead><tbody><tr><td>qtiqmmfsrc</td><td>The qtiqmmfsrc plugin captures the video frames through Qualcomm Camera Service.</td></tr><tr><td>qtimlsnpe</td><td>Loading and executing the SNPE DLC model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose.</td></tr><tr><td>qtimltflite</td><td>Loading and executing LiteRT TFLite model files. It receives input tensors from the preprocessing plugin (qtimlvconverter) and outputs tensors passed to plugins such as qtimlvclassification, qtimlvdetection, qtimlvsegmentation, and qtimlvpose.</td></tr><tr><td>qtimlvconverter</td><td>Converting data from the incoming video buffer into neural network tensors, while performing the necessary format conversion and resizing.</td></tr><tr><td>qtimlvclassification</td><td>Post-processing the output tensors for classification use cases.</td></tr><tr><td>qtimlvdetection</td><td>Post-processing the output tensors for detection use cases.</td></tr><tr><td>qtimlvsegmentation</td><td>Post-processing the output tensors for pixel-level use cases such as image segmentation or depth map processing.</td></tr><tr><td>qtimlvpose</td><td>Post-processing the output tensors for pose estimation use cases.</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="aiml-sample-applications">AI/ML sample applications<a href="#aiml-sample-applications" class="hash-link" aria-label="Direct link to AI/ML sample applications" title="Direct link to AI/ML sample applications">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preparations">Preparations<a href="#preparations" class="hash-link" aria-label="Direct link to Preparations" title="Direct link to Preparations">​</a></h3>
<ol>
<li>
<p>Install the software package.</p>
<p>Refer to the <a href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/run-sample-applications">Run sample applications</a> chapter and ensure that the sample application can run successfully.</p>
</li>
<li>
<p>Download the compressed package containing the model file, label file, and JSON configuration file required for the AI/ML sample application.</p>
<ul>
<li>
<p>On the device, run the following command to download the compressed package <em>ai_sample_app_models_labels_configs.zip</em>.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">wget https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_sample_app_models_labels_configs.zip</span><br></span></code></pre></div></div>
</li>
<li>
<p>Use the following command to extract the compressed package to the <em>/etc</em> directory.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unzip ./ai_sample_app_model_label.zip -d /etc/</span><br></span></code></pre></div></div>
</li>
<li>
<p>List of files in the compressed package:</p>
</li>
</ul>
</li>
</ol>
<table><thead><tr><th>Model File</th><th>Label File</th><th>Json Configuration File</th></tr></thead><tbody><tr><td>models/deeplabv3_plus_mobilenet_quantized.tflite</td><td>labels/classification_0.labels</td><td>configs/config_classification.json</td></tr><tr><td>models/midas_quantized.tflite</td><td>labels/hrnet_pose.labels</td><td>configs/config_monodepth.json</td></tr><tr><td>models/hrnet_pose_quantized.tflite</td><td>labels/yolov5.labels</td><td>configs/config_daisychain_detection_classification.json</td></tr><tr><td>models/yolov5.tflite</td><td>labels/deeplabv3_resnet50.labels</td><td>configs/config_pose.json</td></tr><tr><td>models/inception_v3_quantized.tflite</td><td>labels/monodepth.labels</td><td>configs/config_detection.json</td></tr><tr><td>models/yolov8_det_quantized.tflite</td><td>labels/yolov8.labels</td><td>configs/config_segmentation.json</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-sample-application-gst-ai-classification">Image classification sample application: gst-ai-classification<a href="#image-classification-sample-application-gst-ai-classification" class="hash-link" aria-label="Direct link to Image classification sample application: gst-ai-classification" title="Direct link to Image classification sample application: gst-ai-classification">​</a></h3>
<p>The gst-ai-classification application allows you to identify the subject in an image. The use cases are implemented using the SNPE, LiteRT, or QNN.</p>
<p>The following figure shows the pipeline, which receives a video stream from a camera, file source, or Real-Time Streaming Protocol (RTSP), preprocesses it, runs the inference on AI hardware, and displays the results on the screen.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-12-741d9d30a3784aacdfa297be189e3470.png" width="975" height="326" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application：<em>/etc/configs/config_classification.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/inception_v3_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/classification_0.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Inceptionv3,q-offsets=&lt;38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/inception_v3_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/classification_0.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Inceptionv3,q-offsets=&lt;38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<ol start="2">
<li>Execute the following commands to run the sample application:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-classification --config-file=/etc/configs/config_classification.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-11-082a11bc07fd6a1828909518f3fdc27c.png" width="840" height="471" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-sample-application-gst-ai-object-detection">Object detection sample application: gst-ai-object-detection<a href="#object-detection-sample-application-gst-ai-object-detection" class="hash-link" aria-label="Direct link to Object detection sample application: gst-ai-object-detection" title="Direct link to Object detection sample application: gst-ai-object-detection">​</a></h3>
<p>The gst-ai-object-detection application allows you to detect objects within images and videos.</p>
<p>The following figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, preprocesses it, runs inferences on AI hardware, and displays the results on the screen.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-10-89cf58dc0388cb76a8b8e499f5c48ca3.png" width="975" height="326" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application: <em>/etc/configs/config_detection.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;0&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;yolo-model-type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;yolov8&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/yolov8_det_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/yolov8.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;yolo-model-type&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;yolov8&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/yolov8_det_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/yolov8.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<ol start="2">
<li>Execute the following commands to run the sample application:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-object-detection --config-file=/etc/configs/config_detection.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-29-cc32640c9f279203020de85f7b24bb51.png" width="975" height="548" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="pose-detection-sample-application-gst-ai-pose-detection">Pose detection sample application: gst-ai-pose-detection<a href="#pose-detection-sample-application-gst-ai-pose-detection" class="hash-link" aria-label="Direct link to Pose detection sample application: gst-ai-pose-detection" title="Direct link to Pose detection sample application: gst-ai-pose-detection">​</a></h3>
<p>The gst-ai-pose-detection application allows you to detect the body pose of the subject in an image or video.</p>
<p>The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-8-ee712ed3b63800f979729e008bc80e8c.png" width="975" height="310" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application: <em>/etc/configs/config_pose.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;0&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/hrnet_pose_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/hrnet_pose.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">51</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/hrnet_pose_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/hrnet_pose.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;threshold&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">51</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<ol start="2">
<li>Execute the following commands to run the sample application:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-pose-detection  --config-file=/etc/configs/config_pose.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-7-f1971359954c508282a3baff03b79e55.png" width="845" height="479" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-sample-application-gst-ai-segmentation">Image segmentation sample application: gst-ai-segmentation<a href="#image-segmentation-sample-application-gst-ai-segmentation" class="hash-link" aria-label="Direct link to Image segmentation sample application: gst-ai-segmentation" title="Direct link to Image segmentation sample application: gst-ai-segmentation">​</a></h3>
<p>The gst-ai-segmentation application allows you to divide an image into different and meaningful parts or segments.</p>
<p>The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-6-04cca60377ca19c815ee01ba9ba8561e.png" width="975" height="326" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application: <em>/etc/configs/config_segmentation.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/deeplabv3_plus_mobilenet_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/deeplabv3_resnet50.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;deeplab,q-offsets=&lt;0.0&gt;,q-scales=&lt;1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/deeplabv3_plus_mobilenet_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/deeplabv3_resnet50.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;deeplab,q-offsets=&lt;0.0&gt;,q-scales=&lt;1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<ol start="2">
<li>Execute the following commands to run the sample application:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-segmentation  --config-file=/etc/configs/config_segmentation.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-5-8dc29396b381b73bf3fb5425b1bd2c4d.png" width="975" height="548" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-inputoutput-object-detection-sample-application-gst-ai-multi-input-output-object-detection">Multi input/output object detection sample application: gst-ai-multi-input-output-object-detection<a href="#multi-inputoutput-object-detection-sample-application-gst-ai-multi-input-output-object-detection" class="hash-link" aria-label="Direct link to Multi input/output object detection sample application: gst-ai-multi-input-output-object-detection" title="Direct link to Multi input/output object detection sample application: gst-ai-multi-input-output-object-detection">​</a></h3>
<p>The gst-ai-multi-input-output-object-detection application allows you to perform object detection on video streams from multiple sources such as two cameras, two video files, or over the network protocol such as RTSP.</p>
<p>The figure shows the pipeline, which receives the input from a live camera feed, file, or an RTSP stream, performs preprocessing, conducts inference on the AI hardware, and displays the results on the screen.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-4-446e5b939bd22358dc6d1f07fdf63d7b.png" width="975" height="267" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect two cameras before testing.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-multi-input-output-object-detection --num-camera=2 --out-file=/opt/H.mp4 -d --model=/etc/models/yolov5.tflite --labels=/etc/labels/yolov5.labels</span><br></span></code></pre></div></div>
<p>The current command demonstrates a use case where video streams from two cameras are used simultaneously for object detection, with the inference results saved to an MP4 file and displayed on a screen.</p>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-2-6c70089573f2a146c7506e93efa075a0.png" width="657" height="183" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-and-image-classification-cascade-sample-applicationgst-ai-daisychain-detection-classification">Object detection and image classification cascade sample application：gst-ai-daisychain-detection-classification<a href="#object-detection-and-image-classification-cascade-sample-applicationgst-ai-daisychain-detection-classification" class="hash-link" aria-label="Direct link to Object detection and image classification cascade sample application：gst-ai-daisychain-detection-classification" title="Direct link to Object detection and image classification cascade sample application：gst-ai-daisychain-detection-classification">​</a></h3>
<p>The gst-ai-daisychain-detection-classification sample application can perform cascaded object detection and classification using a camera, file, or RTSP stream.</p>
<p>The figure shows the pipeline, which receives the input from a live camera feed, file, or other sources, performs preprocessing, conducts inference on the AI hardware, and displays the detection and classification results on the screen.</p>
<p><em>Figure: gst-ai-daisychain-detection-classification send pipeline</em>
<img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-1-b0223958c35a657b84a001c69ff12767.png" width="1280" height="405" class="img_ev3q"></p>
<p><em>Figure: gst-ai-daisychain-detection-classification inference pipeline</em>
<img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-3-1162dd6f3167f2250ad0193eb4f4e0a4.png" width="1280" height="487" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application: <em>/etc/configs/config_daisychain_detection_classification.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/yolov8_det_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/yolov8.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/inception_v3_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/classification_0.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Inceptionv3,q-offsets=&lt;38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/yolov8_det_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/yolov8.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/inception_v3_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/classification_0.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;detection-constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;classification-constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Inceptionv3,q-offsets=&lt;38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-daisychain-detection-classification   --config-file=/etc/configs/config_daisychain_detection_classification.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-26-6bf6a233dd1539ce9e1358135da6993b.png" width="1280" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-depth-estimation-sample-application-gst-ai-monodepth">Image depth estimation sample application: gst-ai-monodepth<a href="#image-depth-estimation-sample-application-gst-ai-monodepth" class="hash-link" aria-label="Direct link to Image depth estimation sample application: gst-ai-monodepth" title="Direct link to Image depth estimation sample application: gst-ai-monodepth">​</a></h3>
<p>The gst-ai-monodepth sample application can acquire visual data from three types of input sources: a live camera feed, a local video file, or a network RTSP stream. It uses a monocular depth estimation algorithm (without requiring a stereo camera) to automatically analyze and infer the depth information of each object in the scene — that is, how far each object is from the camera or observer.</p>
<p>The diagram below shows the workflow of the sample application.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-25-5c7e87f10c88e121fbbf48e5a5e91776.png" width="975" height="312" class="img_ev3q"></p>
<ul>
<li>JSON configuration file used by the sample application: <em>/etc/configs/config_monodepth.json</em></li>
</ul>
<p><strong>Follow these steps to perform testing:</strong></p>
<ol>
<li>Use the camera as the input source:</li>
</ol>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Connect the camera before testing.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;camera&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/midas_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/monodepth.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Midas,q-offsets=&lt;0.0&gt;,q-scales=&lt;6.846843242645264&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>Alternatively, use the MP4 video file as the input source:</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Set the <code>file-path</code> attribute below to the path of your MP4 video file.</p></div></div>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;file-path&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/media/video.mp4&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;ml-framework&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/models/midas_quantized.tflite&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;labels&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;/etc/labels/monodepth.labels&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;constants&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Midas,q-offsets=&lt;0.0&gt;,q-scales=&lt;6.846843242645264&gt;;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;runtime&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;dsp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<ol start="2">
<li>Execute the following commands to run the sample application:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-ai-classification --config-file=/etc/configs/config_monodepth.json</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-24-bc557e41d8257d66befc033cce7e7331.png" width="986" height="480" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="aiml-gstreamer-command-line-use-cases">AI/ML gstreamer command-line use cases<a href="#aiml-gstreamer-command-line-use-cases" class="hash-link" aria-label="Direct link to AI/ML gstreamer command-line use cases" title="Direct link to AI/ML gstreamer command-line use cases">​</a></h2>
<p>The AI/ML GStreamer command line use cases demonstrate the practical scenario of using the GStreamer plugin from QIM on a RUBIK Pi 3 device to feed data from a live camera or a local video file and then run a model. The following will detail the steps to run the sample applications.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preparations-1">Preparations<a href="#preparations-1" class="hash-link" aria-label="Direct link to Preparations" title="Direct link to Preparations">​</a></h3>
<ol>
<li>
<p>Install the software package.</p>
<p>Refer to the <a href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/run-sample-applications">Run sample applications</a> chapter and ensure that the sample application can run successfully.</p>
</li>
<li>
<p>Download the model file and label file.</p>
</li>
</ol>
<ul>
<li>
<p>Use the following command on the device to download the compressed file <em>ai_gstreamer_command_line_models_labels.zip</em>, which contains the model files and label files required for the AI/ML GStreamer command line use cases.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">wget https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_gstreamer_command_line_models_labels.zip</span><br></span></code></pre></div></div>
</li>
<li>
<p>Use the following command to extract the compressed file to the <em>/etc</em> directory:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">unzip ./ai_gstreamer_command_line_models_labels.zip -d /opt/</span><br></span></code></pre></div></div>
</li>
</ul>
<p><strong>List of GStreamer command-line use cases using LiteRT and the corresponding model files and label files</strong></p>
<table><thead><tr><th style="text-align:left">GStreamer Command-line Use Case</th><th style="text-align:left">Required Model</th><th style="text-align:left">Required Label File</th></tr></thead><tbody><tr><td style="text-align:left">image-classification-LiteRT-from-camera/file</td><td style="text-align:left">resnet101-resnet101-w8a8.tflite</td><td style="text-align:left">classification_0.labels</td></tr><tr><td style="text-align:left">object-detection-LiteRT-from-camera/file</td><td style="text-align:left">yolov8_det_quantized.tflite</td><td style="text-align:left">yolov8.labels</td></tr><tr><td style="text-align:left">image-segmentation-LiteRT-from-camera/file</td><td style="text-align:left">deeplabv3_plus_mobilenet_quantized.tflite</td><td style="text-align:left">deeplabv3_resnet50.labels</td></tr><tr><td style="text-align:left">pose-detection-LiteRT-from-camera/file</td><td style="text-align:left">hrnet_pose_quantized.tflite</td><td style="text-align:left">hrnet_pose.labels</td></tr></tbody></table>
<p><strong>List of GStreamer command-line use cases using SNPE and the corresponding model files and label files</strong></p>
<table><thead><tr><th style="text-align:left">GStreamer Command-line Use Case</th><th style="text-align:left">Required Model</th><th style="text-align:left">Required Label File</th></tr></thead><tbody><tr><td style="text-align:left">image-classification-LiteRT-from-camera/file</td><td style="text-align:left">inceptionv3.dlc</td><td style="text-align:left">classification.labels</td></tr><tr><td style="text-align:left">object-detection-LiteRT-from-camera/file</td><td style="text-align:left">yolonas.dlc</td><td style="text-align:left">yolonas.labels</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-using-litert">GStreamer command-line use cases using LiteRT<a href="#gstreamer-command-line-use-cases-using-litert" class="hash-link" aria-label="Direct link to GStreamer command-line use cases using LiteRT" title="Direct link to GStreamer command-line use cases using LiteRT">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-for-implementing-ai-functions-by-acquiring-image-data-through-a-camera">GStreamer command-line use cases for implementing AI functions by acquiring image data through a camera<a href="#gstreamer-command-line-use-cases-for-implementing-ai-functions-by-acquiring-image-data-through-a-camera" class="hash-link" aria-label="Direct link to GStreamer command-line use cases for implementing AI functions by acquiring image data through a camera" title="Direct link to GStreamer command-line use cases for implementing AI functions by acquiring image data through a camera">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-camera">Image classification (image-classification-LiteRT-from-camera)<a href="#image-classification-image-classification-litert-from-camera" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-camera)" title="Direct link to Image classification (image-classification-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the resnet101-resnet101-w8a8.tflite model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-23-66cc812771313ac9fecbf078ad04297d.png" width="975" height="316" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0  ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;&quot;  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants=&quot;Inception,q-offsets=&lt;-38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. </span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-1-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-camera">Object detection (object-detection-LiteRT-from-camera)<a href="#object-detection-object-detection-litert-from-camera" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-camera)" title="Direct link to Object detection (object-detection-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the yolov8_det_quantized.tflite model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-22-9c134a6e429f27e3dc49ef75f7adc374.png" width="975" height="289" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants=&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,    q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-21-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-image-segmentation-litert-from-camera">Image segmentation (image-segmentation-LiteRT-from-camera)<a href="#image-segmentation-image-segmentation-litert-from-camera" class="hash-link" aria-label="Direct link to Image segmentation (image-segmentation-LiteRT-from-camera)" title="Direct link to Image segmentation (image-segmentation-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the deeplabv3_plus_mobilenet_quantized.tflite model with HTP. The segmentation results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-20-8c1e9b67a67210d4f7b2cc27b8163eb4.png" width="731" height="211" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants=&quot;deeplab,q-offsets=&lt;-61.0&gt;,q-scales=&lt;0.06232302635908127&gt;;&quot; ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-19-686f9af59c269c1cd7b1ad597c2d581c.png" width="918" height="519" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="pose-detection-pose-detection-litert-from-camera">Pose detection (pose-detection-LiteRT-from-camera)<a href="#pose-detection-pose-detection-litert-from-camera" class="hash-link" aria-label="Direct link to Pose detection (pose-detection-LiteRT-from-camera)" title="Direct link to Pose detection (pose-detection-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to LiteRT, which performs inference using the hrnet_pose_quantized.tflite model with HTP. The pose detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-18-af525de61dd224351c7a344902400b1a.png" width="975" height="289" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants=&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-17-f68564401dde9af3270e947cddd533a9.png" width="864" height="534" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file">GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file<a href="#gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file" class="hash-link" aria-label="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file" title="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-file">Image classification (image-classification-LiteRT-from-file)<a href="#image-classification-image-classification-litert-from-file" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-file)" title="Direct link to Image classification (image-classification-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the resnet101-resnet101-w8a8.tflite model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-4-214c56c6055ceb84c6bfdd039d2cc136.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;&quot;  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants=&quot;Inception,q-offsets=&lt;-38.0&gt;,q-scales=&lt;0.17039915919303894&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. </span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-file">Object detection (object-detection-LiteRT-from-file)<a href="#object-detection-object-detection-litert-from-file" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-file)" title="Direct link to Object detection (object-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the yolov8_det_quantized.tflite model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-5-3143d47d7398911c36fff70e8d7539a5.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants=&quot;YOLOv8,q-offsets=&lt;21.0, 0.0, 0.0&gt;,    q-scales=&lt;3.0546178817749023, 0.003793874057009816, 1.0&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-16-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation-image-segmentation-litert-from-file">Image segmentation (image-segmentation-LiteRT-from-file)<a href="#image-segmentation-image-segmentation-litert-from-file" class="hash-link" aria-label="Direct link to Image segmentation (image-segmentation-LiteRT-from-file)" title="Direct link to Image segmentation (image-segmentation-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the deeplabv3_plus_mobilenet_quantized.tflite model with HTP. The segmentation results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-6-f343bca1a2e74ce2f47395299b649386.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants=&quot;deeplab,q-offsets=&lt;-61.0&gt;,q-scales=&lt;0.06232302635908127&gt;;&quot; ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-14-686f9af59c269c1cd7b1ad597c2d581c.png" width="918" height="519" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="pose-detection-pose-detection-litert-from-file">Pose detection (pose-detection-LiteRT-from-file)<a href="#pose-detection-pose-detection-litert-from-file" class="hash-link" aria-label="Direct link to Pose detection (pose-detection-LiteRT-from-file)" title="Direct link to Pose detection (pose-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to LiteRT, which performs inference using the hrnet_pose_quantized.tflite model with HTP. The pose detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-7-2afa9bb0f9e54645a99a01a8a48829e5.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&quot;waylandsink fullscreen=true&quot; split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options=&quot;QNNExternalDelegate,backend_type=htp;&quot; model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants=&quot;hrnet,q-offsets=&lt;8.0&gt;,q-scales=&lt;0.0040499246679246426&gt;;&quot; ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-15-f68564401dde9af3270e947cddd533a9.png" width="864" height="534" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-using-snpe">GStreamer command-line use cases using SNPE<a href="#gstreamer-command-line-use-cases-using-snpe" class="hash-link" aria-label="Direct link to GStreamer command-line use cases using SNPE" title="Direct link to GStreamer command-line use cases using SNPE">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-camera">GStreamer command-line use cases for implementing AI functionality by acquiring image data from a camera<a href="#gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-camera" class="hash-link" aria-label="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a camera" title="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a camera">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-camera-1">Image classification (image-classification-LiteRT-from-camera)<a href="#image-classification-image-classification-litert-from-camera-1" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-camera)" title="Direct link to Image classification (image-classification-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to SNPE, which performs inference using the inceptionv3.dlc model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-8-ea69f9c326053e3bbb207e929c7cde51.png" width="2086" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink=&quot;waylandsink sync=true fullscreen=true&quot;  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-3-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-camera-1">Object detection (object-detection-LiteRT-from-camera)<a href="#object-detection-object-detection-litert-from-camera-1" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-camera)" title="Direct link to Object detection (object-detection-LiteRT-from-camera)">​</a></h5>
<p>This sample application uses a camera to capture images in real time and sends them to SNPE, which performs inference using the yolonas.labels model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-9-342c8096a6e6e4d8b6d05e9f96629bd3.png" width="2086" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&#x27;waylandsink fullscreen=true sync=true&#x27; split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=&quot;&lt;/heads/Mul, /heads/Sigmoid&gt;&quot; ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-27-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file-1">GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file<a href="#gstreamer-command-line-use-cases-for-implementing-ai-functionality-by-acquiring-image-data-from-a-recorded-mp4-file-1" class="hash-link" aria-label="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file" title="Direct link to GStreamer command-line use cases for implementing AI functionality by acquiring image data from a recorded MP4 file">​</a></h4>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="image-classification-image-classification-litert-from-file-1">Image classification (image-classification-LiteRT-from-file)<a href="#image-classification-image-classification-litert-from-file-1" class="hash-link" aria-label="Direct link to Image classification (image-classification-LiteRT-from-file)" title="Direct link to Image classification (image-classification-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to SNPE, which performs inference using the inceptionv3.dlc model with HTP. The classification results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-10-ba0daee74e210ca9bfdd5f611fa2635a.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink=&quot;waylandsink sync=true fullscreen=true&quot;  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/img_v3_02oe_fa932acb-c7ad-4586-b169-ab0589ff40dg-2-5d1382a3bbb82a0dc9cf91b347fbb60b.jpg" width="1280" height="960" class="img_ev3q"></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection-object-detection-litert-from-file-1">Object detection (object-detection-LiteRT-from-file)<a href="#object-detection-object-detection-litert-from-file-1" class="hash-link" aria-label="Direct link to Object detection (object-detection-LiteRT-from-file)" title="Direct link to Object detection (object-detection-LiteRT-from-file)">​</a></h5>
<p>This sample application uses an MP4 file to obtain image data and sends it to SNPE, which performs inference using the yolonas.labels model with HTP. The object detection results and image information are then displayed on a monitor via Weston. Refer to the following diagram for the pipeline.</p>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/diagram-11-78ba5f302400fab97b557b2d11724176.png" width="2688" height="696" class="img_ev3q"></p>
<ul>
<li>Execute the following commands to run the sample application:</li>
</ul>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">sudo -i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ &amp;&amp; export WAYLAND_DISPLAY=wayland-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gst-launch-1.0 -e --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink=&#x27;waylandsink fullscreen=true sync=true&#x27; split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers=&quot;&lt;/heads/Mul, /heads/Sigmoid&gt;&quot; ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.</span><br></span></code></pre></div></div>
<ul>
<li>The result is shown below:</li>
</ul>
<p><img decoding="async" loading="lazy" src="/rubikpi-ubuntu-user-manual-test-en.github.io/assets/images/image-28-29fb4dee3b1a7948034a312f8bfa8709.png" width="956" height="486" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="refences">Refences<a href="#refences" class="hash-link" aria-label="Direct link to Refences" title="Direct link to Refences">​</a></h2>
<ul>
<li>
<p>Reference documentation for compiling sample applications</p>
<p><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-90441-15/introduction.html?product=1601111740057201&amp;facet=AI%20developer%20workflow" target="_blank" rel="noopener noreferrer">AI Developer_Workflow</a></p>
</li>
<li>
<p>Reference documentation for the QIM SDK and all AI/MM sample applications</p>
<p><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-70020-50/example-applications.html?vproduct=1601111740013072&amp;version=1.5&amp;facet=Qualcomm%20Intelligent%20Multimedia%20SDK" target="_blank" rel="noopener noreferrer">Qualcomm Intelligent Multimedia SDK (IM SDK) Reference - Qualcomm® Linux Documentation</a></p>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/hongyang-rp/rubikpi-ubuntu-user-manual-test-en.github.io/tree/main/docs/Document Home/8.ai.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/camera-software"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Camera Software</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rubikpi-ubuntu-user-manual-test-en.github.io/docs/Document Home/ubuntu-desktop-vs-server"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Comparison Between Ubuntu Desktop Version and Server Version</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#software-and-hardware-architecture" class="table-of-contents__link toc-highlight">Software and hardware architecture</a></li><li><a href="#compile-and-optimize-the-model" class="table-of-contents__link toc-highlight">Compile and optimize the model</a></li><li><a href="#aiml-sample-applications" class="table-of-contents__link toc-highlight">AI/ML sample applications</a></li><li><a href="#aiml-gstreamer-command-line-use-cases" class="table-of-contents__link toc-highlight">AI/ML gstreamer command-line use cases</a></li><li><a href="#refences" class="table-of-contents__link toc-highlight">Refences</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/Qualcomm" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/rubikpi-ubuntu-user-manual-test-en.github.io/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/rubikpi-ai" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>