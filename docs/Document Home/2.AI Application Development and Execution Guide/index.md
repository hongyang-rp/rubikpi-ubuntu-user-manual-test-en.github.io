# Welcome

Welcome to the AI docs for Qualcomm Dragonwing. Dragonwing is a family of IoT SoCs that offer high performance, advanced connectivity and efficient power - paired with fast GPUs and NPUs; and running a variety of operating systems (Ubuntu, Qualcomm Linux, Android and Windows). This makes them perfect for advanced edge AI workloads, or high-performance computer vision applications.  

Artificial Intelligence is rapidly transforming how applications are built, deployed, and experiencedâ€”especially at the edge. This guide is designed to help developers, engineers, and program managers navigate the end-to-end journey of AI application development using Qualcomm-supported tools, runtimes, and frameworks.
Whether you're building models from scratch, deploying pre-trained networks, or orchestrating multimodal AI workflows, this guide provides a structured approach to understanding and executing AI workloads across diverse environments. It covers everything from model creation using platforms like Edge Impulse and AI Hub, to running sample applications with LiteRT, ONNX, Llama.cpp, and Genie, and includes hands-on examples using IMSDK and robotics SDKs like QIRP.
Each chapter is crafted to be modular, allowing you to dive into the tools and flows most relevant to your project. The goal is to empower you with practical insights, reusable samples, and a clear understanding of how to integrate AI into real-world applicationsâ€”efficiently and effectively.
Letâ€™s get started.

## ðŸ“Š AI Development & Execution Flow Summary

|Flow               |Purpose                                                                                               |Link         |
|-------------------|------------------------------------------------------------------------------------------------------|-------------|
|Edge Impulse       |Build and train AI models using sensor data and AutoML tools for edge devices.                        |[Add Link]   |
| Qualcomm AI Hub   | Access pre-trained models optimized for Snapdragon platforms; supports deployment and benchmarking.  | [Add Link]  |
| LiteRT/TFLite     | Run lightweight inference on embedded systems using minimal runtimes like LiteRT and TensorFlow Lite.| [Add Link]  |
| ONNX              | Deploy interoperable models from various training frameworks using ONNX Runtime.                     | [Add Link]  |
| Llama.cpp         | Execute large language models locally using a C++ backend optimized for CPUs and quantized formats.  | [Add Link]  |
| Genie             | Orchestrate AI microservices and multimodal workflows using Qualcommâ€™s generative AI runtime.        | [Add Link]  |